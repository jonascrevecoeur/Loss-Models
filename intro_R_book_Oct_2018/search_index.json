[
["index.html", "Data science in insurance: an R intro 1 Introduction 1.1 Learning outcomes 1.2 Data/science pipeline 1.3 Inspirations and references 1.4 About this book", " Data science in insurance: an R intro Chester Ismay and Albert Y. Kim and Katrien Antonio 2018-10-28 1 Introduction This book assumes no prerequisites: no algebra, no calculus, and no prior programming/coding experience. This is intended to be a gentle introduction to the practice of analyzing data and answering questions using data the way data scientists, statisticians, data journalists, and other researchers would. Our inspiration is the open source ModernDive book (Ismay and Kim 2018), with many tweaks, additions and changes by Katrien Antonio. In particularly, Katrien adjusted this text book towards use by actuarial students and practitioners. We get started with R in Chapter 2: R vs RStudio, coding in R, installing and loading R packages, the references used in this book. Then we look into different types of data and objects in R, including vectors, matrices, data frames and lists in Chapter 3. We get started with data in Chapter 4. Data visualisation is the focus of Chapter 5. More on data wrangling in Chapter 6. Actuaries care about probability distributions, discussed in Chapter 7. Using and writing functions is the topic of Chapter 8. Optimization tools help to optimize non straightforward likelihoods as discussed in Chapter 9. First examples of model building focus on linear and generalized linear models in Chapters 10 and 11 References follow in 12. 1.1 Learning outcomes We hope that by the end of this book, you’ll have learned What is R as an environment for data handling, visualization, analysis and programming. How to use R to calculate, to import/export data, to explore data, to create insightful graphics, to write functions. How to find help in the ‘R community’, including finding examples of coding, books, support. How to perform simple tasks with R and how to look for more advanced tasks, further learning with specific packages. How to answer actuarial questions related to pricing and reserving. How to effectively create “data stories” using these tools. This book will help you develop your “data science toolbox”, including tools such as data visualization, data formatting, data wrangling, and data modeling using regression. 1.2 Data/science pipeline Inside data analysis are many sub-fields that we will discuss throughout this book (though not necessarily in this order): data collection data wrangling data visualization data modeling interpretation of results data communication/storytelling These sub-fields are summarized in what Grolemund and Wickham term the “data/science pipeline” in Figure 1.1. Figure 1.1: Data/Science Pipeline 1.3 Inspirations and references Basically, the book combines my (Katrien Antonio) own research papers and coursenotes with many useful quotes and examples from my favourite R books listed below. This book is very much inspired by the following books or courses: “Mathematical Statistics with Resampling and R” (Chihara and Hesterberg 2011), “OpenIntro: Intro Stat with Randomization and Simulation” (Diez, Barr, and Çetinkaya-Rundel 2014), and “R for Data Science” (Grolemund and Wickham 2016), “Moderndive” (Ismay and Kim 2018), Jared Lander’s “R for everyone” (Lander 2017) “Applied Econometrics with R” (Kleiber and Zeileis 2008) “An Introduction to Statistical Learning” (James et al. 20AD) all the work of Michael Clark, see Michael Clark’s website - many, many courses on the DataCamp platform, including Katrien Antonio and Roel Verbelen’s Valuation of Life Insurance Products in R. 1.4 About this book This book was written using RStudio’s bookdown package by Yihui Xie (Xie 2018). This package simplifies the publishing of books by having all content written in R Markdown. The bookdown/R Markdown source code for all versions of ModernDive is available on GitHub. Could this be a new paradigm for textbooks? Instead of the traditional model of textbook companies publishing updated editions of the textbook every few years, we apply a software design influenced model of publishing more easily updated versions. We can then leverage open-source communities of instructors and developers for ideas, tools, resources, and feedback. As such, we welcome your pull requests. Finally, feel free to modify the book as you wish for your own needs, but please list the authors at the top of index.Rmd as “Chester Ismay, Albert Y. Kim, and YOU!” So, that is exactly what Katrien Antonio did! "],
["getting-started.html", "2 Getting started in R 2.1 Some history 2.2 What are R and RStudio? 2.3 How do I code in R? 2.4 What are R packages? 2.5 Conclusion", " 2 Getting started in R Before we can start exploring data in R, there are some key concepts to understand first: What are R and RStudio? How do I code in R? What are R packages? Much of this chapter is based on two sources which you should feel free to use as references if you are looking for additional details: Ismay’s Getting used to R, RStudio, and R Markdown (Ismay 2016), which includes video screen recordings that you can follow along and pause as you learn. DataCamp’s online tutorials. DataCamp is a browser-based interactive platform for learning data science and their tutorials will help facilitate your learning of the above concepts (and other topics in this book). Go to DataCamp and create an account before continuing. 2.1 Some history R is a dialect of the S language (developed by John Chambers at Bell Labs in the 70s), namely ‘Gnu-S’. R was written by Robert Gentleman and Ross Ihaka (while being at the University of Auckland) in 1993. The (Kleiber and Zeileis 2008) book sketches the history of R, in Sections 1.5 and 1.6. The R source code was first released under the GNU General Public License (GPL) in 1995. Since mid-1997, there has been the R Development Core Team, currently comprising 20 members. In 1998, the Comprehensive R Archive Network CRAN was established, a family of mirror sites around the world that store identical, up-to-date versions of code and documentation for R. The first official release, R version 1.0.0, dates to 2000-02-29. Currently, version 3.5.0 is available. R is open source, i.e. GNU General Public License. The R environment (see About R) is ‘an integrated suite of software facilities for data manipulation, calculation and graphical display’. 2.2 What are R and RStudio? For much of this book, we will assume that you are using R via RStudio. First time users often confuse the two. At its simplest: R is like a car’s engine RStudio is like a car’s dashboard R: Engine RStudio: Dashboard More precisely, R is a programming language that runs computations while RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. So the way of having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well. Optional: For a more in-depth discussion on the difference between R and RStudio IDE, watch this DataCamp video (2m52s). 2.2.1 Installing R and RStudio You will first need to download and install both R and RStudio (Desktop version) on your computer. Download and install R. Note: You must do this first. Click on the download link corresponding to your computer’s operating system. Download and install RStudio. Scroll down to “Installers for Supported Platforms” Click on the download link corresponding to your computer’s operating system. Optional: If you need more detailed instructions on how to install R and RStudio, watch this DataCamp video (1m22s). 2.2.2 Using R via RStudio Recall our car analogy from above. Much as we don’t drive a car by interacting directly with the engine but rather by using elements on the car’s dashboard, we won’t be using R directly but rather we will use RStudio’s interface. After you install R and RStudio on your computer, you’ll have two new programs AKA applications you can open. We will always work in RStudio and not R. In other words: R: Do not open this RStudio: Open this After you open RStudio, you should see the following: Watch the following DataCamp video (4m10s) to learn about the different panes in RStudio, in particular the Console pane where you will later run R code. 2.3 How do I code in R? Now that you’re set up with R and RStudio, you are probably asking yourself “OK. Now how do I use R?” The first thing to note as that unlike other software like Excel, STATA, or SAS that provide point and click interfaces, R is an interpreted language, meaning you have to enter in R commands written in R code i.e. you have to program in R (we use the terms “coding” and “programming” interchangeably in this book). While it is not required to be a seasoned coder/computer programmer to use R, there is still a set of basic programming concepts that R users need to understand. Consequently, while this book is not a book on programming, you will still learn just enough of these basic programming concepts needed to explore and analyze data effectively. 2.3.1 Tips on learning to code Learning to code/program is very much like learning a foreign language, it can be very daunting and frustrating at first. However just as with learning a foreign language, if you put in the effort and are not afraid to make mistakes, anybody can learn. Lastly, there are a few useful things to keep in mind as you learn to program: Computers are stupid: You have to tell a computer everything it needs to do. Furthermore, your instructions can’t have any mistakes in them, nor can they be ambiguous in any way. Take the “copy/paste/tweak” approach: Especially when learning your first programming language, it is often much easier to taking existing code that you know works and modify it to suit your ends, rather than trying to write new code from scratch. We call this the copy/paste/tweak approach. So early on, we suggest not trying to code from scratch, but please take the code we provide throughout this book and play around with it! Practice is key: Just as the only solution to improving your foreign language skills is practice, so also the only way to get better at R is through pracitice. Don’t worry however, we’ll give you plenty of opportunities to practice! 2.4 What are R packages? Another point of confusion with new R users is the notion of a package. R packages extend the functionality of R by providing additional functions, data, and documentation and can be downloaded for free from the internet. They are written by a world-wide community of R users. For example, among the many packages we will use in this book are the ggplot2 package for data visualization in Chapter 5 dplyr package for data wrangling in Chapter 6 There are two key things to remember about R packages: Installation: Most packages are not installed by default when you install R and RStudio. You need to install a package before you can use it. Once you’ve installed it, you likely don’t need to install it again unless you want to update it to a newer version of the package. Loading: Packages are not loaded automatically when you open RStudio. You need to load them everytime you open RStudio using the library() command. A good analogy for R packages is they are like apps you can download onto a mobile phone: R: A new phone R Packages: Apps you can download So, expanding on this analogy a bit: R is like a new mobile phone. It has a certain amount of functionality when you use it for the first time, but it doesn’t have everything. R packages are like the apps you can download onto your phone, much like those offered in the App Store and Google Play. For example: Instagram. In order to use a package, just like in order to use Instagram, you must: First download it and install it. You do this only once. Load it, or in other words, “open” it, using the library() command. So just as you can only start sharing photos with your friends on Instagram if you first install the app and then open it, you can only access an R package’s data and functions if you first install the package and then load it with the library() command. Let’s cover these two steps: 2.4.1 Package installation (Note that if you are working on an RStudio Server, you probably will not need to install your own packages as that has been already done for you. Still it is important that you know this process for later when you are not using the RStudio Server but rather your own installation of RStudio Desktop.) There are two ways to install an R package. For example, to install the ggplot2 package: Easy way: In the Files pane of RStudio: Click on the “Packages” tab Click on “Install” Type the name of the package under “Packages (separate multiple with space or comma):” In this case, type ggplot2 Click “Install” Alternative way: In the Console pane run install.packages(&quot;ggplot2&quot;) (you must include the quotation marks). Repeat this for the dplyr and nycflights13 packages. If you still experience problems, have a look at this blog post Installing R packages. Note: You only have to install a package once, unless you want to update an already installed package to the latest version. If you want to update a package to the latest version, then re-install it by repeating the above steps. 2.4.2 Package loading After you’ve installed a package, you can now load it using the library() command. For example, to load the ggplot2 and dplyr packages, run the following code in the Console pane: library(ggplot2) library(dplyr) Note: You have to reload each package you want to use every time you open a new session of RStudio. This is a little annoying to get used to and will be your most common error as you begin. When you see an error such as Error: could not find function remember that this likely comes from you trying to use a function in a package that has not been loaded. Remember to run the library() function with the appropriate package to fix this error. 2.4.3 Packages on CRAN R comes with a set of base packages or base system, maintained by the R core team only. Examples: base, datasets, graphics. Additional packages are on CRAN (Thursday 1/23/2014: 5,140 packages; Sunday 9/7/2014: 5,852 packages; Saturday 11/08/2014: 6,041 packages; Tuesday 11/8/2016: 9,473 packages; Monday 12/04/2017: 11,946 packages; Tuesday 04/10/2018: 12,430 packages). These packages are developed and maintained by R users worldwide, and shared with the R community through CRAN. 2.5 Conclusion You are now ready to get start your journey as an R-enthusiast! "],
["objects-data-types.html", "3 Objects and data types in R 3.1 How it works 3.2 Variables 3.3 Basic data types 3.4 Everything is an object 3.5 Vectors 3.6 Matrices 3.7 Data frames 3.8 Lists 3.9 Exercises", " 3 Objects and data types in R 3.1 How it works You will now start with writing R code in the console and you will explore a first script of R code. Every line of code is interpreted and executed by R and you get a message whether or not your code was correct. The output of your R code is then shown in the console. R makes use of the # sign to add comments, so that you and others can understand what the R code is about. Just like Twitter! Comments are not run as R code, so they will not influence your result. [Quote from DataCamp’s ‘Introduction to R’ course.] In its most basic form, R can be used as a simple calculator. We illustrate the use of some arithmetic operators in the code below. # use &#39;right click, run line or selection&#39;, of Ctrl+R 10^2+36 [1] 136 3.2 Variables A basic concept in (statistical) programming is called a variable. A variable allows you to store a value (e.g. 4) or an object (e.g. a function description) in R. You can then later use this variable’s name to easily access the value or the object that is stored within this variable. [Quote from DataCamp’s ‘Introduction to R’ course.] # assign value &#39;4&#39; to &#39;a&#39; a &lt;- 4 a [1] 4 # now R remembers what &#39;a&#39; is # calculations with &#39;a&#39; a*5 [1] 20 (a+10)/2 [1] 7 # or give a new value to &#39;a&#39; a &lt;- a+1 a [1] 5 3.3 Basic data types R works with numerous data types. Some of the most basic types to get started are: Decimal values like 4.5 are called numerics. Natural numbers like 4 are called integers. Integers are also numerics. Boolean values (TRUE or FALSE) are called logical. Dates or POSIXct for time based variables. Here, Date stores just a date and POSIXct stores a date and time. Both objects are actually represented as the number of days (Date) or seconds (POSIXct) since January 1, 1970. Text (or string) values are called characters. Note how the quotation marks on the right indicate that “some text” is a character. my_numeric &lt;- 42.5 my_character &lt;- &quot;some text&quot; my_logical &lt;- TRUE my_date &lt;- as.Date(&quot;05/29/2018&quot;, &quot;%m/%d/%Y&quot;) You can check the data type of a variable beforehand. You can do this with the class() function. class(my_numeric) [1] &quot;numeric&quot; # your turn to check the type of &#39;my_character&#39; and &#39;my_logical&#39; and &#39;my_date&#39; 3.4 Everything is an object In R, an analysis is normally broken down into a series of steps. Intermediate results are stored in objects, with minimal output at each step (often none). Instead, the objects are further manipulated to obtain the information required. In fact, the fundamental design principle underlying R (and S) is “everything is an object”. Hence, not only vectors and matrices are objects that can be passed to and returned by functions, but also functions themselves, and even function calls. (Quote from ‘Applied Econometrics in R’, by Kleiber &amp; Zeileis) A variable in R can take on any available data type, or hold any R object. # see all objects stored in R&#39;s memory, where &#39;ls()&#39; is for &#39;List Objects&#39; # and returns a vector of character strings # giving the names of the objects in the specified environment ls() [1] &quot;a&quot; &quot;car_price&quot; &quot;F&quot; &quot;F_educ&quot; &quot;F_so2&quot; [6] &quot;grid&quot; &quot;lm_educ&quot; &quot;lm_so2&quot; &quot;lm0&quot; &quot;lm00&quot; [11] &quot;lm1&quot; &quot;lm1_alt&quot; &quot;lm1_alt_2&quot; &quot;mort_poll&quot; &quot;my_character&quot; [16] &quot;my_date&quot; &quot;my_logical&quot; &quot;my_numeric&quot; &quot;needed_pkgs&quot; &quot;new_pkgs&quot; [21] &quot;p&quot; &quot;p1&quot; &quot;path&quot; &quot;path.car&quot; &quot;path.mort&quot; [26] &quot;x.new&quot; # to remove objects from R&#39;s memory, use rm(a) rm(my_character, my_logical) rm(list=c(&#39;my_date&#39;, &#39;my_numeric&#39;)) rm(list=ls()) 3.5 Vectors Vectors are one-dimension arrays that can hold numeric data, character data, or logical data. In other words, a vector is a simple tool to store data. In R, you create a vector with the combine function c(). You place the vector elements separated by a comma between the parentheses. (Quote from DataCamp’s ‘Introduction to R course’) Vectors are key! Operations are applied to each element of the vector automatically, there is no need to loop through the vector. # To combine elements into a vector, use c(): c(1, 2, 3, 4) [1] 1 2 3 4 # or 1:10 [1] 1 2 3 4 5 6 7 8 9 10 # or seq(from=0, to=10, by=0.5) [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 [16] 7.5 8.0 8.5 9.0 9.5 10.0 # create a variable x x &lt;- 1:20 x [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 x[6:9] [1] 6 7 8 9 x[c(2, 5, 13)] [1] 2 5 13 # or xx &lt;- c(0, 3:5, 20, 0) xx [1] 0 3 4 5 20 0 xx[2:3] [1] 3 4 length(xx) [1] 6 # but c(.) can also concatenate other things than numbers family &lt;- c(&quot;Katrien&quot;, &quot;Jan&quot;, &quot;Leen&quot;) family [1] &quot;Katrien&quot; &quot;Jan&quot; &quot;Leen&quot; family[2] [1] &quot;Jan&quot; str(family) # str() displays the structure of an R object in compact way chr [1:3] &quot;Katrien&quot; &quot;Jan&quot; &quot;Leen&quot; class(family) [1] &quot;character&quot; You can give a name to the elements of a vector with the names() function. Here is how it works my_vector &lt;- c(&quot;Katrien Antonio&quot;, &quot;teacher&quot;) names(my_vector) &lt;- c(&quot;Name&quot;, &quot;Profession&quot;) my_vector Name Profession &quot;Katrien Antonio&quot; &quot;teacher&quot; 3.6 Matrices In R, a matrix is a collection of elements of the same data type (numeric, character, or logical) arranged into a fixed number of rows and columns. Since you are only working with rows and columns, a matrix is called two-dimensional. You can construct a matrix in R with the matrix() function. (Quote from DataCamp’s ‘Introduction to R course’) # a 3x4 matrix, filled with 1,2,..., 12 matrix(1:12, 3, 4, byrow = TRUE) [,1] [,2] [,3] [,4] [1,] 1 2 3 4 [2,] 5 6 7 8 [3,] 9 10 11 12 matrix(1:12, byrow = TRUE, nrow=3) [,1] [,2] [,3] [,4] [1,] 1 2 3 4 [2,] 5 6 7 8 [3,] 9 10 11 12 # hmmm, check help on &#39;matrix&#39; ? matrix # one way of creating matrices is to bind vectors together cbind(1:2, 6:9) # by columns [,1] [,2] [1,] 1 6 [2,] 2 7 [3,] 1 8 [4,] 2 9 rbind(1:3, -(1:3)) # by rows [,1] [,2] [,3] [1,] 1 2 3 [2,] -1 -2 -3 # create matrix object &#39;m&#39; m &lt;- matrix(1:12, 3, 4) m [,1] [,2] [,3] [,4] [1,] 1 4 7 10 [2,] 2 5 8 11 [3,] 3 6 9 12 m[1,4] # extract an element [1] 10 m[,2] # extract a column [1] 4 5 6 nrow(m);ncol(m);dim(m) # useful stuff [1] 3 [1] 4 [1] 3 4 # another example m &lt;- cbind(a = 1:3, b = letters[1:3]) m a b [1,] &quot;1&quot; &quot;a&quot; [2,] &quot;2&quot; &quot;b&quot; [3,] &quot;3&quot; &quot;c&quot; # ask help, what is the built-in &#39;letters&#39;? ? letters 3.7 Data frames Most data sets you will be working with will be stored as data frames. A data frame has the variables of a data set as columns and the observations as rows. This will be a familiar concept for those coming from different statistical software packages such as SAS or SPSS. First, you will look at a ‘classic’ data set from the datasets package that comes with the base R installation. The mtcars (Motor Trend Car Road Tests) data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models). (Quote from DataCamp’s ‘Introduction to R course’) mtcars str(mtcars) &#39;data.frame&#39;: 32 obs. of 11 variables: $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... $ disp: num 160 160 108 258 360 ... $ hp : num 110 110 93 110 175 105 245 62 95 123 ... $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... $ wt : num 2.62 2.88 2.32 3.21 3.44 ... $ qsec: num 16.5 17 18.6 19.4 17 ... $ vs : num 0 0 1 1 0 1 0 1 1 1 ... $ am : num 1 1 1 0 0 0 0 0 0 0 ... $ gear: num 4 4 4 3 3 3 3 4 4 4 ... $ carb: num 4 4 1 1 2 1 4 2 2 4 ... head(mtcars) mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 tail(mtcars) mpg cyl disp hp drat wt qsec vs am gear carb Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.7 0 1 5 2 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.9 1 1 5 2 Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.5 0 1 5 4 Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.5 0 1 5 6 Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.6 0 1 5 8 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.6 1 1 4 2 Since using built-in data sets is not even half the fun of creating your own data sets, you will now work with your own personally created data set. (Quote from DataCamp’s ‘Introduction to R course’) t &lt;- data.frame(x = c(11, 12, 7), y = c(19, 20, 21), z = c(10, 9, 7)) t$x [1] 11 12 7 t[[&quot;x&quot;]] [1] 11 12 7 # quick scan of the object &#39;t&#39; summary(t) x y z Min. : 7.0 Min. :19.0 Min. : 7.00 1st Qu.: 9.0 1st Qu.:19.5 1st Qu.: 8.00 Median :11.0 Median :20.0 Median : 9.00 Mean :10.0 Mean :20.0 Mean : 8.67 3rd Qu.:11.5 3rd Qu.:20.5 3rd Qu.: 9.50 Max. :12.0 Max. :21.0 Max. :10.00 str(t) &#39;data.frame&#39;: 3 obs. of 3 variables: $ x: num 11 12 7 $ y: num 19 20 21 $ z: num 10 9 7 # another way to create the same data frame x &lt;- c(11, 12, 7) y &lt;- c(19, 20, 21) z &lt;- c(10, 9, 7) t &lt;- data.frame(x, y, z) A first data exploration, let’s calculate the mean of the variable z in data frame t! mean(t$z) [1] 8.667 mean(z) # does not work, why not? [1] 8.667 attach(t) # but... The following objects are masked _by_ .GlobalEnv: x, y, z mean(z) [1] 8.667 detach(t) # does the job # or, avoid &quot;attach(.)&quot; and &quot;detach(.)&quot; with(t, mean(z)) [1] 8.667 More on data frames # this does not work # t &lt;- data.frame(x = c(11,12), y = c(19,20,21), z = c(10,9,7)) # but you _can_ do t &lt;- data.frame(x = c(11, 12, NA), y = c(19, 20, 21), z = c(10, 9, 7)) # data frame with different types of information b &lt;- data.frame(x = c(11, 12, NA), y = c(&quot;me&quot;, &quot;you&quot;, &quot;everyone&quot;)) str(b) &#39;data.frame&#39;: 3 obs. of 2 variables: $ x: num 11 12 NA $ y: Factor w/ 3 levels &quot;everyone&quot;,&quot;me&quot;,..: 2 3 1 # hey there! &#39;y&#39; should not be factor, but character variable b$y &lt;- as.character(b$y) str(b) &#39;data.frame&#39;: 3 obs. of 2 variables: $ x: num 11 12 NA $ y: chr &quot;me&quot; &quot;you&quot; &quot;everyone&quot; So, you here you encountered a factor variable. The term factor refers to a statistical data type used to store categorical variables. The difference between a categorical variable and a continuous variable is that a categorical variable can belong to a limited number of categories. A continuous variable, on the other hand, can correspond to an infinite number of values. (Quote from DataCamp’s ‘Introduction to R course’) 3.8 Lists A list in R allows you to gather a variety of objects under one name (that is, the name of the list) in an ordered way. These objects can be matrices, vectors, data frames, even other lists, etc. It is not even required that these objects are related to each other in any way. You could say that a list is some kind super data type: you can store practically any piece of information in it! (Quote from DataCamp’s ‘Introduction to R course’) # a first example of a list L &lt;- list(one = 1, two = c(1, 2), five = seq(1, 4, length=5), six = c(&quot;Katrien&quot;, &quot;Jan&quot;)) names(L) [1] &quot;one&quot; &quot;two&quot; &quot;five&quot; &quot;six&quot; summary(L) Length Class Mode one 1 -none- numeric two 2 -none- numeric five 5 -none- numeric six 2 -none- character class(L) [1] &quot;list&quot; str(L) List of 4 $ one : num 1 $ two : num [1:2] 1 2 $ five: num [1:5] 1 1.75 2.5 3.25 4 $ six : chr [1:2] &quot;Katrien&quot; &quot;Jan&quot; # list within a list # a list containing: a sample from a N(0,1), plus some markup # list within list mylist &lt;- list(sample = rnorm(5), family = &quot;normal distribution&quot;, parameters = list(mean = 0, sd = 1)) mylist $sample [1] -1.24274 0.08772 0.40050 0.58066 -0.14351 $family [1] &quot;normal distribution&quot; $parameters $parameters$mean [1] 0 $parameters$sd [1] 1 str(mylist) List of 3 $ sample : num [1:5] -1.2427 0.0877 0.4005 0.5807 -0.1435 $ family : chr &quot;normal distribution&quot; $ parameters:List of 2 ..$ mean: num 0 ..$ sd : num 1 # now check mylist[[1]] [1] -1.24274 0.08772 0.40050 0.58066 -0.14351 mylist$sample [1] -1.24274 0.08772 0.40050 0.58066 -0.14351 mylist$parameters $mean [1] 0 $sd [1] 1 mylist$parameters$mean [1] 0 3.9 Exercises Learning check Explore the objects and data types in R. Create a vector fav_music with the names of your favourite artists. Create a vector num_records with the number of records you have in your collection of each of those artists. Create vector num_concerts with the number of times you attended a concert of these artists. Put everything together in a data frame, assign the name my_music to this data frame and change the labels of the information stored in the columns to artist, records and concerts. Extract the variable num_records from the data frame my_music. Calculate the total number of records in your collection (for the defined set of artists). Check the structure of the data frame, ask for a summary. "],
["started-with-data.html", "4 Getting started with data in R 4.1 Importing data 4.2 Basic data handling steps 4.3 Exploratory Data Analysis (EDA) 4.4 Exercises", " 4 Getting started with data in R 4.1 Importing data Importing data into R to start your analyses-it should be the easiest step. Unfortunately, this is almost never the case. Data come in all sorts of formats, ranging from CSV and text files and statistical software files to databases and HTML data. Knowing which approach to use is key to getting started with the actual analysis. (Quote from DataCamp’s ‘Importing Data in R (Part 1)’ course) # what is the current working directory? getwd() # which files are currently stored in my working directory? dir() Specify the path where the data are stored. # where are my data files? path &lt;- file.path(&#39;data&#39;) # how to find a path name on your computer? # file.choose() 4.1.1 Importing a .csv file with read.csv() The utils package, which is automatically loaded in your R session on startup, can import CSV files with the read.csv() function. You will now load a data set on swimming pools in Brisbane, Australia (source: data.gov.au). The file contains the column names in the first row. It uses a comma to separate values within rows. (Quote and example from DataCamp’s ‘Importing Data in R (Part 1)’ course) path.pools &lt;- file.path(path, &quot;swimming_pools.csv&quot;) pools &lt;- read.csv(path.pools) str(pools) &#39;data.frame&#39;: 20 obs. of 4 variables: $ Name : Factor w/ 20 levels &quot;Acacia Ridge Leisure Centre&quot;,..: 1 2 3 4 5 6 19 7 8 9 ... $ Address : Factor w/ 20 levels &quot;1 Fairlead Crescent, Manly&quot;,..: 5 20 18 10 9 11 6 15 12 17 ... $ Latitude : num -27.6 -27.6 -27.6 -27.5 -27.4 ... $ Longitude: num 153 153 153 153 153 ... With stringsAsFactors, you can tell R whether it should convert strings in the flat file to factors. pools &lt;- read.csv(path.pools, stringsAsFactors = FALSE) str(pools) &#39;data.frame&#39;: 20 obs. of 4 variables: $ Name : chr &quot;Acacia Ridge Leisure Centre&quot; &quot;Bellbowrie Pool&quot; &quot;Carole Park&quot; &quot;Centenary Pool (inner City)&quot; ... $ Address : chr &quot;1391 Beaudesert Road, Acacia Ridge&quot; &quot;Sugarwood Street, Bellbowrie&quot; &quot;Cnr Boundary Road and Waterford Road Wacol&quot; &quot;400 Gregory Terrace, Spring Hill&quot; ... $ Latitude : num -27.6 -27.6 -27.6 -27.5 -27.4 ... $ Longitude: num 153 153 153 153 153 ... 4.1.2 Importing a .txt file: the Danish fire insurance data read.table() is the most basic importing function; you can specify tons of different arguments in this function. The header argument defaults to FALSE and the sep argument is “” by default. (Quote from DataCamp’s ‘Importing Data in R (Part 1)’ course) path.fire &lt;- file.path(path, &quot;danish.txt&quot;) danish &lt;- read.table(path.fire, header = TRUE) head(danish) # use the argument &#39;n&#39; to display less/more records Date Loss.in.DKM 1 01/03/1980 1.684 2 01/04/1980 2.094 3 01/05/1980 1.733 4 01/07/1980 1.780 5 01/07/1980 4.612 6 01/10/1980 8.725 tail(danish) Date Loss.in.DKM 2162 12/24/1990 1.238 2163 12/27/1990 1.115 2164 12/30/1990 1.403 2165 12/30/1990 4.868 2166 12/30/1990 1.073 2167 12/31/1990 4.125 str(danish) &#39;data.frame&#39;: 2167 obs. of 2 variables: $ Date : Factor w/ 1645 levels &quot;01/01/1981&quot;,&quot;01/01/1984&quot;,..: 11 18 23 31 31 50 50 80 80 92 ... $ Loss.in.DKM: num 1.68 2.09 1.73 1.78 4.61 ... names(danish) [1] &quot;Date&quot; &quot;Loss.in.DKM&quot; dim(danish) [1] 2167 2 What goes wrong when importing the Danish fire insurance losses stored in danish.txt? You can specify the column names and also the column types or column classes of the resulting data frame. You can do this by setting the col.names and the colClasses argument to a vector of strings. (Quote from DataCamp’s ‘Importing Data in R (Part 1)’ course) path.hotdogs &lt;- file.path(path, &quot;hotdogs.txt&quot;) hotdogs &lt;- read.table(path.hotdogs, header = FALSE, col.names = c(&quot;type&quot;, &quot;calories&quot;, &quot;sodium&quot;)) # display structure of hotdogs str(hotdogs) &#39;data.frame&#39;: 54 obs. of 3 variables: $ type : Factor w/ 3 levels &quot;Beef&quot;,&quot;Meat&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... $ calories: int 186 181 176 149 184 190 158 139 175 148 ... $ sodium : int 495 477 425 322 482 587 370 322 479 375 ... # edit the colClasses argument to import the data correctly: hotdogs2 hotdogs2 &lt;- read.table(path.hotdogs, header = FALSE, col.names = c(&quot;type&quot;, &quot;calories&quot;, &quot;sodium&quot;), colClasses = c(&quot;factor&quot;, &quot;NULL&quot;, &quot;numeric&quot;)) # display structure of hotdogs2 str(hotdogs2) &#39;data.frame&#39;: 54 obs. of 2 variables: $ type : Factor w/ 3 levels &quot;Beef&quot;,&quot;Meat&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... $ sodium: num 495 477 425 322 482 587 370 322 479 375 ... What happened? What is the effect of specifying one of the colClasses as NULL? Now fix the column types in the danish data frame. danish$Date &lt;- as.Date(danish$Date, &quot;%m/%d/%Y&quot;) str(danish) &#39;data.frame&#39;: 2167 obs. of 2 variables: $ Date : Date, format: &quot;1980-01-03&quot; &quot;1980-01-04&quot; ... $ Loss.in.DKM: num 1.68 2.09 1.73 1.78 4.61 ... Or you can try to fix this directly when importing the danish.txt. path.fire &lt;- file.path(path, &quot;danish.txt&quot;) danish &lt;- read.table(path.fire, header = TRUE, colClasses = c(&quot;Date&quot;, &quot;numeric&quot;)) However, this requires some extra effort … setAs(&quot;character&quot;,&quot;myDate&quot;, function(from) as.Date(from, format=&quot;%m/%d/%Y&quot;) ) in method for &#39;coerce&#39; with signature &#39;&quot;character&quot;,&quot;myDate&quot;&#39;: no definition for class &quot;myDate&quot; danish2 &lt;- read.table(path.fire, header = TRUE, colClasses = c(&quot;myDate&quot;, &quot;numeric&quot;)) str(danish2) &#39;data.frame&#39;: 2167 obs. of 2 variables: $ Date : Date, format: &quot;1980-01-03&quot; &quot;1980-01-04&quot; ... $ Loss.in.DKM: num 1.68 2.09 1.73 1.78 4.61 ... 4.1.3 Importing a .csv file: policy data policy.path &lt;- file.path(path, &quot;policy.csv&quot;) policy &lt;- read.table(policy.path, header=TRUE, sep=&quot;;&quot;) head(policy) numeropol debut_pol fin_pol freq_paiement langue type_prof alimentation 1 3 14/09/1995 24/04/1996 mensuel F Technicien Végétarien 2 3 25/04/1996 23/12/1996 mensuel F Technicien Végétarien 3 6 1/03/1995 27/02/1996 annuel A Technicien Carnivore 4 6 1/03/1996 14/01/1997 annuel A Technicien Carnivore 5 6 15/01/1997 31/01/1997 annuel A Technicien Carnivore 6 6 1/02/1997 28/02/1997 annuel A Technicien Carnivore type_territoire utilisation presence_alarme marque_voiture sexe cout1 1 Urbain Travail-quotidien non VOLKSWAGEN F NA 2 Urbain Travail-quotidien non VOLKSWAGEN F NA 3 Urbain Travail-occasionnel oui NISSAN M 279.6 4 Urbain Travail-occasionnel oui NISSAN M NA 5 Urbain Travail-occasionnel oui NISSAN M NA 6 Urbain Travail-occasionnel oui NISSAN M NA cout2 cout3 cout4 nbsin exposition cout age duree_permis annee_vehicule 1 NA NA NA 0 0.61096 NA 29 10 1989 2 NA NA NA 0 0.66301 NA 30 11 1989 3 NA NA NA 1 0.99452 279.6 42 21 1994 4 NA NA NA 0 0.87397 NA 43 22 1994 5 NA NA NA 0 0.04384 NA 44 23 1994 6 NA NA NA 0 0.07397 NA 44 23 1994 tail(policy) numeropol debut_pol fin_pol freq_paiement langue type_prof 39070 88942 31/03/2003 30/03/2004 mensuel A Actuaire 39071 88945 21/03/2003 20/03/2004 annuel A Technicien 39072 88972 18/03/2003 17/03/2004 mensuel F Technicien 39073 88981 19/03/2003 18/03/2004 mensuel A Technicien 39074 88986 28/02/2004 27/02/2005 mensuel A Médecin 39075 89009 24/03/2003 23/03/2004 mensuel A Technicien alimentation type_territoire utilisation presence_alarme 39070 Carnivore Urbain Travail-occasionnel oui 39071 Végétalien Urbain Travail-occasionnel oui 39072 Végétarien Semi-urbain Travail-quotidien non 39073 Végétalien Urbain Travail-occasionnel oui 39074 Carnivore Urbain Travail-quotidien oui 39075 Végétarien Urbain Travail-occasionnel non marque_voiture sexe cout1 cout2 cout3 cout4 nbsin exposition cout age 39070 BMW M NA NA NA NA 0 1 NA 45 39071 TOYOTA M NA NA NA NA 0 1 NA 24 39072 PEUGEOT M NA NA NA NA 0 1 NA 58 39073 SUZUKI F NA NA NA NA 0 1 NA 23 39074 FIAT M NA NA NA NA 0 1 NA 41 39075 TOYOTA M NA NA NA NA 0 1 NA 58 duree_permis annee_vehicule 39070 30 1989 39071 5 2000 39072 33 2003 39073 5 1998 39074 19 1989 39075 37 2003 str(policy) &#39;data.frame&#39;: 39075 obs. of 22 variables: $ numeropol : int 3 3 6 6 6 6 6 6 6 6 ... $ debut_pol : Factor w/ 2956 levels &quot;1/01/1996&quot;,&quot;1/01/1997&quot;,..: 554 1681 17 18 587 10 19 20 21 22 ... $ fin_pol : Factor w/ 3093 levels &quot;1/01/1996&quot;,&quot;1/01/1997&quot;,..: 1652 1620 1931 506 2419 2033 1637 2035 2132 2037 ... $ freq_paiement : Factor w/ 2 levels &quot;annuel&quot;,&quot;mensuel&quot;: 2 2 1 1 1 1 1 1 1 1 ... $ langue : Factor w/ 2 levels &quot;A&quot;,&quot;F&quot;: 2 2 1 1 1 1 1 1 1 1 ... $ type_prof : Factor w/ 10 levels &quot;Actuaire&quot;,&quot;Autre&quot;,..: 10 10 10 10 10 10 10 10 10 10 ... $ alimentation : Factor w/ 3 levels &quot;Carnivore&quot;,&quot;Végétalien&quot;,..: 3 3 1 1 1 1 1 1 1 1 ... $ type_territoire: Factor w/ 3 levels &quot;Rural&quot;,&quot;Semi-urbain&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... $ utilisation : Factor w/ 3 levels &quot;Loisir&quot;,&quot;Travail-occasionnel&quot;,..: 3 3 2 2 2 2 2 2 2 2 ... $ presence_alarme: Factor w/ 2 levels &quot;non&quot;,&quot;oui&quot;: 1 1 2 2 2 2 2 2 2 2 ... $ marque_voiture : Factor w/ 31 levels &quot;ALFAROMEO&quot;,&quot;AUDI&quot;,..: 30 30 19 19 19 19 19 19 19 19 ... $ sexe : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 2 2 2 2 2 2 2 2 ... $ cout1 : num NA NA 280 NA NA ... $ cout2 : num NA NA NA NA NA NA NA NA NA NA ... $ cout3 : num NA NA NA NA NA NA NA NA NA NA ... $ cout4 : num NA NA NA NA NA NA NA NA NA NA ... $ nbsin : int 0 0 1 0 0 0 0 0 0 0 ... $ exposition : num 0.611 0.663 0.9945 0.874 0.0438 ... $ cout : num NA NA 280 NA NA ... $ age : int 29 30 42 43 44 44 44 45 46 47 ... $ duree_permis : int 10 11 21 22 23 23 23 24 25 26 ... $ annee_vehicule : int 1989 1989 1994 1994 1994 1994 1994 1998 1998 1998 ... names(policy) [1] &quot;numeropol&quot; &quot;debut_pol&quot; &quot;fin_pol&quot; &quot;freq_paiement&quot; [5] &quot;langue&quot; &quot;type_prof&quot; &quot;alimentation&quot; &quot;type_territoire&quot; [9] &quot;utilisation&quot; &quot;presence_alarme&quot; &quot;marque_voiture&quot; &quot;sexe&quot; [13] &quot;cout1&quot; &quot;cout2&quot; &quot;cout3&quot; &quot;cout4&quot; [17] &quot;nbsin&quot; &quot;exposition&quot; &quot;cout&quot; &quot;age&quot; [21] &quot;duree_permis&quot; &quot;annee_vehicule&quot; dim(policy) [1] 39075 22 4.1.4 Importing a .sas7bdat file #install.packages(&#39;sas7bdat&#39;) library(sas7bdat) path.severity &lt;- file.path(path, &quot;severity.sas7bdat&quot;) severity &lt;- read.sas7bdat(path.severity) head(severity) policyId claimId rc deductible claimAmount 1 6e+05 9e+05 35306 1200 35306 2 6e+05 9e+05 19773 50 19773 3 6e+05 9e+05 41639 100 41639 4 6e+05 9e+05 10649 50 10649 5 6e+05 9e+05 20479 50 20479 6 6e+05 9e+05 9853 50 9853 tail(severity) policyId claimId rc deductible claimAmount 19282 612853 919300 NaN 50 151 19283 612854 919301 1587 300 1587 19284 612854 919302 NaN 300 574 19285 612855 919303 NaN 50 323 19286 612856 919304 1287 1200 1287 19287 612857 919305 1910 1200 1910 str(severity) &#39;data.frame&#39;: 19287 obs. of 5 variables: $ policyId : num 6e+05 6e+05 6e+05 6e+05 6e+05 ... $ claimId : num 9e+05 9e+05 9e+05 9e+05 9e+05 ... $ rc : num 35306 19773 41639 10649 20479 ... $ deductible : num 1200 50 100 50 50 50 50 50 50 50 ... $ claimAmount: num 35306 19773 41639 10649 20479 ... - attr(*, &quot;pkg.version&quot;)= chr &quot;0.5&quot; - attr(*, &quot;column.info&quot;)=List of 5 ..$ :List of 11 .. ..$ name : chr &quot;policyId&quot; .. ..$ offset: int 0 .. ..$ length: int 8 .. ..$ type : chr &quot;numeric&quot; .. ..$ format: chr &quot;BEST&quot; .. ..$ fhdr : int 0 .. ..$ foff : int 44 .. ..$ flen : int 4 .. ..$ lhdr : int 0 .. ..$ loff : int 0 .. ..$ llen : int 0 ..$ :List of 11 .. ..$ name : chr &quot;claimId&quot; .. ..$ offset: int 8 .. ..$ length: int 8 .. ..$ type : chr &quot;numeric&quot; .. ..$ format: chr &quot;BEST&quot; .. ..$ fhdr : int 0 .. ..$ foff : int 56 .. ..$ flen : int 4 .. ..$ lhdr : int 0 .. ..$ loff : int 0 .. ..$ llen : int 0 ..$ :List of 11 .. ..$ name : chr &quot;rc&quot; .. ..$ offset: int 16 .. ..$ length: int 8 .. ..$ type : chr &quot;numeric&quot; .. ..$ format: chr &quot;BEST&quot; .. ..$ fhdr : int 0 .. ..$ foff : int 64 .. ..$ flen : int 4 .. ..$ lhdr : int 0 .. ..$ loff : int 0 .. ..$ llen : int 0 ..$ :List of 11 .. ..$ name : chr &quot;deductible&quot; .. ..$ offset: int 24 .. ..$ length: int 8 .. ..$ type : chr &quot;numeric&quot; .. ..$ format: chr &quot;BEST&quot; .. ..$ fhdr : int 0 .. ..$ foff : int 80 .. ..$ flen : int 4 .. ..$ lhdr : int 0 .. ..$ loff : int 0 .. ..$ llen : int 0 ..$ :List of 11 .. ..$ name : chr &quot;claimAmount&quot; .. ..$ offset: int 32 .. ..$ length: int 8 .. ..$ type : chr &quot;numeric&quot; .. ..$ format: chr &quot;COMMA&quot; .. ..$ fhdr : int 0 .. ..$ foff : int 96 .. ..$ flen : int 5 .. ..$ lhdr : int 0 .. ..$ loff : int 0 .. ..$ llen : int 0 - attr(*, &quot;date.created&quot;)= POSIXct, format: &quot;1960-01-01&quot; - attr(*, &quot;date.modified&quot;)= POSIXct, format: &quot;1960-01-01&quot; - attr(*, &quot;SAS.release&quot;)= chr &quot;9.0401M1&quot; - attr(*, &quot;SAS.host&quot;)= chr &quot;X64_7PRO&quot; - attr(*, &quot;OS.version&quot;)= chr &quot;&quot; - attr(*, &quot;OS.maker&quot;)= chr &quot;&quot; - attr(*, &quot;OS.name&quot;)= chr &quot;&quot; - attr(*, &quot;endian&quot;)= chr &quot;little&quot; - attr(*, &quot;winunix&quot;)= chr &quot;windows&quot; names(severity) [1] &quot;policyId&quot; &quot;claimId&quot; &quot;rc&quot; &quot;deductible&quot; &quot;claimAmount&quot; dim(severity) [1] 19287 5 4.1.5 Importing a .xlsx file You will import data from Excel using the readxl package (authord by Hacley Wickham and maintained by RStudio). Before you can start importing from Excel, you should find out which sheets are available in the workbook. You can use the excel_sheets() function for this. (Quote and example from DataCamp’s ‘Importing Data in R (Part 1)’ course) # load the readxl package library(readxl) path.urbanpop &lt;- file.path(path, &quot;urbanpop.xlsx&quot;) excel_sheets(path.urbanpop) [1] &quot;1960-1966&quot; &quot;1967-1974&quot; &quot;1975-2011&quot; You can import the Excel file with the read_excel() function. Here is the recipe: pop_1 &lt;- read_excel(path.urbanpop, sheet = 1) pop_2 &lt;- read_excel(path.urbanpop, sheet = 2) pop_3 &lt;- read_excel(path.urbanpop, sheet = 3) str(pop_1) Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 209 obs. of 8 variables: $ country: chr &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... $ 1960 : num 769308 494443 3293999 NA NA ... $ 1961 : num 814923 511803 3515148 13660 8724 ... $ 1962 : num 858522 529439 3739963 14166 9700 ... $ 1963 : num 903914 547377 3973289 14759 10748 ... $ 1964 : num 951226 565572 4220987 15396 11866 ... $ 1965 : num 1000582 583983 4488176 16045 13053 ... $ 1966 : num 1058743 602512 4649105 16693 14217 ... # put pop_1, pop_2 and pop_3 in a list: pop_list pop_list &lt;- list(pop_1, pop_2, pop_3) The object pop_1 is a tibble, an object of tbl_df class (the ‘tibble’) that provides stricter checking and better formatting than the traditional data frame. The main advantage to using a tbl_df over a regular data frame is the printing: tbl objects only print a few rows and all the columns that fit on one screen, describing the rest of it as text. If you want to stick to traditional data frames, you can switch as follows pop_1_df &lt;- as.data.frame(pop_1) str(pop_1_df) &#39;data.frame&#39;: 209 obs. of 8 variables: $ country: chr &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... $ 1960 : num 769308 494443 3293999 NA NA ... $ 1961 : num 814923 511803 3515148 13660 8724 ... $ 1962 : num 858522 529439 3739963 14166 9700 ... $ 1963 : num 903914 547377 3973289 14759 10748 ... $ 1964 : num 951226 565572 4220987 15396 11866 ... $ 1965 : num 1000582 583983 4488176 16045 13053 ... $ 1966 : num 1058743 602512 4649105 16693 14217 ... In the previous demo you generated a list of three Excel sheets that you imported. However, loading in every sheet manually and then merging them in a list can be quite tedious. Luckily, you can automate this with lapply(). (Quote from DataCamp’s ‘Importing Data in R (Part 1)’ course) pop_list &lt;- lapply(excel_sheets(path.urbanpop), read_excel, path = path.urbanpop) str(pop_list) List of 3 $ :Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 209 obs. of 8 variables: ..$ country: chr [1:209] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... ..$ 1960 : num [1:209] 769308 494443 3293999 NA NA ... ..$ 1961 : num [1:209] 814923 511803 3515148 13660 8724 ... ..$ 1962 : num [1:209] 858522 529439 3739963 14166 9700 ... ..$ 1963 : num [1:209] 903914 547377 3973289 14759 10748 ... ..$ 1964 : num [1:209] 951226 565572 4220987 15396 11866 ... ..$ 1965 : num [1:209] 1000582 583983 4488176 16045 13053 ... ..$ 1966 : num [1:209] 1058743 602512 4649105 16693 14217 ... $ :Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 209 obs. of 9 variables: ..$ country: chr [1:209] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... ..$ 1967 : num [1:209] 1119067 621180 4826104 17349 15440 ... ..$ 1968 : num [1:209] 1182159 639964 5017299 17996 16727 ... ..$ 1969 : num [1:209] 1248901 658853 5219332 18619 18088 ... ..$ 1970 : num [1:209] 1319849 677839 5429743 19206 19529 ... ..$ 1971 : num [1:209] 1409001 698932 5619042 19752 20929 ... ..$ 1972 : num [1:209] 1502402 720207 5815734 20263 22406 ... ..$ 1973 : num [1:209] 1598835 741681 6020647 20742 23937 ... ..$ 1974 : num [1:209] 1696445 763385 6235114 21194 25482 ... $ :Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 209 obs. of 38 variables: ..$ country: chr [1:209] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... ..$ 1975 : num [1:209] 1793266 785350 6460138 21632 27019 ... ..$ 1976 : num [1:209] 1905033 807990 6774099 22047 28366 ... ..$ 1977 : num [1:209] 2021308 830959 7102902 22452 29677 ... ..$ 1978 : num [1:209] 2142248 854262 7447728 22899 31037 ... ..$ 1979 : num [1:209] 2268015 877898 7810073 23457 32572 ... ..$ 1980 : num [1:209] 2398775 901884 8190772 24177 34366 ... ..$ 1981 : num [1:209] 2493265 927224 8637724 25173 36356 ... ..$ 1982 : num [1:209] 2590846 952447 9105820 26342 38618 ... ..$ 1983 : num [1:209] 2691612 978476 9591900 27655 40983 ... ..$ 1984 : num [1:209] 2795656 1006613 10091289 29062 43207 ... ..$ 1985 : num [1:209] 2903078 1037541 10600112 30524 45119 ... ..$ 1986 : num [1:209] 3006983 1072365 11101757 32014 46254 ... ..$ 1987 : num [1:209] 3113957 1109954 11609104 33548 47019 ... ..$ 1988 : num [1:209] 3224082 1146633 12122941 35095 47669 ... ..$ 1989 : num [1:209] 3337444 1177286 12645263 36618 48577 ... ..$ 1990 : num [1:209] 3454129 1198293 13177079 38088 49982 ... ..$ 1991 : num [1:209] 3617842 1215445 13708813 39600 51972 ... ..$ 1992 : num [1:209] 3788685 1222544 14248297 41049 54469 ... ..$ 1993 : num [1:209] 3966956 1222812 14789176 42443 57079 ... ..$ 1994 : num [1:209] 4152960 1221364 15322651 43798 59243 ... ..$ 1995 : num [1:209] 4347018 1222234 15842442 45129 60598 ... ..$ 1996 : num [1:209] 4531285 1228760 16395553 46343 60927 ... ..$ 1997 : num [1:209] 4722603 1238090 16935451 47527 60462 ... ..$ 1998 : num [1:209] 4921227 1250366 17469200 48705 59685 ... ..$ 1999 : num [1:209] 5127421 1265195 18007937 49906 59281 ... ..$ 2000 : num [1:209] 5341456 1282223 18560597 51151 59719 ... ..$ 2001 : num [1:209] 5564492 1315690 19198872 52341 61062 ... ..$ 2002 : num [1:209] 5795940 1352278 19854835 53583 63212 ... ..$ 2003 : num [1:209] 6036100 1391143 20529356 54864 65802 ... ..$ 2004 : num [1:209] 6285281 1430918 21222198 56166 68301 ... ..$ 2005 : num [1:209] 6543804 1470488 21932978 57474 70329 ... ..$ 2006 : num [1:209] 6812538 1512255 22625052 58679 71726 ... ..$ 2007 : num [1:209] 7091245 1553491 23335543 59894 72684 ... ..$ 2008 : num [1:209] 7380272 1594351 24061749 61118 73335 ... ..$ 2009 : num [1:209] 7679982 1635262 24799591 62357 73897 ... ..$ 2010 : num [1:209] 7990746 1676545 25545622 63616 74525 ... ..$ 2011 : num [1:209] 8316976 1716842 26216968 64817 75207 ... Apart from path and sheet, there are several other arguments you can specify in read_excel(). One of these arguments is called col_names. By default it is TRUE, denoting whether the first row in the Excel sheets contains the column names. If this is not the case, you can set col_names to FALSE. In this case, R will choose column names for you. You can also choose to set col_names to a character vector with names for each column. (Quote from DataCamp’s ‘Importing Data in R (Part 1)’ course) path.urbanpop_nonames &lt;- file.path(path, &quot;urbanpop_nonames.xlsx&quot;) # Import the the first Excel sheet of urbanpop_nonames.xlsx (R gives names): pop_a pop_a &lt;- read_excel(path.urbanpop_nonames, col_names = FALSE) # Import the the first Excel sheet of urbanpop_nonames.xlsx (specify col_names): pop_b cols &lt;- c(&quot;country&quot;, paste0(&quot;year_&quot;, 1960:1966)) pop_b &lt;- read_excel(path.urbanpop_nonames, col_names = cols) # Print the summary of pop_a summary(pop_a) X__1 X__2 X__3 X__4 Length:209 Min. :3.38e+03 Min. :1.03e+03 Min. :1.09e+03 Class :character 1st Qu.:8.90e+04 1st Qu.:7.06e+04 1st Qu.:7.50e+04 Mode :character Median :5.81e+05 Median :5.70e+05 Median :5.94e+05 Mean :4.99e+06 Mean :4.99e+06 Mean :5.14e+06 3rd Qu.:3.08e+06 3rd Qu.:2.81e+06 3rd Qu.:2.95e+06 Max. :1.26e+08 Max. :1.29e+08 Max. :1.32e+08 NA&#39;s :11 X__5 X__6 X__7 X__8 Min. :1.15e+03 Min. :1.22e+03 Min. :1.28e+03 Min. :1.35e+03 1st Qu.:8.19e+04 1st Qu.:8.50e+04 1st Qu.:8.86e+04 1st Qu.:9.36e+04 Median :6.19e+05 Median :6.45e+05 Median :6.79e+05 Median :7.35e+05 Mean :5.30e+06 Mean :5.47e+06 Mean :5.64e+06 Mean :5.79e+06 3rd Qu.:3.15e+06 3rd Qu.:3.30e+06 3rd Qu.:3.32e+06 3rd Qu.:3.42e+06 Max. :1.35e+08 Max. :1.37e+08 Max. :1.40e+08 Max. :1.42e+08 # Print the summary of pop_b summary(pop_b) country year_1960 year_1961 year_1962 Length:209 Min. :3.38e+03 Min. :1.03e+03 Min. :1.09e+03 Class :character 1st Qu.:8.90e+04 1st Qu.:7.06e+04 1st Qu.:7.50e+04 Mode :character Median :5.81e+05 Median :5.70e+05 Median :5.94e+05 Mean :4.99e+06 Mean :4.99e+06 Mean :5.14e+06 3rd Qu.:3.08e+06 3rd Qu.:2.81e+06 3rd Qu.:2.95e+06 Max. :1.26e+08 Max. :1.29e+08 Max. :1.32e+08 NA&#39;s :11 year_1963 year_1964 year_1965 year_1966 Min. :1.15e+03 Min. :1.22e+03 Min. :1.28e+03 Min. :1.35e+03 1st Qu.:8.19e+04 1st Qu.:8.50e+04 1st Qu.:8.86e+04 1st Qu.:9.36e+04 Median :6.19e+05 Median :6.45e+05 Median :6.79e+05 Median :7.35e+05 Mean :5.30e+06 Mean :5.47e+06 Mean :5.64e+06 Mean :5.79e+06 3rd Qu.:3.15e+06 3rd Qu.:3.30e+06 3rd Qu.:3.32e+06 3rd Qu.:3.42e+06 Max. :1.35e+08 Max. :1.37e+08 Max. :1.40e+08 Max. :1.42e+08 In the code printed above paste0 (and also paste) converts its arguments (via as.character) to character strings, and concatenates them (in case of paste separating them by the string given by sep). Many other packages exist to import Excel data, including XLConnect, an Excel Connector for R that provides comprehensive functionality to read, write and format Excel data. See DataCamp’s Importing Data in R (Part 1) course. 4.2 Basic data handling steps You will now learn some basic instructions to handle data in R. You start with basic instructions (from base R) for data handling. More on data wrangling follows in Chapter 6. Useful functions from base are subset, sort, order, merge, cbind and rbind. Manipulating the data typically consumes a lot of effort. This often requires repeated operations on different sections of the data, in a ‘split-apply-combine’ way of working. Let’s illustrate all of this below. Some of the examples that follow are taken from Michael Clark’s `An introduction to R’. 4.2.1 Subsetting The data set state.x77 is available from the package datasets. Some data sets related to the 50 states of the United States of America are available and state.x77 is a matrix with 50 rows and 8 columns giving a bunch of statistics in the respective columns. states &lt;- data.frame(state.x77) str(states) &#39;data.frame&#39;: 50 obs. of 8 variables: $ Population: num 3615 365 2212 2110 21198 ... $ Income : num 3624 6315 4530 3378 5114 ... $ Illiteracy: num 2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ... $ Life.Exp : num 69 69.3 70.5 70.7 71.7 ... $ Murder : num 15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ... $ HS.Grad : num 41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ... $ Frost : num 20 152 15 65 20 166 139 103 11 60 ... $ Area : num 50708 566432 113417 51945 156361 ... names(states) [1] &quot;Population&quot; &quot;Income&quot; &quot;Illiteracy&quot; &quot;Life.Exp&quot; &quot;Murder&quot; [6] &quot;HS.Grad&quot; &quot;Frost&quot; &quot;Area&quot; dim(states) [1] 50 8 head(states) Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area Alabama 3615 3624 2.1 69.05 15.1 41.3 20 50708 Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 Arizona 2212 4530 1.8 70.55 7.8 58.1 15 113417 Arkansas 2110 3378 1.9 70.66 10.1 39.9 65 51945 California 21198 5114 1.1 71.71 10.3 62.6 20 156361 Colorado 2541 4884 0.7 72.06 6.8 63.9 166 103766 states[14, ] Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area Indiana 5313 4458 0.7 70.88 7.1 52.9 122 36097 states[3, 6] [1] 58.1 states[, &#39;Frost&#39;] [1] 20 152 15 65 20 166 139 103 11 60 0 126 127 122 140 114 95 12 161 [20] 101 103 125 160 50 108 155 139 188 174 115 120 82 80 186 124 82 44 126 [39] 127 65 172 70 35 137 168 85 32 100 149 173 states$Frost [1] 20 152 15 65 20 166 139 103 11 60 0 126 127 122 140 114 95 12 161 [20] 101 103 125 160 50 108 155 139 188 174 115 120 82 80 186 124 82 44 126 [39] 127 65 172 70 35 137 168 85 32 100 149 173 You will also use the data stored in state.region, a factor giving the region (Northeast, South, North Central, West) that each state belongs to. state.region [1] South West West South West [6] West Northeast South South South [11] West West North Central North Central North Central [16] North Central South South Northeast South [21] Northeast North Central North Central South North Central [26] West North Central West Northeast Northeast [31] West Northeast South North Central North Central [36] South West Northeast Northeast South [41] North Central South South West Northeast [46] South West South North Central West Levels: Northeast South North Central West length(state.region) [1] 50 # select those states that are in the south of the US mysubset &lt;- subset(states, state.region == &quot;South&quot;) # subset a selection of variables str(states) &#39;data.frame&#39;: 50 obs. of 8 variables: $ Population: num 3615 365 2212 2110 21198 ... $ Income : num 3624 6315 4530 3378 5114 ... $ Illiteracy: num 2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ... $ Life.Exp : num 69 69.3 70.5 70.7 71.7 ... $ Murder : num 15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ... $ HS.Grad : num 41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ... $ Frost : num 20 152 15 65 20 166 139 103 11 60 ... $ Area : num 50708 566432 113417 51945 156361 ... mysubset &lt;- states[, c(1:2, 7:8)] mysubset &lt;- states[, c(&quot;Population&quot;, &quot;Income&quot;, &quot;Frost&quot;, &quot;Area&quot;)] 4.2.2 Find minimum or maximum You will now use the function which.min(), that returns the index of the smallest value in a vector. which.max() works in a similar way. Using the information stored in states, which states in the US have the smallest, respectively highest, population density? least_pop &lt;- which.min(states$Population) states[least_pop, ] Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 most_pop &lt;- which.max(states$Population) states[most_pop, ] Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area California 21198 5114 1.1 71.71 10.3 62.6 20 156361 4.2.3 Sorting You will now sort the states based on their population. sort(states$Population) [1] 365 376 472 579 590 637 681 746 812 813 868 931 [13] 1058 1144 1203 1544 1799 2110 2212 2280 2284 2341 2541 2715 [25] 2816 2861 3100 3387 3559 3615 3806 3921 4122 4173 4589 4767 [37] 4931 4981 5313 5441 5814 7333 8277 9111 10735 11197 11860 12237 [49] 18076 21198 # not what we want, thus sort1.states &lt;- states[order(states$Population), ] head(sort1.states) Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 Wyoming 376 4566 0.6 70.29 6.9 62.9 173 97203 Vermont 472 3907 0.6 71.64 5.5 57.1 168 9267 Delaware 579 4809 0.9 70.06 6.2 54.6 103 1982 Nevada 590 5149 0.5 69.03 11.5 65.2 188 109889 North Dakota 637 5087 0.8 72.78 1.4 50.3 186 69273 # sort by two variables sort2.states &lt;- states[order(states$Illiteracy, states$Income), ] head(sort2.states) Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area South Dakota 681 4167 0.5 72.08 1.7 53.3 172 75955 Iowa 2861 4628 0.5 72.56 2.3 59.0 140 55941 Nevada 590 5149 0.5 69.03 11.5 65.2 188 109889 Vermont 472 3907 0.6 71.64 5.5 57.1 168 9267 Utah 1203 4022 0.6 72.90 4.5 67.3 137 82096 Idaho 813 4119 0.6 71.87 5.3 59.5 126 82677 # sort in reverse order sort3.states &lt;- states[order(-states$Life.Exp), ] head(sort3.states) Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area Hawaii 868 4963 1.9 73.60 6.2 61.9 0 6425 Minnesota 3921 4675 0.6 72.96 2.3 57.6 160 79289 Utah 1203 4022 0.6 72.90 4.5 67.3 137 82096 North Dakota 637 5087 0.8 72.78 1.4 50.3 186 69273 Nebraska 1544 4508 0.6 72.60 2.9 59.3 139 76483 Kansas 2280 4669 0.6 72.58 4.5 59.9 114 81787 4.2.4 Merging You move on with instructions to add columns to existing data frames. mydat &lt;- data.frame(id = factor(1:12), group = factor(rep(1:2, each = 3))) str(mydat) &#39;data.frame&#39;: 12 obs. of 2 variables: $ id : Factor w/ 12 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ group: Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 1 1 1 2 2 2 1 1 1 2 ... head(mydat) id group 1 1 1 2 2 1 3 3 1 4 4 2 5 5 2 6 6 2 x &lt;- rnorm(12) y &lt;- sample(70:100, 12) x2 &lt;- rnorm(12) # add a column mydat$grade &lt;- y head(mydat) id group grade 1 1 1 74 2 2 1 75 3 3 1 89 4 4 2 70 5 5 2 90 6 6 2 85 Now, you’ll merge different data frames by adding new columns to an existing data frame. df &lt;- data.frame(id = mydat$id, y) head(df) id y 1 1 74 2 2 75 3 3 89 4 4 70 5 5 90 6 6 85 mydat2 &lt;- merge(mydat, df, by = &quot;id&quot;, sort = F) # using merge head(mydat2) id group grade y 1 1 1 74 74 2 2 1 75 75 3 3 1 89 89 4 4 2 70 70 5 5 2 90 90 6 6 2 85 85 mydat3 &lt;- cbind(mydat, x) # using cbind() head(mydat3) id group grade x 1 1 1 74 -0.500581 2 2 1 75 0.476077 3 3 1 89 -0.227544 4 4 2 70 0.870813 5 5 2 90 -0.007889 6 6 2 85 -0.730312 Now, you’ll add rows to an existing data frame. # add rows df &lt;- data.frame(id = factor(13:24), group = factor(rep(1:2, e = 3)), grade = sample(y)) df id group grade 1 13 1 86 2 14 1 90 3 15 1 74 4 16 2 71 5 17 2 75 6 18 2 85 7 19 1 96 8 20 1 81 9 21 1 70 10 22 2 89 11 23 2 72 12 24 2 87 mydat2 &lt;- rbind(mydat, df) mydat2 id group grade 1 1 1 74 2 2 1 75 3 3 1 89 4 4 2 70 5 5 2 90 6 6 2 85 7 7 1 87 8 8 1 96 9 9 1 72 10 10 2 71 11 11 2 81 12 12 2 86 13 13 1 86 14 14 1 90 15 15 1 74 16 16 2 71 17 17 2 75 18 18 2 85 19 19 1 96 20 20 1 81 21 21 1 70 22 22 2 89 23 23 2 72 24 24 2 87 4.2.5 Aggregate People experienced with SQL generally want to run an aggregation and group by as one of their first tasks with R. aggregate() splits the data into subsets, computes summary statistics for each, and returns the result in a convenient form. You will work with diamonds, a data set in the ggplot2 package containing the prices and other attributes of almost 54,000 diamonds. ggplot2 is a package authored and maintained by Hadley Wickham to `Create Elegant Data Visualisations Using the Grammar of Graphics’. library(ggplot2) head(diamonds) # A tibble: 6 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 # average price for each type of cut aggregate(price ~ cut, diamonds, mean) cut price 1 Fair 4359 2 Good 3929 3 Very Good 3982 4 Premium 4584 5 Ideal 3458 # do a manual check, using `subset()` s &lt;- subset(diamonds, cut == &#39;Fair&#39;) mean(s$price) [1] 4359 # add arguments to the function called aggregate(price ~ cut, diamonds, mean, na.rm=TRUE) cut price 1 Fair 4359 2 Good 3929 3 Very Good 3982 4 Premium 4584 5 Ideal 3458 Here the argument na.rm of the function mean takes a logical value indicating whether NA values should be stripped before the computation proceeds. And some more useful illustrations: s &lt;- aggregate(price ~ cut, diamonds, mean) s cut price 1 Fair 4359 2 Good 3929 3 Very Good 3982 4 Premium 4584 5 Ideal 3458 dd &lt;- merge(diamonds, s, by=&quot;cut&quot;, sort = &quot;FALSE&quot;) head(dd) cut carat color clarity depth table price.x x y z price.y 1 Ideal 0.23 E SI2 61.5 55.0 326 3.95 3.98 2.43 3458 2 Ideal 1.02 E SI1 61.1 56.0 4675 6.51 6.45 3.96 3458 3 Ideal 1.05 F SI2 60.9 56.0 4675 6.64 6.56 4.02 3458 4 Ideal 0.38 I VS1 61.5 53.9 703 4.66 4.70 2.89 3458 5 Ideal 0.30 E VS1 62.5 54.0 703 4.27 4.32 2.69 3458 6 Ideal 1.22 J SI2 61.2 57.0 4676 6.86 6.90 4.21 3458 head(diamonds) # A tibble: 6 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 head(subset(dd, cut == &quot;Very Good&quot;)) cut carat color clarity depth table price.x x y z price.y 40249 Very Good 1.01 F VS2 61.6 57 6773 6.39 6.46 3.96 3982 40250 Very Good 0.30 F VVS1 61.8 55 783 4.32 4.35 2.68 3982 40251 Very Good 1.15 I SI2 62.0 58 4405 6.69 6.73 4.16 3982 40252 Very Good 0.90 G SI2 62.0 59 3445 6.14 6.19 3.82 3982 40253 Very Good 1.00 H VVS2 62.6 56 6249 6.36 6.39 3.99 3982 40254 Very Good 0.36 E VS2 62.1 59 789 4.55 4.59 2.84 3982 # change name of the column names(dd)[names(dd) == &#39;price.y&#39;] &lt;- &#39;average price&#39; # add additional grouping variable aggregate(price ~ cut + color, diamonds, mean, na.rm=TRUE) cut color price 1 Fair D 4291 2 Good D 3405 3 Very Good D 3470 4 Premium D 3631 5 Ideal D 2629 6 Fair E 3682 7 Good E 3424 8 Very Good E 3215 9 Premium E 3539 10 Ideal E 2598 11 Fair F 3827 12 Good F 3496 13 Very Good F 3779 14 Premium F 4325 15 Ideal F 3375 16 Fair G 4239 17 Good G 4123 18 Very Good G 3873 19 Premium G 4501 20 Ideal G 3721 21 Fair H 5136 22 Good H 4276 23 Very Good H 4535 24 Premium H 5217 25 Ideal H 3889 26 Fair I 4685 27 Good I 5079 28 Very Good I 5256 29 Premium I 5946 30 Ideal I 4452 31 Fair J 4976 32 Good J 4574 33 Very Good J 5104 34 Premium J 6295 35 Ideal J 4918 # store results in an object res &lt;- aggregate(price ~ cut + color, diamonds, mean, na.rm=TRUE) str(res) &#39;data.frame&#39;: 35 obs. of 3 variables: $ cut : Ord.factor w/ 5 levels &quot;Fair&quot;&lt;&quot;Good&quot;&lt;..: 1 2 3 4 5 1 2 3 4 5 ... $ color: Ord.factor w/ 7 levels &quot;D&quot;&lt;&quot;E&quot;&lt;&quot;F&quot;&lt;&quot;G&quot;&lt;..: 1 1 1 1 1 2 2 2 2 2 ... $ price: num 4291 3405 3470 3631 2629 ... head(res) cut color price 1 Fair D 4291 2 Good D 3405 3 Very Good D 3470 4 Premium D 3631 5 Ideal D 2629 6 Fair E 3682 # aggregate two variables, combine with &#39;cbind&#39; aggregate(cbind(price, carat) ~ cut, diamonds, mean) cut price carat 1 Fair 4359 1.0461 2 Good 3929 0.8492 3 Very Good 3982 0.8064 4 Premium 4584 0.8920 5 Ideal 3458 0.7028 aggregate(cbind(price, carat) ~ cut + color, diamonds, mean) cut color price carat 1 Fair D 4291 0.9201 2 Good D 3405 0.7445 3 Very Good D 3470 0.6964 4 Premium D 3631 0.7215 5 Ideal D 2629 0.5658 6 Fair E 3682 0.8566 7 Good E 3424 0.7451 8 Very Good E 3215 0.6763 9 Premium E 3539 0.7177 10 Ideal E 2598 0.5784 11 Fair F 3827 0.9047 12 Good F 3496 0.7759 13 Very Good F 3779 0.7410 14 Premium F 4325 0.8270 15 Ideal F 3375 0.6558 16 Fair G 4239 1.0238 17 Good G 4123 0.8509 18 Very Good G 3873 0.7668 19 Premium G 4501 0.8415 20 Ideal G 3721 0.7007 21 Fair H 5136 1.2192 22 Good H 4276 0.9147 23 Very Good H 4535 0.9159 24 Premium H 5217 1.0164 25 Ideal H 3889 0.7995 26 Fair I 4685 1.1981 27 Good I 5079 1.0572 28 Very Good I 5256 1.0470 29 Premium I 5946 1.1449 30 Ideal I 4452 0.9130 31 Fair J 4976 1.3412 32 Good J 4574 1.0995 33 Very Good J 5104 1.1332 34 Premium J 6295 1.2931 35 Ideal J 4918 1.0636 4.3 Exploratory Data Analysis (EDA) EDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. During the initial phases of EDA you should feel free to investigate every idea that occurs to you. Some of these ideas will pan out, and some will be dead ends. As your exploration continues, you will home in on a few particularly productive areas that you’ll eventually write up and communicate to others. (Quote from (Grolemund and Wickham 2016)) 4.3.1 Exploring a numerical variable You will work with the CPS1985 data from the AER package that accompanies (Kleiber and Zeileis 2008). library(AER) data(&quot;CPS1985&quot;) str(CPS1985) &#39;data.frame&#39;: 534 obs. of 11 variables: $ wage : num 5.1 4.95 6.67 4 7.5 ... $ education : num 8 9 12 12 12 13 10 12 16 12 ... $ experience: num 21 42 1 4 17 9 27 9 11 9 ... $ age : num 35 57 19 22 35 28 43 27 33 27 ... $ ethnicity : Factor w/ 3 levels &quot;cauc&quot;,&quot;hispanic&quot;,..: 2 1 1 1 1 1 1 1 1 1 ... $ region : Factor w/ 2 levels &quot;south&quot;,&quot;other&quot;: 2 2 2 2 2 2 1 2 2 2 ... $ gender : Factor w/ 2 levels &quot;male&quot;,&quot;female&quot;: 2 2 1 1 1 1 1 1 1 1 ... $ occupation: Factor w/ 6 levels &quot;worker&quot;,&quot;technical&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... $ sector : Factor w/ 3 levels &quot;manufacturing&quot;,..: 1 1 1 3 3 3 3 3 1 3 ... $ union : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 2 1 1 1 1 ... $ married : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 1 2 1 1 1 2 1 ... head(CPS1985) wage education experience age ethnicity region gender occupation 1 5.10 8 21 35 hispanic other female worker 1100 4.95 9 42 57 cauc other female worker 2 6.67 12 1 19 cauc other male worker 3 4.00 12 4 22 cauc other male worker 4 7.50 12 17 35 cauc other male worker 5 13.07 13 9 28 cauc other male worker sector union married 1 manufacturing no yes 1100 manufacturing no yes 2 manufacturing no no 3 other no no 4 other no yes 5 other yes no summary(CPS1985$wage) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.00 5.25 7.78 9.02 11.25 44.50 # attach the data set; R knows where to find the variables attach(CPS1985) summary(wage) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.00 5.25 7.78 9.02 11.25 44.50 is.numeric(wage) [1] TRUE mean(wage) [1] 9.024 median(wage) [1] 7.78 fivenum(wage) # Tukey&#39;s five number summary [1] 1.00 5.25 7.78 11.25 44.50 min(wage) [1] 1 max(wage) [1] 44.5 var(wage) [1] 26.41 sd(wage) [1] 5.139 hist(wage, freq = FALSE) hist(log(wage), freq=FALSE, nclass=20, col=&quot;pink&quot;) lines(density(log(wage)), col=4) detach(CPS1985) 4.3.2 Exploring a categorical (or: factor) variable attach(CPS1985) str(occupation) # factor variable with 6 levels Factor w/ 6 levels &quot;worker&quot;,&quot;technical&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... summary(occupation) worker technical services office sales management 156 105 83 97 38 55 detach(CPS1985) To compactify the output you will rename levels 2 and 6 of the factor variable ‘occupation’. levels(CPS1985$occupation)[c(2, 6)] &lt;- c(&quot;techn&quot;, &quot;mgmt&quot;) summary(CPS1985$occupation) worker techn services office sales mgmt 156 105 83 97 38 55 Now you’ll learn how to construct summary tables, barplots and pie charts in R. attach(CPS1985) tab &lt;- table(occupation) tab occupation worker techn services office sales mgmt 156 105 83 97 38 55 prop.table(tab) occupation worker techn services office sales mgmt 0.29213 0.19663 0.15543 0.18165 0.07116 0.10300 barplot(tab) pie(tab) pie(tab,col = gray(seq(0.4, 1.0, length = 6))) detach(CPS1985) 4.3.3 Exploring two categorical (or: factor) variables attach(CPS1985) table(gender, occupation) occupation gender worker techn services office sales mgmt male 126 53 34 21 21 34 female 30 52 49 76 17 21 prop.table(table(gender, occupation)) occupation gender worker techn services office sales mgmt male 0.23596 0.09925 0.06367 0.03933 0.03933 0.06367 female 0.05618 0.09738 0.09176 0.14232 0.03184 0.03933 prop.table(table(gender, occupation), 2) occupation gender worker techn services office sales mgmt male 0.8077 0.5048 0.4096 0.2165 0.5526 0.6182 female 0.1923 0.4952 0.5904 0.7835 0.4474 0.3818 # use mosaic plot plot(gender ~ occupation, data = CPS1985) detach(CPS1985) 4.3.4 Exploring one numerical and one categorical variable attach(CPS1985) # here: apply &#39;mean(.)&#39; to &#39;log(wage)&#39; by &#39;gender&#39; tapply(wage, gender, mean) male female 9.995 7.879 options(digits=5) tapply(log(wage), list(gender, occupation), mean) worker techn services office sales mgmt male 2.1004 2.4466 1.8296 1.9553 2.1411 2.4475 female 1.6679 2.3075 1.7017 1.9311 1.5794 2.2293 detach(CPS1985) # let&#39;s check these results # use subset(.) to extract part of the data s &lt;- subset(CPS1985, select=c(gender, occupation, wage)) s1 &lt;- subset(s, gender == &quot;female&quot; &amp; occupation == &quot;techn&quot;) mean(log(s1$wage)) [1] 2.3075 Now you’ll build an appropriate visualization tool. attach(CPS1985) # see e.g. http://www.r-bloggers.com/box-plot-with-r-tutorial/ boxplot(log(wage) ~ gender) boxplot(log(wage) ~ gender + occupation, col=&quot;light blue&quot;) boxplot(log(wage) ~ gender + occupation, col=&quot;light blue&quot;, las=2) # make it a nice graph .pardefault &lt;- par(no.readonly = T) # to store the default settings of par(.) boxplot(log(wage) ~ gender + occupation, col=&quot;light blue&quot;, las=2, par(mar = c(12, 5, 4, 2) + 0.1)) par(.pardefault) detach(CPS1985) 4.4 Exercises Learning check Import the data set na.txt that is available in the folder ‘data’ that comes with the book. Use read.table and interpret the resulting data frame. Do you detect any problems (wrt missing values, strange observations)? Check for missing values using the is.na funtion applied to a variable from the na data set. If so, try solving those using the arguments of the read.table function. [Hint: check the argument na.strings] Check again for missing values. Make sure female is a factor variable (with two levels). Count the number of missing values per variable. (An exercise taken from (Kleiber and Zeileis 2008)) “PARADE” is the Sunday newspaper magazine supplementing the Sunday or weekend edition of some 500 daily newspapers in the United States of America. An important yearly feature is an article providing information on some 120150 “randomly” selected US citizens, indicating their profession, hometown and state, and their yearly earnings. The Parade2005 (in library AER) data contain the 2005 version, amended by a variable indicating celebrity status (motivated by substantial oversampling of celebrities in these data). For the Parade2005 data: Load the data Parade2005 from the AER package, use data(&quot;Parade2005&quot;) to make the data accessible. Determine the mean earnings in California. Determine the number of individuals residing in Idaho. Determine the mean and the median earnings of celebrities. Obtain boxplots of log(earnings) stratified by celebrity Plot the density of log(earnings), use density. "],
["data-viz.html", "5 Visualizing data in R 5.1 Basic plot instructions 5.2 More fancy plots 5.3 Exercises", " 5 Visualizing data in R You’ll now continue with building insightful graphics in R. 5.1 Basic plot instructions 5.1.1 Scatterplot Your starting point is the construction of a scatterplot. You’ll work with the Journals data from the AER package. # load the &#39;Journals&#39; data set in the AER package data(&quot;Journals&quot;) # scan the data head(Journals) title APEL Asian-Pacific Economic Literature SAJoEH South African Journal of Economic History CE Computational Economics MEPiTE MOCT-MOST Economic Policy in Transitional Economics JoSE Journal of Socio-Economics LabEc Labour Economics publisher society price pages charpp citations foundingyear APEL Blackwell no 123 440 3822 21 1986 SAJoEH So Afr ec history assn no 20 309 1782 22 1986 CE Kluwer no 443 567 2924 22 1987 MEPiTE Kluwer no 276 520 3234 22 1991 JoSE Elsevier no 295 791 3024 24 1972 LabEc Elsevier no 344 609 2967 24 1994 subs field APEL 14 General SAJoEH 59 Economic History CE 17 Specialized MEPiTE 2 Area Studies JoSE 96 Interdisciplinary LabEc 15 Labor names(Journals) [1] &quot;title&quot; &quot;publisher&quot; &quot;society&quot; &quot;price&quot; &quot;pages&quot; [6] &quot;charpp&quot; &quot;citations&quot; &quot;foundingyear&quot; &quot;subs&quot; &quot;field&quot; # e.g. get variable &#39;price&#39; Journals$price [1] 123 20 443 276 295 344 90 242 226 262 279 165 242 905 355 [16] 375 135 171 284 242 371 115 355 355 835 223 172 62 191 411 [31] 274 130 100 80 235 392 410 464 650 558 317 495 535 123 717 [46] 481 54 379 168 82 355 95 240 448 255 448 392 475 85 108 [61] 394 336 565 255 165 99 203 318 476 473 186 170 824 805 132 [76] 50 424 130 90 805 96 448 130 595 474 410 395 437 270 265 [91] 899 133 262 506 1140 211 799 760 442 296 272 45 614 436 481 [106] 95 357 280 142 710 490 870 1147 743 759 36 224 82 160 1170 [121] 90 742 575 163 175 120 590 2120 205 128 1539 346 1046 97 686 [136] 914 85 206 1154 45 1146 95 138 115 640 122 110 923 1000 1234 [151] 1492 810 90 177 74 113 145 590 1154 1450 1431 47 45 47 81 [166] 1010 334 190 180 1893 1400 301 1339 90 310 226 148 159 178 47 summary(Journals$price) Min. 1st Qu. Median Mean 3rd Qu. Max. 20 134 282 418 541 2120 # focus on price of journal per citation Journals$citeprice &lt;- Journals$price/Journals$citations Now you’ll construct a scatterplot of the number of subscriptions versus the price per citation. attach(Journals) plot(log(subs), log(citeprice)) rug(log(subs)) # adds ticks, thus visualizing the marginal distributions of # the variables, along one or both axes of an existing plot. rug(log(citeprice), side = 2) detach(Journals) # avoid &quot;attach()&quot; and &quot;detach()&quot; plot(log(subs) ~ log(citeprice), data = Journals) R has many plotting options that allow you to flex a graph. For example, pch for the plotting character, col for the color of the plotting characters, xlim and ylim to adjust the limits on the x- and y-axis of the scatterplot. plot(log(citeprice)~log(subs), data = Journals, pch = 19, col = &quot;blue&quot;, xlim = c(0, 8), ylim = c(-7, 4), main = &quot;Library subscriptions&quot;) rug(log(Journals$subs)) rug(log(Journals$citeprice), side=2) # subset data, look at journal entitled &quot;Econometrica&quot; journal &lt;- &quot;Econometrica&quot; journal_info &lt;- subset(Journals, title==journal) x.val &lt;- log(journal_info$subs) y.val &lt;- log(journal_info$citeprice) text(x.val, y.val, journal, pos=2) 5.1.2 Saving the scatterplot as a pdf It is often very useful to directly store your customized graph as a pdf (with appropriate dimensions) in a particular directory on your machine. path &lt;- file.path(&#39;C:/Users/u0043788/Dropbox/PE Introduction to R/graphs&#39;) graph.path &lt;- file.path(path, &quot;myfile.pdf&quot;) pdf(graph.path, height = 5, width = 6) plot(log(citeprice)~log(subs), data = Journals, pch = 19, col = &quot;blue&quot;, xlim = c(0, 8), ylim = c(-7, 4), main = &quot;Library subscriptions&quot;) rug(log(Journals$subs)) rug(log(Journals$citeprice),side=2) journal &lt;- &quot;Econometrica&quot; journal_info &lt;- subset(Journals, title==journal) x.val &lt;- log(journal_info$subs) y.val &lt;- log(journal_info$citeprice) text(x.val, y.val, journal, pos=2) dev.off() 5.1.3 The curve() function This function draws a curve corresponding to a function over the interval [from, to]. curve(dnorm, from = -5, to = 5, col = &quot;slategray&quot;, lwd = 3, main = &quot;Density of the standard normal distribution&quot;) text(-5, 0.3, expression(f(x) == frac(1, sigma ~~ sqrt(2*pi)) ~~ e^{-frac((x - mu)^2, 2*sigma^2)}),adj=0) 5.2 More fancy plots R has many dedicated packages for advanced plotting. You will work with two of them in this Section. 5.2.1 Creating graphics with ggplot2 ggplot2 is a package created and maintained by prof. Hadley Wickham, it’s aim is to Create Elegant Data Visualisations Using the Grammar of Graphics. Here is the basic explanation of how ggplot2 works from (Grolemund and Wickham 2016). With ggplot2, you begin a plot with the function ggplot(). ggplot() creates a coordinate system that you can add layers to. The first argument of ggplot() is the dataset to use in the graph. So ggplot(data = mpg) or ggplot(mpg) creates an empty graph. You complete your graph by adding one or more layers to ggplot(). The function geom_point() adds a layer of points to your plot, which creates a scatterplot. ggplot2 comes with many geom functions that each add a different type of layer to a plot. Each geom function in ggplot2 takes a mapping argument. This defines how variables in your dataset are mapped to visual properties. The mapping argument is always paired with aes(), and the \\(x\\) and \\(y\\) arguments of aes() specify which variables to map to the \\(x\\) and \\(y\\) axes. ggplot2 looks for the mapped variable in the data argument, in this case, mpg. library(ggplot2) # use default theme ggplot(data = mtcars, mapping = aes(x = hp, y = mpg)) + geom_point(shape=1, alpha = 1/2)+ geom_smooth() `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; # shorter ggplot(mtcars, aes(x = hp, y = mpg)) + geom_point(shape=1, alpha = 1/2)+ geom_smooth() `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; # use black and white lay-out ggplot(mtcars, aes(x = hp, y = mpg)) + theme_bw() + geom_point(shape=1, alpha = 1/2)+ geom_smooth() `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; You can add a third variable to a two dimensional scatterplot by mapping it to an aesthetic. An aesthetic is a visual property of the objects in your plot. Aesthetics include things like the size, the shape, or the color of your points. You can display a point in different ways by changing the values of its aesthetic properties. ggplot(mtcars, aes(x = hp, y = mpg))+ geom_point(mapping = aes(color = gear)) Or you could have mapped this variable to the alpha aesthetic, which controls the transparency of the points, or the shape of the points. ggplot(mtcars, aes(x = hp, y = mpg))+ geom_point(mapping = aes(alpha = gear)) ggplot(mtcars, aes(x = hp, y = mpg))+ geom_point(mapping = aes(size = gear)) You’ll now construct a boxplot of mpg per cyl using ggplot(). ggplot(mtcars, aes(factor(cyl), mpg))+ geom_boxplot() + geom_jitter() + theme_bw() Another way to code the same example p &lt;- ggplot(mtcars, aes(factor(cyl), mpg)) p + geom_boxplot() + geom_jitter() + theme_bw() 5.2.2 Fancy correlation plots You use the package corrplot to visualize correlations between variables. For more examples, see corrplot. library(corrplot) corrplot 0.84 loaded # get correlation matrix M &lt;- cor(mtcars) str(M) num [1:11, 1:11] 1 -0.852 -0.848 -0.776 0.681 ... - attr(*, &quot;dimnames&quot;)=List of 2 ..$ : chr [1:11] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... ..$ : chr [1:11] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... M mpg cyl disp hp drat wt qsec vs mpg 1.00000 -0.85216 -0.84755 -0.77617 0.681172 -0.86766 0.418684 0.66404 cyl -0.85216 1.00000 0.90203 0.83245 -0.699938 0.78250 -0.591242 -0.81081 disp -0.84755 0.90203 1.00000 0.79095 -0.710214 0.88798 -0.433698 -0.71042 hp -0.77617 0.83245 0.79095 1.00000 -0.448759 0.65875 -0.708223 -0.72310 drat 0.68117 -0.69994 -0.71021 -0.44876 1.000000 -0.71244 0.091205 0.44028 wt -0.86766 0.78250 0.88798 0.65875 -0.712441 1.00000 -0.174716 -0.55492 qsec 0.41868 -0.59124 -0.43370 -0.70822 0.091205 -0.17472 1.000000 0.74454 vs 0.66404 -0.81081 -0.71042 -0.72310 0.440278 -0.55492 0.744535 1.00000 am 0.59983 -0.52261 -0.59123 -0.24320 0.712711 -0.69250 -0.229861 0.16835 gear 0.48028 -0.49269 -0.55557 -0.12570 0.699610 -0.58329 -0.212682 0.20602 carb -0.55093 0.52699 0.39498 0.74981 -0.090790 0.42761 -0.656249 -0.56961 am gear carb mpg 0.599832 0.48028 -0.550925 cyl -0.522607 -0.49269 0.526988 disp -0.591227 -0.55557 0.394977 hp -0.243204 -0.12570 0.749812 drat 0.712711 0.69961 -0.090790 wt -0.692495 -0.58329 0.427606 qsec -0.229861 -0.21268 -0.656249 vs 0.168345 0.20602 -0.569607 am 1.000000 0.79406 0.057534 gear 0.794059 1.00000 0.274073 carb 0.057534 0.27407 1.000000 # visualize the correlation structure corrplot(M, method=&quot;circle&quot;) corrplot(M, method=&quot;square&quot;) corrplot(M, method=&quot;color&quot;) corrplot(M, type=&quot;upper&quot;) corrplot(M, type=&quot;upper&quot;, method=&quot;square&quot;) 5.3 Exercises Learning check Use the Danish fire insurance losses. Plot the arrival of losses over time. Use type= &quot;l&quot; for a line plot, label the \\(x\\) and \\(y\\)-axis, and give the plot a title using main. Do the same with instructions from ggplot2. Get inspiration from R for Data Science and use geom_line() to create the line plot. Use the data set ‘car_price.csv’ available in the documentation. Import the data in R. Explore the data. Make a scatterplot of price versus income, use basic plotting instructions and use ggplot2. Add a smooth line to each of the plots (using lines to add a line to an existing plot and lowess to do scatterplot smoothing and using geom_smooth in the ggplot2 grammar). Use the mpg data set. Work through the following steps. The data contains observations collected by the US Environment Protection Agency on 38 models of car. Explore the data. Plot displ, a car’s engine size, in litres on the \\(x\\)-axis and hwy, on the \\(y\\)-axis, that is the car’s fuel efficiency on the highway, in miles per gallon (mpg). Now do the same but use different colors for the points, based on the class variable in mpg. Add a smooth line. "],
["data-wrangling.html", "6 Data wrangling in R 6.1 Ideas from the tidyverse 6.2 Data science the data.table way 6.3 Exercises", " 6 Data wrangling in R For advanced, and fast, data handling with large R objects and lots of flexibility, two lines of work are available: the RStudio line (with Hadley Wickham) offering the packages from the tidyverse: see tidyverse the data.table line developed by Matt Dowle, see e.g. DataCamp’s course on data.table. Both have a very specific syntax, with a demanding learning curve. 6.1 Ideas from the tidyverse 6.1.1 A tibble instead of a data.frame Within the tidyverse tibbles are a modern take on data frames. They keep the features that have stood the test of time, and drop the features that used to be convenient but are now frustrating (i.e. converting character vectors to factors). (Quote from tibble vignette) You can use tibble to create a new tibble and as_tibble transforms an object (e.g. a data frame) into a tibble. library(ggplot2) diamonds # A tibble: 53,940 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 # ... with 53,930 more rows 6.1.2 Pipes in R Read the story behind the pipe operator in R in this tutorial from DataCamp pipes in R. In R, the pipe operator is %&gt;%. You can think of this operator as being similar to the + in a ggplot2 statement, as introduced in Chapter 5. It takes the output of one statement and makes it the input of the next statement. When describing it, you can think of it as a “THEN”. 6.1.3 Filter observations using filter Here is a first example of using the pipe in R. library(ggplot2) library(dplyr) Attaching package: &#39;dplyr&#39; The following object is masked from &#39;package:car&#39;: recode The following objects are masked from &#39;package:stats&#39;: filter, lag The following objects are masked from &#39;package:base&#39;: intersect, setdiff, setequal, union diamonds %&gt;% filter(cut == &quot;Ideal&quot;) # A tibble: 21,551 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.23 Ideal J VS1 62.8 56 340 3.93 3.9 2.46 3 0.31 Ideal J SI2 62.2 54 344 4.35 4.37 2.71 4 0.3 Ideal I SI2 62 54 348 4.31 4.34 2.68 5 0.33 Ideal I SI2 61.8 55 403 4.49 4.51 2.78 6 0.33 Ideal I SI2 61.2 56 403 4.49 4.5 2.75 7 0.33 Ideal J SI1 61.1 56 403 4.49 4.55 2.76 8 0.23 Ideal G VS1 61.9 54 404 3.93 3.95 2.44 9 0.32 Ideal I SI1 60.9 55 404 4.45 4.48 2.72 10 0.3 Ideal I SI2 61 59 405 4.3 4.33 2.63 # ... with 21,541 more rows The code chunk above will translate to something like “you take the diamonds data, then you subset the data”. This is one of the most powerful things about the tidyverse. In fact, having a standardized chain of processing actions is called “a pipeline”. Here is another example where you now filter diamonds based on two characteristics. diamonds %&gt;% filter(cut == &quot;Ideal&quot; &amp; color == &quot;E&quot;) # A tibble: 3,903 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.26 Ideal E VVS2 62.9 58 554 4.02 4.06 2.54 3 0.7 Ideal E SI1 62.5 57 2757 5.7 5.72 3.57 4 0.59 Ideal E VVS2 62 55 2761 5.38 5.43 3.35 5 0.74 Ideal E SI2 62.2 56 2761 5.8 5.84 3.62 6 0.7 Ideal E VS2 60.7 58 2762 5.73 5.76 3.49 7 0.74 Ideal E SI1 62.3 54 2762 5.8 5.83 3.62 8 0.7 Ideal E SI1 60.9 57 2768 5.73 5.76 3.5 9 0.6 Ideal E VS1 61.7 55 2774 5.41 5.44 3.35 10 0.7 Ideal E SI1 62.7 55 2774 5.68 5.74 3.58 # ... with 3,893 more rows diamonds %&gt;% filter(cut == &quot;Ideal&quot; &amp; color %in% c(&quot;E&quot;, &quot;D&quot;)) # A tibble: 6,737 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.3 Ideal D SI1 62.5 57 552 4.29 4.32 2.69 3 0.3 Ideal D SI1 62.1 56 552 4.3 4.33 2.68 4 0.26 Ideal E VVS2 62.9 58 554 4.02 4.06 2.54 5 0.7 Ideal E SI1 62.5 57 2757 5.7 5.72 3.57 6 0.59 Ideal E VVS2 62 55 2761 5.38 5.43 3.35 7 0.74 Ideal E SI2 62.2 56 2761 5.8 5.84 3.62 8 0.7 Ideal E VS2 60.7 58 2762 5.73 5.76 3.49 9 0.71 Ideal D SI2 62.3 56 2762 5.73 5.69 3.56 10 0.74 Ideal E SI1 62.3 54 2762 5.8 5.83 3.62 # ... with 6,727 more rows 6.1.4 Summarize variables using summarize The code chunk below will translate to something like “you take the diamonds data, then you subset the data and then you calculate mean and standard deviation of these data”. diamonds %&gt;% filter(cut == &quot;Ideal&quot;) %&gt;% summarize(mean = mean(price), std_dev = sd(price)) # A tibble: 1 x 2 mean std_dev &lt;dbl&gt; &lt;dbl&gt; 1 3458. 3808. 6.1.5 Summarize based on groupings of another variable So, here is what you’d like to do. # base R way with aggregate aggregate(price ~ cut, diamonds, mean) cut price 1 Fair 4358.8 2 Good 3928.9 3 Very Good 3981.8 4 Premium 4584.3 5 Ideal 3457.5 How can you do this with the pipe? diamonds %&gt;% group_by(cut) %&gt;% summarize(mean = mean(price)) # A tibble: 5 x 2 cut mean &lt;ord&gt; &lt;dbl&gt; 1 Fair 4359. 2 Good 3929. 3 Very Good 3982. 4 Premium 4584. 5 Ideal 3458. Now you want to group by multiple variables. diamonds %&gt;% group_by(cut, color) %&gt;% summarize(price = mean(price)) # A tibble: 35 x 3 # Groups: cut [?] cut color price &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; 1 Fair D 4291. 2 Fair E 3682. 3 Fair F 3827. 4 Fair G 4239. 5 Fair H 5136. 6 Fair I 4685. 7 Fair J 4976. 8 Good D 3405. 9 Good E 3424. 10 Good F 3496. # ... with 25 more rows Now you want to calculate multiple metrics. diamonds %&gt;% group_by(cut) %&gt;% summarize(price = mean(price), carat = mean(carat)) # A tibble: 5 x 3 cut price carat &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Fair 4359. 1.05 2 Good 3929. 0.849 3 Very Good 3982. 0.806 4 Premium 4584. 0.892 5 Ideal 3458. 0.703 And finally, multiple metrics and multiple grouping variables. diamonds %&gt;% group_by(cut, color) %&gt;% summarize(price = mean(price), carat = mean(carat)) # A tibble: 35 x 4 # Groups: cut [?] cut color price carat &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Fair D 4291. 0.920 2 Fair E 3682. 0.857 3 Fair F 3827. 0.905 4 Fair G 4239. 1.02 5 Fair H 5136. 1.22 6 Fair I 4685. 1.20 7 Fair J 4976. 1.34 8 Good D 3405. 0.745 9 Good E 3424. 0.745 10 Good F 3496. 0.776 # ... with 25 more rows 6.1.6 Joining tibbles Now you want to add the mean price and mean carat per cut to the original tibble. You use the variable cut as the key to identify observations. d &lt;- diamonds %&gt;% group_by(cut) %&gt;% summarize(price = mean(price), carat = mean(carat)) new_diamonds &lt;- diamonds %&gt;% inner_join(d, by = &quot;cut&quot;) View(diamonds) View(new_diamonds) 6.2 Data science the data.table way 6.2.1 Speed junkies love data.table data.table is a package designed for speed junkies. “The R data.table package is rapidly making its name as the number one choice for handling large datasets in R.” It extends and exchanges the functionality of the basic data.frame in R. The syntax is different and you’ll have to get used to it. A data.table cheat sheet is available here. 6.2.2 What is a data.table? Here you see some basic illustrations with the diamonds data. library(data.table) data.table 1.11.6 Latest news: r-datatable.com Attaching package: &#39;data.table&#39; The following objects are masked from &#39;package:dplyr&#39;: between, first, last library(ggplot2) str(diamonds) Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 53940 obs. of 10 variables: $ carat : num 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ... $ cut : Ord.factor w/ 5 levels &quot;Fair&quot;&lt;&quot;Good&quot;&lt;..: 5 4 2 4 2 3 3 3 1 3 ... $ color : Ord.factor w/ 7 levels &quot;D&quot;&lt;&quot;E&quot;&lt;&quot;F&quot;&lt;&quot;G&quot;&lt;..: 2 2 2 6 7 7 6 5 2 5 ... $ clarity: Ord.factor w/ 8 levels &quot;I1&quot;&lt;&quot;SI2&quot;&lt;&quot;SI1&quot;&lt;..: 2 3 5 4 2 6 7 3 4 5 ... $ depth : num 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ... $ table : num 55 61 65 58 58 57 57 55 61 61 ... $ price : int 326 326 327 334 335 336 336 337 337 338 ... $ x : num 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ... $ y : num 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ... $ z : num 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ... diamonds_DT &lt;- data.table(diamonds) diamonds_DT # notice intelligent printing of this DT carat cut color clarity depth table price x y z 1: 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2: 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3: 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4: 0.29 Premium I VS2 62.4 58 334 4.20 4.23 2.63 5: 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 --- 53936: 0.72 Ideal D SI1 60.8 57 2757 5.75 5.76 3.50 53937: 0.72 Good D SI1 63.1 55 2757 5.69 5.75 3.61 53938: 0.70 Very Good D SI1 62.8 60 2757 5.66 5.68 3.56 53939: 0.86 Premium H SI2 61.0 58 2757 6.15 6.12 3.74 53940: 0.75 Ideal D SI2 62.2 55 2757 5.83 5.87 3.64 summary(diamonds_DT$cut) Fair Good Very Good Premium Ideal 1610 4906 12082 13791 21551 6.2.3 Identify keys Instead of using subset from the base R, you will use the setkey to extract the observations you want to have. # key is used to index the data.table and will provide the extra speed setkey(diamonds_DT, cut) tables() NAME NROW NCOL MB COLS KEY 1: diamonds_DT 53,940 10 3 carat,cut,color,clarity,depth,table,... cut Total: 3MB diamonds_DT[J(&quot;Ideal&quot;), ] carat cut color clarity depth table price x y z 1: 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2: 0.23 Ideal J VS1 62.8 56 340 3.93 3.90 2.46 3: 0.31 Ideal J SI2 62.2 54 344 4.35 4.37 2.71 4: 0.30 Ideal I SI2 62.0 54 348 4.31 4.34 2.68 5: 0.33 Ideal I SI2 61.8 55 403 4.49 4.51 2.78 --- 21547: 0.79 Ideal I SI1 61.6 56 2756 5.95 5.97 3.67 21548: 0.71 Ideal E SI1 61.9 56 2756 5.71 5.73 3.54 21549: 0.71 Ideal G VS1 61.4 56 2756 5.76 5.73 3.53 21550: 0.72 Ideal D SI1 60.8 57 2757 5.75 5.76 3.50 21551: 0.75 Ideal D SI2 62.2 55 2757 5.83 5.87 3.64 # more than one column can be set as key setkey(diamonds_DT, cut, color) tables() NAME NROW NCOL MB COLS KEY 1: diamonds_DT 53,940 10 3 carat,cut,color,clarity,depth,table,... cut,color Total: 3MB # access rows according to both keys, use function &#39;J&#39; diamonds_DT[J(&quot;Ideal&quot;, &quot;E&quot;), ] carat cut color clarity depth table price x y z 1: 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2: 0.26 Ideal E VVS2 62.9 58 554 4.02 4.06 2.54 3: 0.70 Ideal E SI1 62.5 57 2757 5.70 5.72 3.57 4: 0.59 Ideal E VVS2 62.0 55 2761 5.38 5.43 3.35 5: 0.74 Ideal E SI2 62.2 56 2761 5.80 5.84 3.62 --- 3899: 0.70 Ideal E SI1 61.7 55 2745 5.71 5.74 3.53 3900: 0.51 Ideal E VVS1 61.9 54 2745 5.17 5.11 3.18 3901: 0.56 Ideal E VVS1 62.1 56 2750 5.28 5.29 3.28 3902: 0.77 Ideal E SI2 62.1 56 2753 5.84 5.86 3.63 3903: 0.71 Ideal E SI1 61.9 56 2756 5.71 5.73 3.54 diamonds_DT[J(&quot;Ideal&quot;, c(&quot;E&quot;, &quot;D&quot;)), ] carat cut color clarity depth table price x y z 1: 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2: 0.26 Ideal E VVS2 62.9 58 554 4.02 4.06 2.54 3: 0.70 Ideal E SI1 62.5 57 2757 5.70 5.72 3.57 4: 0.59 Ideal E VVS2 62.0 55 2761 5.38 5.43 3.35 5: 0.74 Ideal E SI2 62.2 56 2761 5.80 5.84 3.62 --- 6733: 0.51 Ideal D VVS2 61.7 56 2742 5.16 5.14 3.18 6734: 0.51 Ideal D VVS2 61.3 57 2742 5.17 5.14 3.16 6735: 0.81 Ideal D SI1 61.5 57 2748 6.00 6.03 3.70 6736: 0.72 Ideal D SI1 60.8 57 2757 5.75 5.76 3.50 6737: 0.75 Ideal D SI2 62.2 55 2757 5.83 5.87 3.64 # what would be the alternative with base R? subset(diamonds, diamonds$cut==&quot;Ideal&quot; &amp;&amp; diamonds$color==c(&quot;E&quot;, &quot;D&quot;)) # A tibble: 53,940 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 # ... with 53,930 more rows 6.2.4 Alternative and faster ways to aggregate Instead of using aggregate from the base R, you will identify the by variable(s). # base R way with aggregate aggregate(price ~ cut, diamonds, mean) cut price 1 Fair 4358.8 2 Good 3928.9 3 Very Good 3981.8 4 Premium 4584.3 5 Ideal 3457.5 system.time(aggregate(price ~ cut, diamonds, mean)) user system elapsed 0.03 0.00 0.03 # aggregation with data.table # will go faster thanks to indexing diamonds_DT[ , mean(price), by=cut] cut V1 1: Fair 4358.8 2: Good 3928.9 3: Very Good 3981.8 4: Premium 4584.3 5: Ideal 3457.5 system.time(diamonds_DT[ , mean(price), by=cut]) user system elapsed 0.01 0.07 0.08 # give variable names in the create date.table diamonds_DT[ , list(price = mean(price)), by=cut] cut price 1: Fair 4358.8 2: Good 3928.9 3: Very Good 3981.8 4: Premium 4584.3 5: Ideal 3457.5 # aggregate on multiple columns diamonds_DT[ , mean(price), by=list(cut,color)] cut color V1 1: Fair D 4291.1 2: Fair E 3682.3 3: Fair F 3827.0 4: Fair G 4239.3 5: Fair H 5135.7 6: Fair I 4685.4 7: Fair J 4975.7 8: Good D 3405.4 9: Good E 3423.6 10: Good F 3495.8 11: Good G 4123.5 12: Good H 4276.3 13: Good I 5078.5 14: Good J 4574.2 15: Very Good D 3470.5 16: Very Good E 3214.7 17: Very Good F 3778.8 18: Very Good G 3872.8 19: Very Good H 4535.4 20: Very Good I 5255.9 21: Very Good J 5103.5 22: Premium D 3631.3 23: Premium E 3538.9 24: Premium F 4324.9 25: Premium G 4500.7 26: Premium H 5216.7 27: Premium I 5946.2 28: Premium J 6294.6 29: Ideal D 2629.1 30: Ideal E 2597.6 31: Ideal F 3374.9 32: Ideal G 3720.7 33: Ideal H 3889.3 34: Ideal I 4452.0 35: Ideal J 4918.2 cut color V1 # aggregate multiple arguments diamonds_DT[ , list(price = mean(price), carat = mean(carat)), by = cut] cut price carat 1: Fair 4358.8 1.04614 2: Good 3928.9 0.84918 3: Very Good 3981.8 0.80638 4: Premium 4584.3 0.89195 5: Ideal 3457.5 0.70284 diamonds_DT[ , list(price = mean(price), carat = mean(carat), caratSum = sum(carat)), by=cut] cut price carat caratSum 1: Fair 4358.8 1.04614 1684.3 2: Good 3928.9 0.84918 4166.1 3: Very Good 3981.8 0.80638 9742.7 4: Premium 4584.3 0.89195 12301.0 5: Ideal 3457.5 0.70284 15146.8 # multiple metrics and multiple grouping variables diamonds_DT[ , list(price = mean(price), carat = mean(carat)), by = list(cut, color)] cut color price carat 1: Fair D 4291.1 0.92012 2: Fair E 3682.3 0.85661 3: Fair F 3827.0 0.90471 4: Fair G 4239.3 1.02382 5: Fair H 5135.7 1.21917 6: Fair I 4685.4 1.19806 7: Fair J 4975.7 1.34118 8: Good D 3405.4 0.74452 9: Good E 3423.6 0.74513 10: Good F 3495.8 0.77593 11: Good G 4123.5 0.85090 12: Good H 4276.3 0.91473 13: Good I 5078.5 1.05722 14: Good J 4574.2 1.09954 15: Very Good D 3470.5 0.69642 16: Very Good E 3214.7 0.67632 17: Very Good F 3778.8 0.74096 18: Very Good G 3872.8 0.76680 19: Very Good H 4535.4 0.91595 20: Very Good I 5255.9 1.04695 21: Very Good J 5103.5 1.13322 22: Premium D 3631.3 0.72155 23: Premium E 3538.9 0.71774 24: Premium F 4324.9 0.82704 25: Premium G 4500.7 0.84149 26: Premium H 5216.7 1.01645 27: Premium I 5946.2 1.14494 28: Premium J 6294.6 1.29309 29: Ideal D 2629.1 0.56577 30: Ideal E 2597.6 0.57840 31: Ideal F 3374.9 0.65583 32: Ideal G 3720.7 0.70071 33: Ideal H 3889.3 0.79952 34: Ideal I 4452.0 0.91303 35: Ideal J 4918.2 1.06359 cut color price carat 6.2.5 Joining data.tables How to join data.tables? # join two data.tables d &lt;- diamonds_DT[ , list(price = mean(price), carat = mean(carat)), by = cut] d cut price carat 1: Fair 4358.8 1.04614 2: Good 3928.9 0.84918 3: Very Good 3981.8 0.80638 4: Premium 4584.3 0.89195 5: Ideal 3457.5 0.70284 setkey(diamonds_DT, cut) dmerge &lt;- diamonds_DT[d] dmerge carat cut color clarity depth table price x y z i.price 1: 0.75 Fair D SI2 64.6 57 2848 5.74 5.72 3.70 4358.8 2: 0.71 Fair D VS2 56.9 65 2858 5.89 5.84 3.34 4358.8 3: 0.90 Fair D SI2 66.9 57 2885 6.02 5.90 3.99 4358.8 4: 1.00 Fair D SI2 69.3 58 2974 5.96 5.87 4.10 4358.8 5: 1.01 Fair D SI2 64.6 56 3003 6.31 6.24 4.05 4358.8 --- 53936: 0.71 Ideal J SI1 60.6 57 2700 5.78 5.83 3.52 3457.5 53937: 0.81 Ideal J VS2 62.1 56 2708 5.92 5.97 3.69 3457.5 53938: 0.84 Ideal J VS2 61.1 57 2709 6.09 6.12 3.73 3457.5 53939: 0.82 Ideal J VS2 61.6 56 2741 6.00 6.04 3.71 3457.5 53940: 0.83 Ideal J VS2 62.3 55 2742 6.01 6.03 3.75 3457.5 i.carat 1: 1.04614 2: 1.04614 3: 1.04614 4: 1.04614 5: 1.04614 --- 53936: 0.70284 53937: 0.70284 53938: 0.70284 53939: 0.70284 53940: 0.70284 6.3 Exercises Learning check (An exercise taken from (Kleiber and Zeileis 2008)) “PARADE” is the Sunday newspaper magazine supplementing the Sunday or weekend edition of some 500 daily newspapers in the United States of America. An important yearly feature is an article providing information on some 120150 “randomly” selected US citizens, indicating their profession, hometown and state, and their yearly earnings. The Parade2005 (in library AER) data contain the 2005 version, amended by a variable indicating celebrity status (motivated by substantial oversampling of celebrities in these data). For the Parade2005 data and by using %&gt;% answer the following questions. Load the data Parade2005 from the AER package, use data(&quot;Parade2005&quot;) to make the data accessible. Determine the mean earnings in California. Determine the number of individuals residing in Idaho. Determine the mean and the median earnings of celebrities. "],
["probs.html", "7 Working with probability distributions in R 7.1 Discrete distributions 7.2 Continuous distributions 7.3 Exercises", " 7 Working with probability distributions in R In this Section you’ll learn how to work with probability distributions in R. Before you start, it is important to know that for many standard distributions R has 4 crucial functions: Density: e.g. dexp, dgamma, dlnorm Quantile: e.g. qexp, qgamma, qlnorm Cdf: e.g. pexp, pgamma, plnorm Simulation: e.g. rexp, rgamma, rlnorm The parameters of the distribution are then specified in the arguments of these functions. Below are some examples from Katrien’s course on Loss Models at KU Leuven. 7.1 Discrete distributions 7.1.1 The binomial distribution nSim &lt;- 100 p &lt;- 0.3 n &lt;- 6 # generate &#39;nSim&#39; obs. from Bin(n,p) distribution data_binom &lt;- rbinom(nSim, n, p) # calculate mean and variance mean(data_binom) # empirical mean [1] 1.9 var(data_binom) # empirical variance [1] 1.2626 n*p # theoretical mean [1] 1.8 n*p*(1-p) # theoretical variance [1] 1.26 # visualize range &lt;- seq(-1,n,1/1000) plot(ecdf(data_binom)) # ecdf lines(range,pbinom(range, n, p), col = &#39;red&#39;) # cdf par(mfrow=c(1,2)) plot(0:n, dbinom(0:n, n, p), type = &#39;h&#39;) # pdf plot(prop.table(table(data_binom))) par(mfrow=c(1,1)) 7.1.2 The Poisson distribution nSim &lt;- 100 lambda &lt;- 1 # generate &#39;nSim&#39; observations from Poisson(\\lambda) distribution data_pois &lt;- rpois(nSim, lambda) # calculate mean and variance mean(data_pois) # empirical mean [1] 0.99 var(data_pois) # empirical variance [1] 1.1817 lambda # theoretical mean [1] 1 lambda # theoretical variance [1] 1 # visualize range &lt;- seq(0,8, 1/1000) plot(ecdf(data_pois)) # ecdf lines(range,ppois(range, lambda), col = &#39;red&#39;) # cdf par(mfrow=c(1,2)) plot(0:8, dpois(0:8, lambda), type = &#39;h&#39;) # pdf plot(prop.table(table(data_pois))) par(mfrow=c(1,1)) 7.2 Continuous distributions 7.2.1 The normal distribution # evaluate cdf of N(0,1) in 0 pnorm(0, mean=0, sd=1) [1] 0.5 # or shorter pnorm(0, 0, 1) [1] 0.5 # 95% quantile of N(0,1) qnorm(0.95, mean=0, sd=1) [1] 1.6449 # a set of quantiles qnorm(c(0.025, 0.05, 0.5, 0.95, 0.975), 0, 1) [1] -1.9600 -1.6449 0.0000 1.6449 1.9600 # generate observations from N(0,1) x &lt;- rnorm(10000, mean=10, sd=1) # visualize hist(x, probability=TRUE, nclass=55, col=&quot;pink&quot;) curve(dnorm(x, mean=10, sd=1), xlim=range(x), col=&quot;black&quot;,add=TRUE) 7.2.2 The gamma distribution # check parametrization of gamma density in R ? dgamma # grid of points to evaluate the gamma density x &lt;- seq(from = 0, to = 20, by = 0.001) # choose a color palette colors &lt;- c(&quot;#000000&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) # shape and rate parameter combinations shown in the plot shape &lt;- c(1, 2, 3) rate &lt;- c(0.5, 0.5, 0.5) plot(x, dgamma(x, shape = shape[1], rate = rate[1]), type=&#39;l&#39;, xlab =&#39;x&#39;, ylab=&#39;Gamma density&#39;, main=&#39;Effect of the shape parameter on the Gamma density&#39;) for(i in 2:length(shape)){ lines(x, dgamma(x, shape = shape[i], rate = rate[i]), col=colors[i]) } # add a legend legend(&quot;topright&quot;, paste(&quot;shape = &quot;, shape, &quot;, rate = &quot;, rate, sep=&quot;&quot;), col = colors, lty=1) 7.3 Exercises Learning check Generating random numbers, tossing coins. Set your seed to 1 and generate 10 random numbers (between 0 and 1) using runif and save these numbers in an object called random_numbers. Using the function ifelse and the object random_numbers simulate coin tosses. Hint: if random_numbers is bigger than 0.5 then the result is head, otherwise it is tail. Another way of generating random coin tosses is by using the rbinom function. Set the seed again to 1 and simulate with this function 10 coin tosses. Simulate samples from a normal distribution. Imagine a population in which the average height is 1.7m with a standard deviation of 0.1. Using rnorm simulate the height of 100 people and save it in an object called heights. To get an idea of the values in heights apply the function summary to it. What is the probability that a person will be smaller or equal to 1.9m? Use prnorm. What is the probability that a person will be taller or equal to 1.6m? Use pnorm. The waiting time (in minutes) at a doctor’s clinic follows an exponential distribution with a rate parameter of 1/50. Use the function rexp to simulate the waiting time of 30 people at the doctor’s office. What is the probability that a person will wait less than 10 minutes? Use pexp. What is the waiting time average? "],
["functions.html", "8 Writing functions in R 8.1 Conditionals and control flow 8.2 Logical operators 8.3 Loops 8.4 Functions in R 8.5 The apply family 8.6 Exercises", " 8 Writing functions in R 8.1 Conditionals and control flow In this chapter you’ll learn about relational operators to see how R objects compare and logical operators to combine logicals. Next, you’ll use this knowledge to build conditional statements. [Quote from DataCamp’s `Intermediate R’ course] Make sure not to mix up == and =, where the latter is used for assignment and the former checks equality. 3 == (2 + 1) [1] TRUE &quot;intermediate&quot; != &quot;r&quot; [1] TRUE TRUE != FALSE [1] TRUE &quot;Rchitect&quot; != &quot;rchitect&quot; [1] TRUE Now you’ll focus on inequalities. (1 + 2) &gt; 4 [1] FALSE &quot;dog&quot; &lt; &quot;Cats&quot; [1] FALSE TRUE &lt;= FALSE [1] FALSE For string comparison, R determines the greater than relationship based on alphabetical order. Also, keep in mind that TRUE corresponds to 1 in R, and FALSE coerces to 0 behind the scenes. R’s relational operators also work on vectors. katrien &lt;- c(19, 22, 4, 5, 7) katrien &gt; 5 [1] TRUE TRUE FALSE FALSE TRUE jan &lt;- c(34, 55, 76, 25, 4) jan &lt;= 30 [1] FALSE FALSE FALSE TRUE TRUE 8.2 Logical operators Check the following statements TRUE &amp; TRUE [1] TRUE FALSE | TRUE [1] TRUE 5 &lt;= 5 &amp; 2 &lt; 3 [1] TRUE 3 &lt; 4 | 7 &lt; 6 [1] TRUE The logical operators applied to vectors katrien &gt; 5 &amp; jan &lt;= 30 [1] FALSE FALSE FALSE FALSE TRUE The ! operator reverses the result of a logical value. !TRUE [1] FALSE !(5 &gt; 3) [1] FALSE !!FALSE [1] FALSE Time to check out the if statement in R. num_attendees &lt;- 30 if (num_attendees &gt; 5) { print(&quot;You&#39;re popular!&quot;) } [1] &quot;You&#39;re popular!&quot; In the if else statement it is important that the else keyword comes on the same line as the closing bracket of the if part! num_attendees &lt;- 5 if (num_attendees &gt; 5) { print(&quot;You&#39;re popular!&quot;) }else{ print(&quot;You are not so popular!&quot;) } [1] &quot;You are not so popular!&quot; Let’s add a control statement num_attendees &lt;- 5 num_questions &lt;- 3 if (num_attendees &lt;= 5 &amp; num_questions &lt;=2) { print(&quot;Easy workshop&quot;) }else{ print(&quot;Work to do!&quot;) } [1] &quot;Work to do!&quot; 8.3 Loops 8.3.1 The while loop You’ll start with a while loop. todo &lt;- 64 while (todo &gt; 30) { print(&quot;Work harder&quot;) todo &lt;- todo - 7 } [1] &quot;Work harder&quot; [1] &quot;Work harder&quot; [1] &quot;Work harder&quot; [1] &quot;Work harder&quot; [1] &quot;Work harder&quot; todo [1] 29 The example below puts many things together (taken from DataCamp’s `Intermediate R’ course). i &lt;- 1 while (i &lt;= 10) { print(3 * i) if ( (3 * i) %% 8 == 0) { break } i &lt;- i + 1 } [1] 3 [1] 6 [1] 9 [1] 12 [1] 15 [1] 18 [1] 21 [1] 24 8.3.2 The for loop primes &lt;- c(2, 3, 5, 7, 11, 13) # loop version 1 for (p in primes) { print(p) } [1] 2 [1] 3 [1] 5 [1] 7 [1] 11 [1] 13 # loop version 2 for (i in 1:length(primes)) { print(primes[i]) } [1] 2 [1] 3 [1] 5 [1] 7 [1] 11 [1] 13 A loop to count the number of r’s in a given character string. rquote &lt;- &quot;r&#39;s internals are irrefutably intriguing&quot; chars &lt;- strsplit(rquote, split = &quot;&quot;)[[1]] chars [1] &quot;r&quot; &quot;&#39;&quot; &quot;s&quot; &quot; &quot; &quot;i&quot; &quot;n&quot; &quot;t&quot; &quot;e&quot; &quot;r&quot; &quot;n&quot; &quot;a&quot; &quot;l&quot; &quot;s&quot; &quot; &quot; &quot;a&quot; &quot;r&quot; &quot;e&quot; &quot; &quot; &quot;i&quot; [20] &quot;r&quot; &quot;r&quot; &quot;e&quot; &quot;f&quot; &quot;u&quot; &quot;t&quot; &quot;a&quot; &quot;b&quot; &quot;l&quot; &quot;y&quot; &quot; &quot; &quot;i&quot; &quot;n&quot; &quot;t&quot; &quot;r&quot; &quot;i&quot; &quot;g&quot; &quot;u&quot; &quot;i&quot; [39] &quot;n&quot; &quot;g&quot; # Initialize rcount rcount &lt;- 0 # Finish the for loop for (char in chars) { if (char == &quot;r&quot;) { rcount &lt;- rcount + 1 } if (char == &quot;u&quot;) { break } } # Print out rcount rcount [1] 5 8.4 Functions in R 8.4.1 Use a function What is a function? Let’s start from a classic one, the `mean()’. ? mean help(mean) args(mean) function (x, ...) NULL In mean(x, trim = 0, na.rm = FALSE, ...) x is required; if you do not specify it, R will throw an error. trim and na.rm are optional arguments: they have a default value which is used if the arguments are not explicitly specified. [Quote from DataCamp’s ‘Intermediate R’ course.] You will now use the mean function as follows katrien &lt;- c(2, 9, 6, 8, NA) mean(katrien) [1] NA mean(katrien, na.rm = TRUE) [1] 6.25 Functions return objects that can be used elsewhere. As such, you can use a function within function. katrien &lt;- c(2, 9, 6, 8, NA) jan &lt;- c(0, 3, 2, NA, 5) katrien - jan [1] 2 6 4 NA NA mean(abs(katrien - jan), na.rm = TRUE) [1] 4 8.4.2 Write your own function Creating a function in R is basically the assignment of a function object to a variable. That’s why you will use the assignment operator -&gt;. Here is the basic syntax for writing a function in R. my_sqrt &lt;- function(x) { sqrt(x) } # Use the function my_sqrt(12) [1] 3.4641 my_sqrt(16) [1] 4 sum_abs &lt;- function(x, y) { abs(x) + abs(y) } # Use the function sum_abs(-2, 3) [1] 5 You can define default argument values in your own R functions as well. Here you see an example. my_sqrt &lt;- function(x, print_info = TRUE) { y &lt;- sqrt(x) if (print_info) { print(paste(&quot;sqrt&quot;, x, &quot;equals&quot;, y)) } return(y) } # some calls of the function my_sqrt(16) [1] &quot;sqrt 16 equals 4&quot; [1] 4 my_sqrt(16, FALSE) [1] 4 my_sqrt(16, TRUE) [1] &quot;sqrt 16 equals 4&quot; [1] 4 R works in a vectorized way. Check this by calling the function my_sqrt on an input vector. v &lt;- c(16, 25, 36) my_sqrt(v) [1] &quot;sqrt 16 equals 4&quot; &quot;sqrt 25 equals 5&quot; &quot;sqrt 36 equals 6&quot; [1] 4 5 6 8.5 The apply family Whenever you’re using a for loop, you might want to revise your code and see whether you can a member of the apply family instead. [Quote from DataCamp’s `Intermediate R’ course] apply is the first member of this family. It must be applied on a matrix. It takes the following arguments: first argument: matrix you are working with second argument: margin to apply the function over (1 for rows and 2 for columns) third argument: function you want to apply. Here you see how it works. my_matrix &lt;- matrix(1:9, nrow = 3) # sum the rows apply(my_matrix, 1, sum) [1] 12 15 18 # sum the columns apply(my_matrix, 2, sum) [1] 6 15 24 # impute a missing observation in my_matrix my_matrix[2,1] &lt;- NA apply(my_matrix, 1, sum) [1] 12 NA 18 apply(my_matrix, 1, sum, na.rm = TRUE) [1] 12 13 18 You already encountered a first illustration of tapply in Chapter 4. The tapply function is useful when we need to break up a vector into groups defined by some classifying factor, compute a function on the subsets, and return the results in a convenient form. wages &lt;- c(5500, 3500, 6500, 7500) gender &lt;- c(&quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;) region &lt;- c(&quot;North&quot;, &quot;South&quot;, &quot;North&quot;, &quot;South&quot;) salary &lt;- data.frame(wages, gender, region) tapply(salary$wages, salary$gender, mean) F M 4500 7000 tapply(salary$wages, list(salary$gender, salary$region), mean) North South F 5500 3500 M 6500 7500 lapply works by applying a function to each element of a list and returning the results as a list. To return the result of lapply as a vector instead of a list, use sapply. my_list &lt;- list(A = matrix(1:9, 3), B = 1:5, C = matrix(1:4, 2), D = 2) my_list $A [,1] [,2] [,3] [1,] 1 4 7 [2,] 2 5 8 [3,] 3 6 9 $B [1] 1 2 3 4 5 $C [,1] [,2] [1,] 1 3 [2,] 2 4 $D [1] 2 lapply(my_list, sum) $A [1] 45 $B [1] 15 $C [1] 10 $D [1] 2 sapply(my_list, sum) A B C D 45 15 10 2 my_names &lt;- c(&quot;Katrien&quot;, &quot;Jan&quot;, &quot;Leen&quot;) lapply(my_names, nchar) [[1]] [1] 7 [[2]] [1] 3 [[3]] [1] 4 sapply(my_names, nchar) Katrien Jan Leen 7 3 4 mapply applies a function to each element of multiple lists. 8.6 Exercises Learning check Create a function that will return the sum of 2 integers. The function var in R calculates the unbiased variance estimator, given a random sample of data. Write a function variance which returns the biased or unbiased estimate of the variance, depending on the value of the argument bias which can be TRUE or FALSE. By default the function variance should produce the same result as var. Formulas: unbiased = \\(\\frac{1}{n-1}\\sum_i (x_i-\\bar{x})^2\\) and biased = \\(\\frac{1}{n}\\sum_i(x_i-\\bar{x})^2\\) where \\(\\bar{x}=\\frac{1}{n} \\sum_i x_i\\). Create a function that given a vector and an integer will return how many times the integer appears inside the vector. Create a function that given a vector will print by screen the mean and the standard deviation, it will optionally also print the median. "],
["optimization.html", "9 Optimization in R 9.1 Find the root of a function 9.2 Find the maximum of a function 9.3 Do Maximum Likelihood Estimation (MLE)", " 9 Optimization in R Actuaries often write functions (e.g. a likelihood) that have to be optimized. Here you’ll get to know some R functionalities to do optimization. 9.1 Find the root of a function Consider the function \\(f: x \\mapsto x^2-3^{-x}\\). What is the root of this function over the interval \\([0,1]\\)? # in one line of code uniroot(function(x) x^2-3^(-x), lower=0, upper=1) $root [1] 0.68602 $f.root [1] -8.0827e-06 $iter [1] 4 $init.it [1] NA $estim.prec [1] 6.1035e-05 ? uniroot # in more lines of code f &lt;- function(x){ x^2-3^(-x) } # calculate root opt &lt;- uniroot(f, lower=0, upper=1) # check arguments names(opt) [1] &quot;root&quot; &quot;f.root&quot; &quot;iter&quot; &quot;init.it&quot; &quot;estim.prec&quot; # evaluate &#39;f(.)&#39; in the root f(opt$root) [1] -8.0827e-06 # visualize the function range &lt;- seq(-2, 2, by=0.2) plot(range, f(range), type=&quot;l&quot;) points(opt$root, f(opt$root), pch=20) segments(opt$root, -7, opt$root, 0, lty=2) segments(-3, 0, opt$root, 0, lty=2) 9.2 Find the maximum of a function You look for the maximum of the beta density with a given set of parameters. # visualize the density shape1 &lt;- 3 shape2 &lt;- 2 x &lt;- seq(from=0, to=1, by=0.01) curve(dbeta(x,shape1,shape2), xlim=range(x)) opt_beta &lt;- optimize(dbeta, interval=c(0,1), maximum=TRUE, shape1, shape2) points(opt_beta$maximum, opt_beta$objective, pch=20, cex=1.5) segments(opt_beta$maximum, 0, opt_beta$maximum, opt_beta$objective, lty=2) 9.3 Do Maximum Likelihood Estimation (MLE) nsim &lt;- 10000 x &lt;- rgamma(nsim, shape=3, rate=1.5) # calculate log-likelihood f &lt;- function(p,x){ -sum(dgamma(x, shape=p[1], rate=p[2], log=TRUE)) } nlm(f, c(1, 1), x=x) Warning in dgamma(x, shape = p[1], rate = p[2], log = TRUE): NaNs produced Warning in nlm(f, c(1, 1), x = x): NA/Inf replaced by maximum positive value Warning in dgamma(x, shape = p[1], rate = p[2], log = TRUE): NaNs produced Warning in nlm(f, c(1, 1), x = x): NA/Inf replaced by maximum positive value Warning in dgamma(x, shape = p[1], rate = p[2], log = TRUE): NaNs produced Warning in nlm(f, c(1, 1), x = x): NA/Inf replaced by maximum positive value $minimum [1] 14440 $estimate [1] 3.0432 1.5106 $gradient [1] -0.0031518 0.0035787 $code [1] 1 $iterations [1] 14 # same example, now use &#39;optim&#39; optim(c(1, 1), f, x=x) $par [1] 3.0436 1.5109 $value [1] 14440 $counts function gradient 71 NA $convergence [1] 0 $message NULL "],
["lms.html", "10 Linear Regression Models in R 10.1 A simple linear regression model 10.2 A multiple linear regression model 10.3 Exercises", " 10 Linear Regression Models in R Your journey as a model builder in R will start from studying linear models and the use of the lm function. 10.1 A simple linear regression model You analayze Ford dealership data as registered in Milwaukee, September/October 1990. Data on 62 credit card applicants are available, including the car purchase price \\(Y\\) and the applicant’s annual income \\(X\\). Data are available in the .csv file car_price. You first load the data. path &lt;- file.path(&#39;data&#39;) path.car &lt;- file.path(path, &quot;car_price.csv&quot;) car_price &lt;- read.csv(path.car) Then you explore the data. attach(car_price) summary(price) Min. 1st Qu. Median Mean 3rd Qu. Max. 7200 11850 14500 15416 18500 28000 summary(income) Min. 1st Qu. Median Mean 3rd Qu. Max. 15000 30500 40950 45190 55500 102000 # average mean(price) [1] 15416 mean(income) [1] 45190 # standard deviation sd(price) [1] 5040.4 sd(income) [1] 20873 # the 5-th and 95-th percentiles quantile(price, c(0.05, 0.95)) 5% 95% 7830 25900 quantile(income, c(0.05, 0.95)) 5% 95% 18150 85700 # histograms of price and income # density histogram for &#39;price&#39; hist(price, br = 20, xlim = c(5000, 30000), col=&quot;grey&quot;, freq=FALSE) lines(density(price), col=4) # frequency histogram for &#39;income/1000&#39; hist(income/1000, br=10, xlab=&quot;income (in $000&#39;s)&quot;, xlim=c(0, 120), col=&quot;grey&quot;) # scatter plot &#39;income/1000&#39; versus &#39;price&#39; plot(income/1000, price, pch=21, cex=1.2, xlab=&quot;income (in $000&#39;s)&quot;) detach(car_price) Explore the data with ggplot. library(&quot;ggplot2&quot;) ggplot(car_price, aes(x = income/1000, y = price)) + theme_bw() + geom_point(shape=1, alpha = 1/2) + geom_smooth() `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; You will now fit a simple regression model with income as predictor to purchase price. That is: \\[\\begin{eqnarray*} Y_i &amp;=&amp; \\beta_0+\\beta_1 \\cdot x_i +\\epsilon_i, \\end{eqnarray*}\\] where \\(Y_i\\) is the car price for observation \\(i\\), \\(x_i\\) the corresponding income and \\(\\epsilon_i\\) an error term. \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) the slope. Recall that fitting a (simple) linear regression model implies minimizing the residual sum of squares. That is: \\[\\begin{eqnarray*} (\\hat{\\beta}_0,\\ \\hat{\\beta}_1) = \\min_{\\beta_0, \\beta_1} \\sum_{i=1}^n \\left(Y_i - (\\beta_0+\\beta_1 \\cdot x_{i})\\right)^2, \\end{eqnarray*}\\] and the fitted values are then specified as \\(\\hat{y}_i=\\hat{\\beta}_0+\\hat{\\beta}_1\\cdot x_{i}\\). The corresponding residuals are then defined as \\(\\hat{\\epsilon}_i = y_i - \\hat{y}_i\\). You assign the output of the lm function to the object lm1. lm1 &lt;- lm(price ~ income, data = car_price) summary(lm1) Call: lm(formula = price ~ income, data = car_price) Residuals: Min 1Q Median 3Q Max -5365 -1185 -251 1334 6284 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.87e+03 7.50e+02 7.82 9.8e-11 *** income 2.11e-01 1.51e-02 14.01 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2460 on 60 degrees of freedom Multiple R-squared: 0.766, Adjusted R-squared: 0.762 F-statistic: 196 on 1 and 60 DF, p-value: &lt;2e-16 # check attributes of object &#39;lm1&#39; names(lm1) [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; # some useful stuff: &#39;coefficients&#39;, &#39;residuals&#39;, &#39;fitted.values&#39;, &#39;model&#39; lm1$coef (Intercept) income 5866.33390 0.21132 lm1$residuals 1 2 3 4 5 6 7 8 -719.290 1146.822 4329.836 3258.062 -3649.431 -483.570 4144.823 -1206.051 9 10 11 12 13 14 15 16 -664.585 480.710 6284.374 -1040.023 -4383.403 -1262.670 -4421.372 4273.216 17 18 19 20 21 22 23 24 -1373.761 33.583 -1051.346 -1102.387 1392.034 -1836.192 54.316 167.554 25 26 27 28 29 30 31 32 -257.092 -877.824 3124.091 1427.921 367.554 -2355.177 77.679 -2462.670 33 34 35 36 37 38 39 40 1159.977 -274.078 732.384 -2707.167 -2566.817 -1120.805 5129.669 2895.864 41 42 43 44 45 46 47 48 1111.484 2197.613 1971.301 -883.403 3129.669 1744.823 1026.006 142.908 49 50 51 52 53 54 55 56 -3653.428 190.119 -1616.575 -1085.318 -1866.501 -4379.739 1980.943 -40.023 57 58 59 60 61 62 3735.415 -1000.472 -672.079 -245.684 -1087.233 -5364.585 lm1$fitted.values 1 2 3 4 5 6 7 8 9 10 14319.3 9353.2 9670.2 14741.9 11149.4 22983.6 16855.2 12206.1 15164.6 14319.3 11 12 13 14 15 16 17 18 19 20 21715.6 12840.0 11783.4 13262.7 27421.4 10726.8 23173.8 11466.4 13051.3 19602.4 21 22 23 24 25 26 27 28 29 30 14108.0 9036.2 12945.7 10832.4 18757.1 17277.8 15375.9 11572.1 10832.4 16855.2 31 32 33 34 35 36 37 38 39 40 15122.3 13262.7 12840.0 19074.1 15967.6 11107.2 12966.8 14720.8 20870.3 10304.1 41 42 43 44 45 46 47 48 49 50 25688.5 19602.4 12628.7 11783.4 20870.3 16855.2 13474.0 18757.1 26153.4 16009.9 51 52 53 54 55 56 57 58 59 60 9416.6 13685.3 17066.5 19179.7 24019.1 12840.0 15164.6 17700.5 11572.1 12945.7 61 62 15587.2 15164.6 To visualize this linear model fit you can use the built-in plot function, applied to object lm1. # use built-in plot function # you may have noticed that we have used the function plot with all kinds of arguments: # one or two variables, a data frame, and now a linear model fit; # in R jargon plot is a generic function; it checks for the kind of object that you # are plotting and then calls the appropriate (more specialized) function to do the work. plot(lm1) Or you can construct your own plots, e.g. by adding the least squares line to the scatter plot. # add the regression line to the scatter plot plot(car_price$income, car_price$price, pch=21, cex=1.2, xlab = &quot;income&quot;, main = &quot;Simple linear regression&quot;) # add LS line like this abline(lm1, col=&quot;blue&quot;, lwd=2) # or like this abline(lm1$coefficients[1], lm1$coefficients[2]) Similarly, you can illustrate the fit with ggplot. ggplot(car_price, aes(x = income, y = price)) + theme_bw() + geom_point(shape=1, alpha = 1/2) + geom_smooth()+geom_abline(intercept = lm1$coef[1], slope = lm1$coef[2], colour=&quot;red&quot;, size=1.25) `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; The least squares fit minimizes the sum of the squares of the vertical distances between the observed response values and the least squares line (or plane). You now graphically illustrate what vertical distance means. plot(car_price$income, car_price$price, pch=21, cex=1.2, xlab = &quot;income&quot;, main = &quot;Simple linear regression&quot;) abline(lm1, col = &quot;blue&quot;, lwd=2) segments(car_price$income, car_price$price, car_price$income, lm1$fitted.values, lty=1) You now return to the summary(lm1) and try to understand the (rest of the) output that is printed. summary(lm1) Call: lm(formula = price ~ income, data = car_price) Residuals: Min 1Q Median 3Q Max -5365 -1185 -251 1334 6284 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.87e+03 7.50e+02 7.82 9.8e-11 *** income 2.11e-01 1.51e-02 14.01 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2460 on 60 degrees of freedom Multiple R-squared: 0.766, Adjusted R-squared: 0.762 F-statistic: 196 on 1 and 60 DF, p-value: &lt;2e-16 Recall that in a general linear model \\(Y = X\\beta + \\epsilon\\) where \\(E[\\epsilon]=0\\) and \\(\\text{Var}(\\epsilon)=\\sigma^2 I\\) with \\(I\\) the identity matrix, the following estimator is used for the variance of the error terms \\[\\begin{eqnarray*} s^2 &amp;=&amp; \\frac{1}{n-(p+1)}(Y-X\\hat{\\beta})^{&#39;}(Y-X\\hat{\\beta}), \\end{eqnarray*}\\] where \\(\\beta = (\\beta_0, \\beta_1, \\ldots, \\beta_p)^{&#39;}\\). You can recognize this in the output of lm1 as follows error.SS &lt;- sum(lm1$resid^2) error.SS [1] 362851718 sqrt(error.SS/(nrow(car_price)-2)) [1] 2459.2 The proportion of the variability in the data that is explained by the regression model is \\[\\begin{eqnarray*} R^2 &amp;=&amp; \\frac{\\text{Regression SS}}{\\text{Total SS}} \\\\ &amp;=&amp; \\frac{\\sum_{i=1}^n (\\hat{Y}_i-\\bar{Y})^2}{\\sum_{i=1}^n (Y_i-\\bar{Y})^2} \\\\ &amp;=&amp; \\frac{\\sum_{i=1}^n (Y_i-\\bar{Y})^2 - \\sum_{i=1}^n (Y_i-\\hat{Y})^2}{\\sum_{i=1}^n (Y_i-\\bar{Y})^2}. \\end{eqnarray*}\\] attach(car_price) total.SS &lt;- sum((price-mean(price))^2) total.SS [1] 1549743871 error.SS &lt;- sum(lm1$resid^2) error.SS [1] 362851718 # R^2? (total.SS-error.SS)/total.SS [1] 0.76586 detach(car_price) Finally, the output of lm1 displays the result of a so-called \\(F\\) test, constructed as follows: attach(car_price) # anova table in R? anova(lm1) Analysis of Variance Table Response: price Df Sum Sq Mean Sq F value Pr(&gt;F) income 1 1.19e+09 1.19e+09 196 &lt;2e-16 *** Residuals 60 3.63e+08 6.05e+06 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # F-statistic in anova and in output lm1? lm0 &lt;- lm(price ~ 1) error0.SS &lt;- sum(lm0$resid^2) # calculate F-statistic F &lt;- ((anova(lm0)$&quot;Sum Sq&quot;)-(anova(lm1)$&quot;Sum Sq&quot;[2]))/(anova(lm1)$&quot;Mean Sq&quot;[2]) F [1] 196.26 # critical values qf(0.95, 1, 60) [1] 4.0012 1-pf(F, 1, 60) [1] 0 detach(car_price) 10.2 A multiple linear regression model You’ll now move on from simple to multiple linear regression. You model the data by McDonald and Schwing (1973) published in Technometrics. The sampled data consists of variables obtained for year 1960 for 60 Standard Metropolitan Statistical Areas (SMSA) in the US. The goal is to relate mortality in these SMSA to explanatory variables. For each sample area, you have information concerning the age-adjusted mortality rate for all causes, expressed as deaths per 100,000 population. This will be your response variable. The list of explanatory variables is: weather-related variables: prec: average annual precipitation in inches jant: average January temperature in degrees F jult: average July temperature in degrees F humid: annual average % relative humidity at 1 pm scocio-economic characteristics: ovr65: % of 1960 SMSA population aged 65 and older popn: average household size educ : median school years completed by those over 22 hous : % of housing units which are sound and with all facilities educ : median school years completed by those over 22 dens : population per sq mile in urbanized areas, 1960 nonw : % of non-white population in urbanized areas, 1960 wwdrk : % of employed in white collar occupations poor : % of families with income less than $3,000 pollutants: hc : relative pollution potential of hydrocarbons nox : relative pollution potential of oxides of nitrogen so2: relative pollution potential of sulfur dioxides First, you’ll load the data. path &lt;- file.path(&#39;data&#39;) path.mort &lt;- file.path(path, &quot;pollution.csv&quot;) mort_poll &lt;- read.csv(path.mort) Then, you’ll explore the data. attach(mort_poll) The following objects are masked from mort_poll (pos = 17): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 18): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 19): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 20): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 21): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk summary(mort_poll) prec jant jult ovr65 popn Min. :10.0 Min. :12.0 Min. :63.0 Min. : 5.60 Min. :2.92 1st Qu.:32.8 1st Qu.:27.0 1st Qu.:72.0 1st Qu.: 7.67 1st Qu.:3.21 Median :38.0 Median :31.5 Median :74.0 Median : 9.00 Median :3.27 Mean :37.4 Mean :34.0 Mean :74.6 Mean : 8.80 Mean :3.26 3rd Qu.:43.2 3rd Qu.:40.0 3rd Qu.:77.2 3rd Qu.: 9.70 3rd Qu.:3.36 Max. :60.0 Max. :67.0 Max. :85.0 Max. :11.80 Max. :3.53 educ hous dens nonw wwdrk Min. : 9.0 Min. :66.8 Min. :1441 Min. : 0.80 Min. :33.8 1st Qu.:10.4 1st Qu.:78.4 1st Qu.:3104 1st Qu.: 4.95 1st Qu.:43.2 Median :11.1 Median :81.2 Median :3567 Median :10.40 Median :45.5 Mean :11.0 Mean :80.9 Mean :3876 Mean :11.87 Mean :46.1 3rd Qu.:11.5 3rd Qu.:83.6 3rd Qu.:4520 3rd Qu.:15.65 3rd Qu.:49.5 Max. :12.3 Max. :90.7 Max. :9699 Max. :38.50 Max. :59.7 poor hc nox so2 humid Min. : 9.4 Min. : 1.0 Min. : 1.0 Min. : 1.0 Min. :38.0 1st Qu.:12.0 1st Qu.: 7.0 1st Qu.: 4.0 1st Qu.: 11.0 1st Qu.:55.0 Median :13.2 Median : 14.5 Median : 9.0 Median : 30.0 Median :57.0 Mean :14.4 Mean : 37.9 Mean : 22.6 Mean : 53.8 Mean :57.7 3rd Qu.:15.2 3rd Qu.: 30.2 3rd Qu.: 23.8 3rd Qu.: 69.0 3rd Qu.:60.0 Max. :26.4 Max. :648.0 Max. :319.0 Max. :278.0 Max. :73.0 mort Min. : 791 1st Qu.: 898 Median : 944 Mean : 940 3rd Qu.: 983 Max. :1113 # get correlation matrix round(cor(mort_poll), 4) prec jant jult ovr65 popn educ hous dens nonw prec 1.0000 0.0922 0.5033 0.1011 0.2634 -0.4904 -0.4908 -0.0035 0.4132 jant 0.0922 1.0000 0.3463 -0.3981 -0.2092 0.1163 0.0149 -0.1001 0.4538 jult 0.5033 0.3463 1.0000 -0.4340 0.2623 -0.2385 -0.4150 -0.0610 0.5753 ovr65 0.1011 -0.3981 -0.4340 1.0000 -0.5091 -0.1389 0.0650 0.1620 -0.6378 popn 0.2634 -0.2092 0.2623 -0.5091 1.0000 -0.3951 -0.4106 -0.1843 0.4194 educ -0.4904 0.1163 -0.2385 -0.1389 -0.3951 1.0000 0.5522 -0.2439 -0.2088 hous -0.4908 0.0149 -0.4150 0.0650 -0.4106 0.5522 1.0000 0.1819 -0.4103 dens -0.0035 -0.1001 -0.0610 0.1620 -0.1843 -0.2439 0.1819 1.0000 -0.0057 nonw 0.4132 0.4538 0.5753 -0.6378 0.4194 -0.2088 -0.4103 -0.0057 1.0000 wwdrk -0.2973 0.2380 -0.0214 -0.1177 -0.4257 0.7032 0.3387 -0.0318 -0.0044 poor 0.5066 0.5653 0.6193 -0.3098 0.2599 -0.4033 -0.6807 -0.1629 0.7049 hc -0.5318 0.3508 -0.3565 -0.0205 -0.3882 0.2868 0.3868 0.1203 -0.0259 nox -0.4873 0.3210 -0.3377 -0.0021 -0.3584 0.2244 0.3483 0.1653 0.0184 so2 -0.1069 -0.1078 -0.0993 0.0172 -0.0041 -0.2343 0.1180 0.4321 0.1593 humid -0.0773 0.0679 -0.4528 0.1124 -0.1357 0.1765 0.1219 -0.1250 -0.1180 mort 0.5095 -0.0300 0.2770 -0.1746 0.3573 -0.5110 -0.4268 0.2655 0.6437 wwdrk poor hc nox so2 humid mort prec -0.2973 0.5066 -0.5318 -0.4873 -0.1069 -0.0773 0.5095 jant 0.2380 0.5653 0.3508 0.3210 -0.1078 0.0679 -0.0300 jult -0.0214 0.6193 -0.3565 -0.3377 -0.0993 -0.4528 0.2770 ovr65 -0.1177 -0.3098 -0.0205 -0.0021 0.0172 0.1124 -0.1746 popn -0.4257 0.2599 -0.3882 -0.3584 -0.0041 -0.1357 0.3573 educ 0.7032 -0.4033 0.2868 0.2244 -0.2343 0.1765 -0.5110 hous 0.3387 -0.6807 0.3868 0.3483 0.1180 0.1219 -0.4268 dens -0.0318 -0.1629 0.1203 0.1653 0.4321 -0.1250 0.2655 nonw -0.0044 0.7049 -0.0259 0.0184 0.1593 -0.1180 0.6437 wwdrk 1.0000 -0.1852 0.2037 0.1600 -0.0685 0.0607 -0.2848 poor -0.1852 1.0000 -0.1298 -0.1025 -0.0965 -0.1522 0.4105 hc 0.2037 -0.1298 1.0000 0.9838 0.2823 -0.0202 -0.1772 nox 0.1600 -0.1025 0.9838 1.0000 0.4094 -0.0459 -0.0774 so2 -0.0685 -0.0965 0.2823 0.4094 1.0000 -0.1026 0.4259 humid 0.0607 -0.1522 -0.0202 -0.0459 -0.1026 1.0000 -0.0885 mort -0.2848 0.4105 -0.1772 -0.0774 0.4259 -0.0885 1.0000 # create dataframes # weather related vars mort_poll_1 &lt;- data.frame(mort, prec, jant, jult, humid) # socio-economic vars mort_poll_2 &lt;- data.frame(mort, ovr65, popn, educ, hous, dens, nonw, wwdrk, poor) # pollution effects mort_poll_3 &lt;- data.frame(mort, hc, nox, so2) # matrix scatterplots pairs(mort_poll_1, cex=1, pch=19) pairs(mort_poll_2, cex=0.5, pch=19) pairs(mort_poll_3, cex=1, pch=19) detach(mort_poll) First, you fit a rather simple linear model to explain mort. That is \\[\\begin{eqnarray*} Y &amp;=&amp; \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i, \\end{eqnarray*}\\] where \\(\\beta_0\\) is the intercept, \\(x_1\\) the educ and \\(x_2\\) the so2. attach(mort_poll) The following objects are masked from mort_poll (pos = 17): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 18): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 19): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 20): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 21): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk lm1 &lt;- lm(mort ~ educ + so2) summary(lm1) Call: lm(formula = mort ~ educ + so2) Residuals: Min 1Q Median 3Q Max -136.56 -30.37 -7.73 34.48 148.80 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1274.602 89.761 14.20 &lt; 2e-16 *** educ -32.017 8.020 -3.99 0.00019 *** so2 0.318 0.107 2.97 0.00432 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 50.6 on 57 degrees of freedom Multiple R-squared: 0.36, Adjusted R-squared: 0.338 F-statistic: 16.1 on 2 and 57 DF, p-value: 2.96e-06 detach(mort_poll) You inspect the analysis-of-variance table for this linear model lm1. That is anova(lm1) Analysis of Variance Table Response: mort Df Sum Sq Mean Sq F value Pr(&gt;F) educ 1 59612 59612 23.26 1.1e-05 *** so2 1 22642 22642 8.84 0.0043 ** Residuals 57 146054 2562 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 attach(mort_poll) The following objects are masked from mort_poll (pos = 17): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 18): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 19): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 20): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 21): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk lm0 &lt;- lm(mort ~ 1) lm_educ &lt;- lm(mort ~ educ) anova(lm_educ) Analysis of Variance Table Response: mort Df Sum Sq Mean Sq F value Pr(&gt;F) educ 1 59612 59612 20.5 3e-05 *** Residuals 58 168696 2909 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 F_educ &lt;- ((anova(lm0)$&quot;Sum Sq&quot;)-(anova(lm_educ)$&quot;Sum Sq&quot;[2]))/(anova(lm1)$&quot;Mean Sq&quot;[3]) F_educ [1] 23.265 F_so2 &lt;- ((anova(lm_educ)$&quot;Sum Sq&quot;[2])-(anova(lm1)$&quot;Sum Sq&quot;[3]))/(anova(lm1)$&quot;Mean Sq&quot;[3]) F_so2 [1] 8.8363 detach(mort_poll) You will now use the object lm1 to construct confidence and prediction intervals for a single observation. Given a set of predictor values in \\(x_0\\) the predicted response is \\(\\hat{y}_0 = x_0^{&#39;}\\hat{\\beta}\\). The uncertainty in predicting the mean response is \\(\\text{Var}(x_0^{&#39;}\\hat{\\beta})\\) whereas the uncertainty in predicting the value of an observation is \\(\\text{Var}(x_0^{&#39;}\\hat{\\beta}+\\epsilon_0)\\). attach(mort_poll) The following objects are masked from mort_poll (pos = 17): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 18): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 19): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 20): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 21): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk x0 &lt;- data.frame(educ = 10, so2 = exp(2)) predict(lm1, x0, interval = &quot;confidence&quot;) fit lwr upr 1 956.78 932.55 981.01 predict(lm1, x0, interval = &quot;prediction&quot;) fit lwr upr 1 956.78 852.56 1061 detach(mort_poll) For a grid of educ values, when so2 is fixed, this goes as follows: attach(mort_poll) The following objects are masked from mort_poll (pos = 17): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 18): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 19): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 20): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 21): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk grid &lt;- seq(8, 15, 0.1) x.new &lt;- data.frame(educ = grid, so2 = exp(2)) p &lt;- predict(lm1, x.new, se=TRUE, interval=&quot;prediction&quot;) p1 &lt;- predict(lm1, x.new, se=TRUE, interval=&quot;confidence&quot;) # use `matplot` to plot the columns of one matrix against the columnsof another matplot(grid, p$fit, lty=c(1,2,2), col=c(&quot;black&quot;, &quot;red&quot;, &quot;red&quot;), type = &quot;l&quot;, xlab = &quot;educ&quot;, ylab = &quot;mort&quot;, main = &quot;Predicted mort over a range of educ, log(so2)=2&quot;) matlines(grid, p1$fit, lty = c(1, 2, 2), col = c(&quot;black&quot;, &quot;blue&quot;, &quot;blue&quot;)) rug(educ) # for an explanation wrt different shapes, see # http://stats.stackexchange.com/questions/85560/shape-of-confidence-interval-for-p# redicted-values-in-linear-regression detach(mort_poll) Then you fit a linear model with all 15 variables in the dataset. attach(mort_poll) The following objects are masked from mort_poll (pos = 17): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 18): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 19): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 20): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 21): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk lm2 &lt;- lm(mort ~ prec + jant + jult + humid + hc + nox + so2 + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor) lm2$coef (Intercept) prec jant jult humid hc 1.7640e+03 1.9054e+00 -1.9376e+00 -3.1004e+00 1.0680e-01 -6.7214e-01 nox so2 ovr65 popn educ hous 1.3401e+00 8.6252e-02 -9.0654e+00 -1.0683e+02 -1.7157e+01 -6.5112e-01 dens nonw wwdrk poor 3.6005e-03 4.4596e+00 -1.8706e-01 -1.6764e-01 detach(mort_poll) Now perform model selection stepwise, based on AIC. # model selection based on AIC library(MASS) Attaching package: &#39;MASS&#39; The following object is masked from &#39;package:dplyr&#39;: select attach(mort_poll) The following objects are masked from mort_poll (pos = 18): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 19): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 20): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 21): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk The following objects are masked from mort_poll (pos = 22): dens, educ, hc, hous, humid, jant, jult, mort, nonw, nox, ovr65, poor, popn, prec, so2, wwdrk lm1 &lt;- lm(mort ~ 1) # get AIC, mind the difference AIC(lm1) [1] 668.92 extractAIC(lm1) [1] 1.00 496.65 # for linear models with unknown scale (i.e., for lm and aov), # -2 log L is computed from the deviance and uses a different additive constant to # logLik and hence AIC # forward search stepAIC(lm1, list(upper = ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid, lower = ~ 1), direction = &quot;forward&quot;) Start: AIC=496.65 mort ~ 1 Df Sum of Sq RSS AIC + nonw 1 94613 133695 467 + educ 1 59612 168696 480 + prec 1 59266 169041 481 + hous 1 41592 186716 487 + poor 1 38470 189838 488 + log(so2) 1 37087 191221 488 + popn 1 29149 199159 490 + log(nox) 1 19465 208843 493 + wwdrk 1 18518 209789 494 + jult 1 17520 210788 494 + dens 1 16093 212214 494 &lt;none&gt; 228308 497 + hc 1 7172 221136 497 + ovr65 1 6960 221347 497 + humid 1 1788 226520 498 + jant 1 206 228102 499 Step: AIC=466.54 mort ~ nonw Df Sum of Sq RSS AIC + educ 1 33853 99841 451 + log(so2) 1 31223 102471 453 + jant 1 29835 103859 453 + ovr65 1 21435 112259 458 + wwdrk 1 18153 115541 460 + dens 1 16540 117155 461 + prec 1 16324 117371 461 + hous 1 7264 126430 465 + log(nox) 1 6836 126859 465 + hc 1 5892 127803 466 &lt;none&gt; 133695 467 + jult 1 2973 130721 467 + popn 1 2112 131582 468 + poor 1 851 132844 468 + humid 1 37 133658 469 Step: AIC=451.02 mort ~ nonw + educ Df Sum of Sq RSS AIC + log(so2) 1 18177 81664 441 + jant 1 17453 82389 441 + poor 1 10921 88920 446 + log(nox) 1 8814 91027 447 + ovr65 1 7352 92489 448 + dens 1 7262 92579 448 + jult 1 6836 93005 449 &lt;none&gt; 99841 451 + prec 1 2468 97373 452 + hc 1 617 99224 453 + humid 1 530 99312 453 + popn 1 359 99482 453 + hous 1 167 99674 453 + wwdrk 1 14 99827 453 Step: AIC=440.96 mort ~ nonw + educ + log(so2) Df Sum of Sq RSS AIC + prec 1 9399 72266 436 + jant 1 7885 73780 437 + hc 1 6355 75309 438 + ovr65 1 2770 78895 441 &lt;none&gt; 81664 441 + poor 1 2236 79428 441 + dens 1 794 80871 442 + humid 1 692 80973 442 + hous 1 538 81126 443 + log(nox) 1 312 81352 443 + wwdrk 1 281 81383 443 + jult 1 132 81532 443 + popn 1 3 81661 443 Step: AIC=435.63 mort ~ nonw + educ + log(so2) + prec Df Sum of Sq RSS AIC + jant 1 5764 66502 433 + poor 1 2878 69388 435 &lt;none&gt; 72266 436 + hc 1 1594 70672 436 + log(nox) 1 1089 71177 437 + jult 1 981 71285 437 + dens 1 752 71513 437 + humid 1 508 71758 437 + wwdrk 1 405 71861 437 + hous 1 125 72141 438 + popn 1 70 72196 438 + ovr65 1 2 72263 438 Step: AIC=432.64 mort ~ nonw + educ + log(so2) + prec + jant Df Sum of Sq RSS AIC + log(nox) 1 8313 58188 427 &lt;none&gt; 66502 433 + popn 1 1724 64778 433 + dens 1 1455 65047 433 + jult 1 1097 65405 434 + humid 1 958 65544 434 + poor 1 474 66028 434 + hous 1 74 66428 435 + ovr65 1 56 66446 435 + wwdrk 1 27 66475 435 + hc 1 25 66477 435 Step: AIC=426.63 mort ~ nonw + educ + log(so2) + prec + jant + log(nox) Df Sum of Sq RSS AIC + popn 1 2793 55396 426 &lt;none&gt; 58188 427 + dens 1 1379 56810 427 + hc 1 792 57396 428 + jult 1 43 58145 429 + poor 1 25 58164 429 + humid 1 16 58172 429 + hous 1 8 58181 429 + wwdrk 1 6 58182 429 + ovr65 1 0 58188 429 Step: AIC=425.67 mort ~ nonw + educ + log(so2) + prec + jant + log(nox) + popn Df Sum of Sq RSS AIC &lt;none&gt; 55396 426 + ovr65 1 1802 53594 426 + hc 1 1587 53809 426 + dens 1 570 54826 427 + jult 1 142 55254 428 + wwdrk 1 129 55267 428 + poor 1 54 55341 428 + humid 1 10 55386 428 + hous 1 4 55392 428 Call: lm(formula = mort ~ nonw + educ + log(so2) + prec + jant + log(nox) + popn) Coefficients: (Intercept) nonw educ log(so2) prec jant 1315.91 4.50 -19.04 -6.15 2.19 -2.86 log(nox) popn 24.01 -73.76 # backward search lm1 &lt;- lm(mort ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid) stepAIC(lm1, list(upper = ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid, lower = ~ 1), direction = &quot;backward&quot;) Start: AIC=435.55 mort ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - poor 1 1 50019 434 - hous 1 224 50243 434 - wwdrk 1 447 50465 434 - dens 1 448 50467 434 - humid 1 704 50723 434 - ovr65 1 1490 51508 435 - jult 1 1543 51562 435 &lt;none&gt; 50019 436 - log(so2) 1 1758 51776 436 - hc 1 1879 51897 436 - educ 1 2667 52686 437 - popn 1 5218 55237 440 - jant 1 6882 56901 441 - prec 1 9111 59130 444 - nonw 1 10236 60255 445 - log(nox) 1 10651 60669 445 Step: AIC=433.55 mort ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - hous 1 386 50406 432 - dens 1 450 50469 432 - wwdrk 1 473 50492 432 - humid 1 728 50747 432 - jult 1 1561 51580 433 &lt;none&gt; 50019 434 - ovr65 1 1703 51723 434 - log(so2) 1 1757 51776 434 - hc 1 1950 51969 434 - educ 1 2668 52687 435 - popn 1 5284 55304 438 - prec 1 9583 59602 442 - log(nox) 1 10672 60691 443 - jant 1 11569 61589 444 - nonw 1 13554 63573 446 Step: AIC=432.01 mort ~ prec + jant + jult + ovr65 + popn + educ + dens + nonw + wwdrk + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - dens 1 256 50662 430 - wwdrk 1 401 50806 430 - humid 1 691 51097 431 - ovr65 1 1434 51840 432 - jult 1 1434 51840 432 &lt;none&gt; 50406 432 - log(so2) 1 1774 52179 432 - hc 1 2076 52482 432 - educ 1 4067 54473 435 - popn 1 5101 55507 436 - prec 1 9231 59637 440 - log(nox) 1 10385 60791 441 - jant 1 11860 62266 443 - nonw 1 17454 67860 448 Step: AIC=430.31 mort ~ prec + jant + jult + ovr65 + popn + educ + nonw + wwdrk + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - wwdrk 1 351 51013 429 - humid 1 752 51414 429 - jult 1 1398 52059 430 - log(so2) 1 1630 52292 430 &lt;none&gt; 50662 430 - ovr65 1 1735 52397 430 - hc 1 2141 52803 431 - educ 1 4949 55610 434 - popn 1 6396 57058 435 - prec 1 9657 60318 439 - log(nox) 1 10995 61657 440 - jant 1 12455 63117 442 - nonw 1 17200 67862 446 Step: AIC=428.73 mort ~ prec + jant + jult + ovr65 + popn + educ + nonw + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - humid 1 672 51684 428 - jult 1 1512 52524 428 - log(so2) 1 1699 52712 429 &lt;none&gt; 51013 429 - ovr65 1 1816 52829 429 - hc 1 1862 52874 429 - popn 1 6052 57064 433 - prec 1 10101 61113 438 - educ 1 10538 61551 438 - log(nox) 1 10737 61749 438 - jant 1 13041 64054 440 - nonw 1 16852 67864 444 Step: AIC=427.51 mort ~ prec + jant + jult + ovr65 + popn + educ + nonw + hc + log(nox) + log(so2) Df Sum of Sq RSS AIC - jult 1 847 52531 426 - log(so2) 1 1161 52846 427 - hc 1 1248 52932 427 &lt;none&gt; 51684 428 - ovr65 1 1801 53485 428 - popn 1 5567 57251 432 - prec 1 9735 61419 436 - log(nox) 1 10131 61815 436 - educ 1 10422 62106 437 - jant 1 13560 65244 439 - nonw 1 16185 67869 442 Step: AIC=426.49 mort ~ prec + jant + ovr65 + popn + educ + nonw + hc + log(nox) + log(so2) Df Sum of Sq RSS AIC - log(so2) 1 1024 53555 426 - hc 1 1063 53594 426 - ovr65 1 1277 53809 426 &lt;none&gt; 52531 426 - popn 1 4802 57333 430 - prec 1 9117 61648 434 - educ 1 9576 62107 435 - log(nox) 1 11716 64248 437 - jant 1 13524 66055 438 - nonw 1 15419 67951 440 Step: AIC=425.65 mort ~ prec + jant + ovr65 + popn + educ + nonw + hc + log(nox) Df Sum of Sq RSS AIC - hc 1 959 54515 425 - ovr65 1 1306 54861 425 &lt;none&gt; 53555 426 - popn 1 4006 57562 428 - prec 1 8495 62050 432 - educ 1 8553 62108 433 - nonw 1 14573 68129 438 - jant 1 14730 68285 438 - log(nox) 1 17866 71421 441 Step: AIC=424.71 mort ~ prec + jant + ovr65 + popn + educ + nonw + log(nox) Df Sum of Sq RSS AIC - ovr65 1 1809 56323 425 &lt;none&gt; 54515 425 - popn 1 3867 58382 427 - educ 1 9051 63566 432 - prec 1 12481 66996 435 - nonw 1 14126 68641 437 - log(nox) 1 18324 72839 440 - jant 1 21424 75939 443 Step: AIC=424.67 mort ~ prec + jant + popn + educ + nonw + log(nox) Df Sum of Sq RSS AIC &lt;none&gt; 56323 425 - popn 1 2067 58391 425 - educ 1 7387 63710 430 - prec 1 11450 67773 434 - log(nox) 1 16531 72854 438 - jant 1 20400 76724 441 - nonw 1 33353 89676 451 Call: lm(formula = mort ~ prec + jant + popn + educ + nonw + log(nox)) Coefficients: (Intercept) prec jant popn educ nonw 1232.06 2.08 -2.38 -60.09 -16.88 4.36 log(nox) 17.74 # both directions search lm1 &lt;- lm(mort ~ 1) lm1 &lt;- lm(mort ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid) stepAIC(lm1, list(upper = ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid, lower = ~ 1), direction = &quot;both&quot;) Start: AIC=435.55 mort ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - poor 1 1 50019 434 - hous 1 224 50243 434 - wwdrk 1 447 50465 434 - dens 1 448 50467 434 - humid 1 704 50723 434 - ovr65 1 1490 51508 435 - jult 1 1543 51562 435 &lt;none&gt; 50019 436 - log(so2) 1 1758 51776 436 - hc 1 1879 51897 436 - educ 1 2667 52686 437 - popn 1 5218 55237 440 - jant 1 6882 56901 441 - prec 1 9111 59130 444 - nonw 1 10236 60255 445 - log(nox) 1 10651 60669 445 Step: AIC=433.55 mort ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - hous 1 386 50406 432 - dens 1 450 50469 432 - wwdrk 1 473 50492 432 - humid 1 728 50747 432 - jult 1 1561 51580 433 &lt;none&gt; 50019 434 - ovr65 1 1703 51723 434 - log(so2) 1 1757 51776 434 - hc 1 1950 51969 434 - educ 1 2668 52687 435 + poor 1 1 50019 436 - popn 1 5284 55304 438 - prec 1 9583 59602 442 - log(nox) 1 10672 60691 443 - jant 1 11569 61589 444 - nonw 1 13554 63573 446 Step: AIC=432.01 mort ~ prec + jant + jult + ovr65 + popn + educ + dens + nonw + wwdrk + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - dens 1 256 50662 430 - wwdrk 1 401 50806 430 - humid 1 691 51097 431 - ovr65 1 1434 51840 432 - jult 1 1434 51840 432 &lt;none&gt; 50406 432 - log(so2) 1 1774 52179 432 - hc 1 2076 52482 432 + hous 1 386 50019 434 + poor 1 163 50243 434 - educ 1 4067 54473 435 - popn 1 5101 55507 436 - prec 1 9231 59637 440 - log(nox) 1 10385 60791 441 - jant 1 11860 62266 443 - nonw 1 17454 67860 448 Step: AIC=430.31 mort ~ prec + jant + jult + ovr65 + popn + educ + nonw + wwdrk + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - wwdrk 1 351 51013 429 - humid 1 752 51414 429 - jult 1 1398 52059 430 - log(so2) 1 1630 52292 430 &lt;none&gt; 50662 430 - ovr65 1 1735 52397 430 - hc 1 2141 52803 431 + dens 1 256 50406 432 + hous 1 193 50469 432 + poor 1 61 50600 432 - educ 1 4949 55610 434 - popn 1 6396 57058 435 - prec 1 9657 60318 439 - log(nox) 1 10995 61657 440 - jant 1 12455 63117 442 - nonw 1 17200 67862 446 Step: AIC=428.73 mort ~ prec + jant + jult + ovr65 + popn + educ + nonw + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - humid 1 672 51684 428 - jult 1 1512 52524 428 - log(so2) 1 1699 52712 429 &lt;none&gt; 51013 429 - ovr65 1 1816 52829 429 - hc 1 1862 52874 429 + wwdrk 1 351 50662 430 + dens 1 206 50806 430 + hous 1 161 50852 431 + poor 1 103 50909 431 - popn 1 6052 57064 433 - prec 1 10101 61113 438 - educ 1 10538 61551 438 - log(nox) 1 10737 61749 438 - jant 1 13041 64054 440 - nonw 1 16852 67864 444 Step: AIC=427.51 mort ~ prec + jant + jult + ovr65 + popn + educ + nonw + hc + log(nox) + log(so2) Df Sum of Sq RSS AIC - jult 1 847 52531 426 - log(so2) 1 1161 52846 427 - hc 1 1248 52932 427 &lt;none&gt; 51684 428 - ovr65 1 1801 53485 428 + humid 1 672 51013 429 + wwdrk 1 270 51414 429 + dens 1 264 51420 429 + poor 1 142 51542 429 + hous 1 129 51555 429 - popn 1 5567 57251 432 - prec 1 9735 61419 436 - log(nox) 1 10131 61815 436 - educ 1 10422 62106 437 - jant 1 13560 65244 439 - nonw 1 16185 67869 442 Step: AIC=426.49 mort ~ prec + jant + ovr65 + popn + educ + nonw + hc + log(nox) + log(so2) Df Sum of Sq RSS AIC - log(so2) 1 1024 53555 426 - hc 1 1063 53594 426 - ovr65 1 1277 53809 426 &lt;none&gt; 52531 426 + jult 1 847 51684 428 + wwdrk 1 434 52098 428 + dens 1 172 52360 428 + hous 1 89 52442 428 + poor 1 30 52502 428 + humid 1 7 52524 428 - popn 1 4802 57333 430 - prec 1 9117 61648 434 - educ 1 9576 62107 435 - log(nox) 1 11716 64248 437 - jant 1 13524 66055 438 - nonw 1 15419 67951 440 Step: AIC=425.65 mort ~ prec + jant + ovr65 + popn + educ + nonw + hc + log(nox) Df Sum of Sq RSS AIC - hc 1 959 54515 425 - ovr65 1 1306 54861 425 &lt;none&gt; 53555 426 + log(so2) 1 1024 52531 426 + jult 1 710 52846 427 + wwdrk 1 526 53029 427 + hous 1 139 53416 427 + dens 1 59 53496 428 + poor 1 40 53515 428 + humid 1 32 53523 428 - popn 1 4006 57562 428 - prec 1 8495 62050 432 - educ 1 8553 62108 433 - nonw 1 14573 68129 438 - jant 1 14730 68285 438 - log(nox) 1 17866 71421 441 Step: AIC=424.71 mort ~ prec + jant + ovr65 + popn + educ + nonw + log(nox) Df Sum of Sq RSS AIC - ovr65 1 1809 56323 425 &lt;none&gt; 54515 425 + hc 1 959 53555 426 + log(so2) 1 921 53594 426 + jult 1 556 53959 426 + wwdrk 1 233 54281 426 + hous 1 221 54293 426 + humid 1 211 54304 426 + poor 1 109 54406 427 + dens 1 86 54429 427 - popn 1 3867 58382 427 - educ 1 9051 63566 432 - prec 1 12481 66996 435 - nonw 1 14126 68641 437 - log(nox) 1 18324 72839 440 - jant 1 21424 75939 443 Step: AIC=424.67 mort ~ prec + jant + popn + educ + nonw + log(nox) Df Sum of Sq RSS AIC &lt;none&gt; 56323 425 + ovr65 1 1809 54515 425 - popn 1 2067 58391 425 + hc 1 1462 54861 425 + log(so2) 1 928 55396 426 + dens 1 348 55975 426 + wwdrk 1 187 56136 426 + humid 1 102 56222 427 + jult 1 97 56226 427 + poor 1 47 56277 427 + hous 1 15 56308 427 - educ 1 7387 63710 430 - prec 1 11450 67773 434 - log(nox) 1 16531 72854 438 - jant 1 20400 76724 441 - nonw 1 33353 89676 451 Call: lm(formula = mort ~ prec + jant + popn + educ + nonw + log(nox)) Coefficients: (Intercept) prec jant popn educ nonw 1232.06 2.08 -2.38 -60.09 -16.88 4.36 log(nox) 17.74 detach(mort_poll) 10.3 Exercises Learning check Load the Boston Housing dataset from the mlbench package. Use the following instructions library(mlbench) data(&quot;BostonHousing&quot;) Inspect the different types of variables present. Explore and visualize the distribution of our target variable medv. Explore and visualize any potential correlations between medv and the variables crim, rm, age, rad, tax and lstat. Set a seed of 123 and split your data into a train and test set using a 75/25 split. You may find the caret library helpful here. We have seen that crim, rm, tax, and lstat could be good predictors of medv. To get the ball rolling, let us fit a linear model for these terms. Obtain an R-squared value for your model and examine the diagnostic plots found by plotting your linear model. We can see a few problems with our model immediately with variables such as 381 exhibiting a high leverage, a poor QQ plot in the tails a relatively poor r-squared value. Let us try another model, this time transforming medv due to the positive skewness it exhibited. Examine the diagnostics for the model. What do you conclude? Is this an improvement on the first model? One assumption of a linear model is that the mean of the residuals is zero. You could try and test this. Create a data frame of your predicted values and the original values. Plot this to visualize the performance of your model. "],
["glms.html", "11 Generalized Linear Models in R 11.1 Modelling count data with Poisson regression models 11.2 Overdispersed Poisson regression 11.3 Negative Binomial regression", " 11 Generalized Linear Models in R You’ll now study the use of Generalized Linear Models in R for insurance ratemaking. You focus first on the example from Rob Kaas’ et al. (2008) Modern Actuarial Risk Theory book (see Section 9.5 in this book), with simulated claim frequency data. 11.1 Modelling count data with Poisson regression models 11.1.1 A first data set This example uses artifical, simulated data. You consider data on claim frequencies, registered on 54 risk cells over a period of 7 years. n gives the number of claims, and expo the corresponding number of policies in a risk cell; each policy is followed over a period of 7 years and n is the number of claims reported over this total period. n &lt;- scan(n = 54) 1 8 10 8 5 11 14 12 11 10 5 12 13 12 15 13 12 24 12 11 6 8 16 19 28 11 14 4 12 8 18 3 17 6 11 18 12 3 10 18 10 13 12 31 16 16 13 14 8 19 20 9 23 27 expo &lt;- scan(n = 54) * 7 10 22 30 11 15 20 25 25 23 28 19 22 19 21 19 16 18 29 25 18 20 13 26 21 27 14 16 11 23 26 29 13 26 13 17 27 20 18 20 29 27 24 23 26 18 25 17 29 11 24 16 11 22 29 n expo [1] 1 8 10 8 5 11 14 12 11 10 5 12 13 12 15 13 12 24 12 11 6 8 16 19 28 [26] 11 14 4 12 8 18 3 17 6 11 18 12 3 10 18 10 13 12 31 16 16 13 14 8 19 [51] 20 9 23 27 [1] 70 154 210 77 105 140 175 175 161 196 133 154 133 147 133 112 126 203 175 [20] 126 140 91 182 147 189 98 112 77 161 182 203 91 182 91 119 189 140 126 [39] 140 203 189 168 161 182 126 175 119 203 77 168 112 77 154 203 The goal is to illustrate ratemaking by explaining the expected number of claims as a function of a set of observable risk factors. Since artificial data are used in this example, you use simulated or self constructed risk factors. 4 factor variables are created, the sex of the policyholder (1=female and 2=male), the region where she lives (1=countryside, 2=elsewhere and 3=big city), the type of car (1=small, 2=middle and 3=big) and job class of the insured (1=civil servant/actuary/…, 2=in-between and 3=dynamic drivers). You use the R instruction rep() to construct these risk factors. In total 54 risk cells are created in this way. Note that you use the R instruction as.factor() to specify the risk factors as factor (or: categorical) covariates. sex &lt;- as.factor(rep(1:2, each=27, len=54)) region &lt;- as.factor(rep(1:3, each=9, len=54)) type &lt;- as.factor(rep(1:3, each=3, len=54)) job &lt;- as.factor(rep(1:3, each=1, len=54)) sex [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 [39] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 Levels: 1 2 region [1] 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 2 2 [39] 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 Levels: 1 2 3 type [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 [39] 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 Levels: 1 2 3 job [1] 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 [39] 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 Levels: 1 2 3 11.1.2 Fit a Poisson GLM The response variable \\(N_i\\) is the number of claims reported on risk cell i, hence it is reasonable to assume a Poisson distribution for this random variable. You fit the following Poisson GLM to the data \\[\\begin{eqnarray*} N_i &amp;\\sim&amp; \\text{POI}(d_i \\cdot \\lambda_i) \\end{eqnarray*}\\] where \\(\\lambda_i = \\exp{(\\boldsymbol{x}^{&#39;}_i\\boldsymbol{\\beta})}\\) and \\(d_i\\) is the exposure for risk cell \\(i\\). In R you use the instruction glm to fit a GLM. Covariates are listed with +, and the log of expo is used as an offset. Indeed, \\[\\begin{eqnarray*} N_i &amp;\\sim&amp; \\text{POI}(d_i \\cdot \\lambda_i) \\\\ &amp;= &amp; \\text{POI}(\\exp{(\\boldsymbol{x}^{&#39;}_i \\boldsymbol{\\beta}+\\log{(d_i)})}) \\end{eqnarray*}\\] The R instruction to fit this GLM (with sex, region, type and job the factor variables that construct the linear predictor) then goes as follows g1 &lt;- glm(n ~ sex + region + type + job + offset(log(expo)), fam = poisson(link = log)) where the argument fam= indicates the distribution from the exponential family that is assumed. In this case you work with the Poisson distribution with logarithmic link (which is the default link in R). All available distributions and their default link functions are listed here http://stat.ethz.ch/R-manual/R-patched/library/stats/html/family.html. You store the results of the glm fit in the object g1. You consult this object with the summary instruction summary(g1) Call: glm(formula = n ~ sex + region + type + job + offset(log(expo)), family = poisson(link = log)) Deviance Residuals: Min 1Q Median 3Q Max -1.9278 -0.6303 -0.0215 0.5380 2.3000 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.0996 0.1229 -25.21 &lt; 2e-16 *** sex2 0.1030 0.0763 1.35 0.1771 region2 0.2347 0.0992 2.36 0.0180 * region3 0.4643 0.0965 4.81 1.5e-06 *** type2 0.3946 0.1017 3.88 0.0001 *** type3 0.5844 0.0971 6.02 1.8e-09 *** job2 -0.0362 0.0970 -0.37 0.7091 job3 0.0607 0.0926 0.66 0.5121 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 104.73 on 53 degrees of freedom Residual deviance: 41.93 on 46 degrees of freedom AIC: 288.2 Number of Fisher Scoring iterations: 4 This summary of a glm fit lists (among others) the following items: the covariates used in the model, the corresponding estimates for the regression parameters (\\(\\boldsymbol{\\hat{\\beta}}\\)), their standard errors, \\(z\\) statistics and corresponding \\(P\\) values; the dispersion parameter used; for the standard Poisson regression model this dispersion parameter is equal to 1, as indicated in the R output; the null deviance - the deviance of the model that uses only an intercept - and the residual deviance - the deviance of the current model; the null deviance corresponds to \\(53\\) degrees of freedom, that is \\(54-1\\) where \\(54\\) is the number of observations used and \\(1\\) the number of parameters (here: just the intercept); the residual deviance corresponds to \\(54-8=46\\) degrees of freedom, since it uses \\(8\\) parameters; the AIC calculated for the considered regression model; the number of Fisher’s iterations needed to get convergence of the iterative numerical method to calculate the MLEs of the regression parameters in \\(\\boldsymbol{\\beta}\\). The instruction names shows the names of the variables stored within a glm object. One of these variables is called coef and contains the vector of regression parameter estimates (\\(\\hat{\\boldsymbol{\\beta}}\\)). It can be extracted with the instruction g1$coef. names(g1) [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;fitted.values&quot; [4] &quot;effects&quot; &quot;R&quot; &quot;rank&quot; [7] &quot;qr&quot; &quot;family&quot; &quot;linear.predictors&quot; [10] &quot;deviance&quot; &quot;aic&quot; &quot;null.deviance&quot; [13] &quot;iter&quot; &quot;weights&quot; &quot;prior.weights&quot; [16] &quot;df.residual&quot; &quot;df.null&quot; &quot;y&quot; [19] &quot;converged&quot; &quot;boundary&quot; &quot;model&quot; [22] &quot;call&quot; &quot;formula&quot; &quot;terms&quot; [25] &quot;data&quot; &quot;offset&quot; &quot;control&quot; [28] &quot;method&quot; &quot;contrasts&quot; &quot;xlevels&quot; g1$coef (Intercept) sex2 region2 region3 type2 type3 -3.099592 0.103034 0.234677 0.464340 0.394632 0.584428 job2 job3 -0.036174 0.060717 Other variables can be consulted in a similar way. For example, fitted values at the original level are \\(\\hat{\\mu}_i=\\exp{(\\hat{\\eta}_i)}\\) where the fitted values at the level of the linear predictor are stored in \\(\\hat{\\eta}_i=\\log{(d_i)}+\\boldsymbol{x}^{&#39;}_i\\hat{\\boldsymbol{\\beta}}\\). You then plot the fitted values versus the observed number of claims n. You add two reference lines: the diagonal and the least squares line. g1$fitted.values 1 2 3 4 5 6 7 8 9 10 3.1547 6.6938 10.0566 5.1492 6.7722 9.9483 14.1487 13.6460 13.8316 11.1697 11 12 13 14 15 16 17 18 19 20 7.3101 9.3255 11.2466 11.9888 11.9506 11.4503 12.4239 22.0527 12.5477 8.7134 21 22 23 24 25 26 27 28 29 30 10.6665 9.6817 18.6755 16.6187 24.3109 12.1578 15.3083 3.8468 7.7576 9.6617 31 32 33 34 35 36 37 38 39 40 15.0485 6.5062 14.3364 8.1558 10.2864 17.9993 8.8442 7.6770 9.3978 19.0289 41 42 43 44 45 46 47 48 49 50 17.0871 16.7338 18.2461 19.8933 15.1734 13.9095 9.1224 17.1450 9.0813 19.1099 51 52 53 54 14.0361 10.9794 21.1786 30.7575 g1$linear.predictors 1 2 3 4 5 6 7 8 9 10 11 1.1489 1.9012 2.3082 1.6388 1.9128 2.2974 2.6496 2.6134 2.6270 2.4132 1.9893 12 13 14 15 16 17 18 19 20 21 22 2.2328 2.4201 2.4840 2.4808 2.4380 2.5196 3.0934 2.5295 2.1649 2.3671 2.2702 23 24 25 26 27 28 29 30 31 32 33 2.9272 2.8105 3.1909 2.4980 2.7284 1.3472 2.0487 2.2682 2.7113 1.8728 2.6628 34 35 36 37 38 39 40 41 42 43 44 2.0987 2.3308 2.8903 2.1798 2.0382 2.2405 2.9460 2.8383 2.8174 2.9040 2.9904 45 46 47 48 49 50 51 52 53 54 2.7195 2.6326 2.2107 2.8417 2.2062 2.9502 2.6416 2.3960 3.0530 3.4261 plot(g1$fitted.values, n, xlab = &quot;Fitted values&quot;, ylab = &quot;Observed claims&quot;) abline(lm(g1$fitted ~ n), col=&quot;light blue&quot;, lwd=2) abline(0, 1, col = &quot;dark blue&quot;, lwd=2) To extract the AIC you use AIC(g1) [1] 288.24 11.1.3 The use of exposure The use of expo, the exposure measure, in a Poisson GLM often leads to confusion. For example, the following glm instruction uses a transformed response variable \\(n/expo\\) g2 &lt;- glm(n/expo ~ sex+region+type+job,fam=poisson(link=log)) summary(g2) Call: glm(formula = n/expo ~ sex + region + type + job, family = poisson(link = log)) Deviance Residuals: Min 1Q Median 3Q Max -0.16983 -0.05628 -0.00118 0.04680 0.17676 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.1392 1.4846 -2.11 0.034 * sex2 0.1007 0.9224 0.11 0.913 region2 0.2624 1.2143 0.22 0.829 region3 0.4874 1.1598 0.42 0.674 type2 0.4095 1.2298 0.33 0.739 type3 0.5757 1.1917 0.48 0.629 job2 -0.0308 1.1502 -0.03 0.979 job3 0.0957 1.1150 0.09 0.932 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 0.77537 on 53 degrees of freedom Residual deviance: 0.31739 on 46 degrees of freedom AIC: Inf Number of Fisher Scoring iterations: 5 and the object g3 stores the result of a Poisson fit on the same response variable, while taking expo into account as weights in the likelihood. g3 &lt;- glm(n/expo ~ sex+region+type+job,weights=expo,fam=poisson(link=log)) summary(g3) Call: glm(formula = n/expo ~ sex + region + type + job, family = poisson(link = log), weights = expo) Deviance Residuals: Min 1Q Median 3Q Max -1.9278 -0.6303 -0.0215 0.5380 2.3000 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.0996 0.1229 -25.21 &lt; 2e-16 *** sex2 0.1030 0.0763 1.35 0.1771 region2 0.2347 0.0992 2.36 0.0180 * region3 0.4643 0.0965 4.81 1.5e-06 *** type2 0.3946 0.1017 3.88 0.0001 *** type3 0.5844 0.0971 6.02 1.8e-09 *** job2 -0.0362 0.0970 -0.37 0.7091 job3 0.0607 0.0926 0.66 0.5121 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 104.73 on 53 degrees of freedom Residual deviance: 41.93 on 46 degrees of freedom AIC: Inf Number of Fisher Scoring iterations: 5 Based on this output you conclude that g1 (with the log of exposure as offset in the linear predictor) and g3 are the same, but g2 is not. The mathematical explanation for this observation is given in the note ‘WeightsInGLMs.pdf’ available from Katrien’s lecture notes (available upon request). 11.1.4 Analysis of deviance for GLMs 11.1.4.1 The basics You now focus on the selection of variables within a GLM based on a drop in deviance analysis. Your starting point is the GLM object g1 and the anova instruction. g1 &lt;- glm(n ~ 1 + region + type + job, poisson, offset = log(expo)) anova(g1, test=&quot;Chisq&quot;) Analysis of Deviance Table Model: poisson, link: log Response: n Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) NULL 53 104.7 region 2 21.6 51 83.1 2.0e-05 *** type 2 38.2 49 44.9 5.1e-09 *** job 2 1.2 47 43.8 0.55 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The analysis of deviance table first summarizes the Poisson GLM object (response n, link is log, family is poisson). The table starts with the deviance of the NULL model (just using an intercept), and then adds risk factors sequentially. Recall that in this example only factor covariates are present. Adding region (which has three levels, and requires two dummy variables) to the NULL model causes a drop in deviance of 21.597, corresponding to 54-1-2 degrees of freedom and a resulting (residual) deviance of 83.135. The drop in deviance test allows to test whether the model term region is significant. That is: \\[ H_0: \\beta_{\\text{region}_2}=0\\ \\text{and}\\ \\beta_{\\text{region}_3}=0. \\] The distribution of the corresponding test statistic is a Chi-squared distribution with 2 (i.e 53-51) degrees of freedom. The corresponding \\(P\\)-value is 2.043e-05. Hence, the model using region and the intercept is preferred above the NULL model. We can verify the \\(P\\)-value by calculating the following probability \\[ Pr(X &gt; 21.597)\\ \\text{with}\\ X \\sim \\chi^2_{2}.\\] Indeed, this is the probability - under \\(H_0\\) - to obtain a value of the test statistic that is the same or more extreme than the actual observed value of the test statistic. Calculations in R are as follows: # p-value for region 1 - pchisq(21.597, 2) [1] 2.043e-05 # or pchisq(21.597, 2, lower.tail = FALSE) [1] 2.043e-05 Continuing the discussion of the above printed anova table, the next step is to add type to the model using an intercept and region. This causes a drop in deviance of 38.195. You conclude that also type is a significant model term. The last step adds job to the existing model (with intercept, region and type). You conclude that job does not have a significant impact when explaining the expected number of claims. Based on this analysis of deviance table region and type seem to be relevant risk factors, but job is not, when explaining the expected number of claims. The Chi-squared distribution is used here, since the regular Poisson regression model does not require the estimation of a dispersion parameter. anova(g1,test=&quot;Chisq&quot;) The setting changes when the dispersion parameter is unknown and should be estimated. If you run the analysis of deviance for glm object g1 with the F distribution as distribution for the test statistic, you obtain: # what if we use &#39;F&#39; instead of &#39;Chisq&#39;? anova(g1,test=&quot;F&quot;) Warning in anova.glm(g1, test = &quot;F&quot;): using F test with a &#39;poisson&#39; family is inappropriate Analysis of Deviance Table Model: poisson, link: log Response: n Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev F Pr(&gt;F) NULL 53 104.7 region 2 21.6 51 83.1 10.80 2.0e-05 *** type 2 38.2 49 44.9 19.10 5.1e-09 *** job 2 1.2 47 43.8 0.59 0.55 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # not appropriate for regular Poisson regression, see Warning message in the console! and a Warning message is printed in the console that says # Warning message: # In anova.glm(g1, test = &quot;F&quot;) : # using F test with a &#39;poisson&#39; family is inappropriate It is insightful to understand how the output shown for the \\(F\\) statistic and corresponding \\(P\\)-value is calculated. For example, the drop in deviance test comparing the NULL model viz a model using an intercept and region corresponds to an observed test statistic of 10.7985. The calculation of the \\(F\\) statistic requires \\[ \\frac{\\text{Drop-in-deviance}/q}{\\hat{\\phi}}, \\] where \\(q\\) is the difference in degrees of freedom between the compared models and \\(\\hat{\\phi}\\) is the estimate for the dispersion parameter. In this example \\(F\\) corresponding to region is calculated as (21.597/2)/1 [1] 10.799 However, as explained, since the model investigated has a known dispersion, the Chi-squared test is most appropriate here. More details are here: https://stat.ethz.ch/R-manual/R-devel/library/stats/html/anova.glm.html. 11.1.5 An example You are now ready to study a complete analysis-of-deviance table. This table investigates 10 possible model specifications g1-g10. # construct an analysis-of-deviance table g1 &lt;- glm(n ~ 1, poisson , offset=log(expo)) g2 &lt;- glm(n ~ sex, poisson , offset=log(expo)) g3 &lt;- glm(n ~ sex+region, poisson, offset=log(expo)) g4 &lt;- glm(n ~ sex+region+sex:region, poisson, offset=log(expo)) g5 &lt;- glm(n ~ type, poisson, offset=log(expo)) g6 &lt;- glm(n ~ region, poisson, offset=log(expo)) g7 &lt;- glm(n ~ region+type, poisson, offset=log(expo)) g8 &lt;- glm(n ~ region+type+region:type, poisson, offset=log(expo)) g9 &lt;- glm(n ~ region+type+job, poisson, offset=log(expo)) g10 &lt;- glm(n ~ region+type+sex, poisson, offset=log(expo)) For example, the residual deviance obtained with model g8 (using intercept, region, type and the interaction of region and type) is 42.4, see summary(g8) Call: glm(formula = n ~ region + type + region:type, family = poisson, offset = log(expo)) Deviance Residuals: Min 1Q Median 3Q Max -1.8296 -0.4893 -0.0622 0.5377 1.8974 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.9887 0.1525 -19.60 &lt;2e-16 *** region2 0.1499 0.2061 0.73 0.467 region3 0.4216 0.1927 2.19 0.029 * type2 0.4338 0.1985 2.19 0.029 * type3 0.4520 0.1927 2.35 0.019 * region2:type2 -0.0808 0.2664 -0.30 0.762 region3:type2 -0.0223 0.2537 -0.09 0.930 region2:type3 0.2556 0.2562 1.00 0.318 region3:type3 0.1086 0.2449 0.44 0.657 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 104.732 on 53 degrees of freedom Residual deviance: 42.412 on 45 degrees of freedom AIC: 290.7 Number of Fisher Scoring iterations: 4 g8$deviance [1] 42.412 Using the technique of drop in deviance analysis you compare the models that are nested (!!) and decide which model specification is the preferred one. To do this, one can run multiple anova instructions such as anova(g1, g2, test = &quot;Chisq&quot;) Analysis of Deviance Table Model 1: n ~ 1 Model 2: n ~ sex Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 53 105 2 52 103 1 1.93 0.17 which compares nested models g1 and g2, or g7 and g8 anova(g7, g8, test = &quot;Chisq&quot;) Analysis of Deviance Table Model 1: n ~ region + type Model 2: n ~ region + type + region:type Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 49 44.9 2 45 42.4 4 2.53 0.64 11.2 Overdispersed Poisson regression The overdispersed Poisson model builds a regression model for the mean of the response variable \\[ EN_i = \\exp{(\\log d_i + \\boldsymbol{x}_i^{&#39;}\\boldsymbol{\\beta})} \\] and expressses the variance as \\[ \\text{Var}(N_i) = \\phi \\cdot EN_i, \\] with \\(N_i\\) the number of claims reported by policyholder \\(i\\) and \\(\\phi\\) an unknown dispersion parameter that should be estimated. This is called a quasi-Poisson model (see http://stat.ethz.ch/R-manual/R-patched/library/stats/html/family.html) and Section 1 in http://data.princeton.edu/wws509/notes/c4a.pdf for a more detailed explanation. To illustrate the differences between a regular Poisson and an overdispersed Poisson model, we fit the models g.poi and g.quasi: g.poi &lt;- glm(n ~ 1 + region + type, poisson, offset = log(expo)) summary(g.poi) Call: glm(formula = n ~ 1 + region + type, family = poisson, offset = log(expo)) Deviance Residuals: Min 1Q Median 3Q Max -1.9233 -0.6564 -0.0573 0.4790 2.3144 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.0313 0.1015 -29.86 &lt; 2e-16 *** region2 0.2314 0.0990 2.34 0.0195 * region3 0.4605 0.0965 4.77 1.8e-06 *** type2 0.3942 0.1015 3.88 0.0001 *** type3 0.5833 0.0971 6.01 1.9e-09 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 104.73 on 53 degrees of freedom Residual deviance: 44.94 on 49 degrees of freedom AIC: 285.2 Number of Fisher Scoring iterations: 4 g.quasi &lt;- glm(n ~ 1 + region + type, quasipoisson, offset = log(expo)) summary(g.quasi) Call: glm(formula = n ~ 1 + region + type, family = quasipoisson, offset = log(expo)) Deviance Residuals: Min 1Q Median 3Q Max -1.9233 -0.6564 -0.0573 0.4790 2.3144 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -3.0313 0.0961 -31.54 &lt; 2e-16 *** region2 0.2314 0.0938 2.47 0.01715 * region3 0.4605 0.0913 5.04 6.7e-06 *** type2 0.3942 0.0961 4.10 0.00015 *** type3 0.5833 0.0919 6.35 6.8e-08 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for quasipoisson family taken to be 0.89654) Null deviance: 104.73 on 53 degrees of freedom Residual deviance: 44.94 on 49 degrees of freedom AIC: NA Number of Fisher Scoring iterations: 4 Parameter estimates in both models are the same, but standard errors (and hence \\(P\\)-values) are not! You also see that g.poi reports z-value whereas g.quasi reports t-value, because the latter model estimates an extra parameter, i.e. the dispersion parameter. Various methods are available to estimate the dispersion parameter, e.g. \\[ \\hat{\\phi} = \\frac{\\text{Deviance}}{n-(p+1)}\\] and \\[ \\hat{\\phi} = \\frac{\\text{Pearson}\\ \\chi^2}{n-(p+1)}\\] where \\(p+1\\) is the total number of parameters (including the intercept) used in the considered model. The (residual) deviance is the deviance of the considered model and can also be obtained as the sum of squared deviance residuals. The Pearson \\(\\chi^2\\) statistic is the sum of the squared Pearson residuals. The latter is the default in R. Hence, you can verify the dispersion parameter of 0.896 as printed in the summary of g.quasi: # dispersion parameter in g is estimated as follows phi &lt;- sum(residuals(g.poi, &quot;pearson&quot;)^2)/g.poi$df.residual phi [1] 0.89654 Since \\(\\hat{\\phi}\\) is less than 1, the result seems to indicate underdispersion. However, as discussed in Section 2.4 ‘Overdispersion’ in the book of Denuit et al. (2007), real data on reported claim counts very often reveal overdispersion. The counterintuitive result that is obtained here is probably due to the fact that artificial, self-constructed data are used. When going from g.poi (regular Poisson) to g.quasi the standard errors are changed as follows: \\[ \\text{SE}_{\\text{Q-POI}} = \\sqrt{\\hat{\\phi}} \\cdot \\text{SE}_{\\text{POI}},\\] where \\(\\text{Q-POI}\\) is for quasi-Poisson. As a last step, you run the analysis of deviance for the quasi-Poisson model: anova(g.quasi, test = &quot;F&quot;) Analysis of Deviance Table Model: quasipoisson, link: log Response: n Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev F Pr(&gt;F) NULL 53 104.7 region 2 21.6 51 83.1 12.0 5.6e-05 *** type 2 38.2 49 44.9 21.3 2.2e-07 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For example, the \\(F\\)-statistic for region is calculated as F &lt;- (21.597/2)/phi F [1] 12.045 and the corresponding \\(P\\)-value is pf(F, 2, 49, lower.tail = FALSE) [1] 5.5642e-05 11.3 Negative Binomial regression You now focus on the use of yet another useful count regression model, that is the Negative Binomial regression model. The routine to fit a NB regression model is available in the package MASS and is called glm.nb, see https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/glm.nb.html # install.packages(&quot;MASS&quot;) library(MASS) g.nb &lt;- glm.nb(n ~ 1+region+sex+offset(log(expo))) summary(g.nb) Call: glm.nb(formula = n ~ 1 + region + sex + offset(log(expo)), init.theta = 26.38897379, link = log) Deviance Residuals: Min 1Q Median 3Q Max -2.6222 -0.6531 -0.0586 0.6587 2.3542 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.7342 0.1019 -26.84 &lt; 2e-16 *** region2 0.2359 0.1195 1.97 0.04837 * region3 0.4533 0.1176 3.85 0.00012 *** sex2 0.1049 0.0939 1.12 0.26392 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for Negative Binomial(26.389) family taken to be 1) Null deviance: 71.237 on 53 degrees of freedom Residual deviance: 54.917 on 50 degrees of freedom AIC: 316.1 Number of Fisher Scoring iterations: 1 Theta: 26.4 Std. Err.: 15.3 2 x log-likelihood: -306.06 "],
["biblio.html", "12 References", " 12 References "]
]
