---
title: "Machine learning in R - Day 1"
subtitle: "Hands-on workshop at Nationale Nederlanden <html><div style='float:left'></div><hr align='center' color='#116E8A' size=1px width=97%></html>"
author: "Katrien Antonio, Jonas Crevecoeur and Roel Henckaerts"
date: "[NN ML workshop](https://www.github.com/katrienantonio/workshop-ML) | February 11-13, 2020"
output:
  xaringan::moon_reader:
    css: [default, css/metropolis.css, css/metropolis-fonts.css, css/my-css.css] 
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      highlightLines: true
      countIncrementalSlides: false
      highlightSpans: true
graphics: yes
editor_options: 
  chunk_output_type: console
header-includes:
- \usepackage{tikz}
- \usetikzlibrary{shapes.geometric,shapes, snakes, arrows}
- \usepackage{amsfonts}
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{color}
- \usepackage{graphicx}
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(knitr)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
# options(knitr.table.format = "html")
library(tidyverse)
library(fontawesome) # from github: https://github.com/rstudio/fontawesome
library(DiagrammeR)
library(emo) # from github: https://github.com/hadley/emo
library(gt) # from github: https://github.com/rstudio/gt
library(countdown) # from github: https://github.com/gadenbuie/countdown 
library(here)
```

```{r setup_greenwell, include=FALSE}
# Set global R options
options(htmltools.dir.version = FALSE, servr.daemon = TRUE, 
        crayon.enabled = TRUE)

# Set global knitr chunk options
knitr::opts_chunk$set(
  dev = "svg",
  fig.align = "center",
  cache = TRUE,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE 
)

# colors - I copied most of these from # https://github.com/edrubin/EC524W20
dark2 <- RColorBrewer::brewer.pal(8, name = "Dark2")
KULbg <- "#116E8A"
red_pink   = "#e64173"
turquoise  = "#20B2AA"
orange     = "#FFA500"
red        = "#fb6107"
blue       = "#3b3b9a"
green      = "#8bb174"
grey_light = "grey70"
grey_mid   = "grey50"
grey_dark  = "grey20"
purple     = "#6A5ACD"
slate      = "#314f4f"
```

class: inverse, center, middle
name: prologue

# Prologue

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

name: introduction

# Introduction

### Course

`r fa(name = "github", fill = KULbg)` https://github.com/katrienantonio/workshop-ML

The course repo on GitHub, where we upload presentations, data sets, etc.

--

### Us

`r fa(name = "link", fill = KULbg)` [https://katrienantonio.github.io/](https://katrienantonio.github.io/)

`r fa(name = "paper-plane", fill = KULbg)` [katrien.antonio@kuleuven.be](mailto:katrien.antonio@kuleuven.be) & [jonas.crevecoeur@kuleuven.be](mailto:jonas.crevecoeur@kuleuven.be) & [roel.henckaerts@kuleuven.be](mailto:roel.henckaerts@kuleuven.be)

`r fa('graduation-cap', fill = KULbg)` (Katrien) Professor in insurance data science
`r fa('graduation-cap', fill = KULbg)` (Jonas, Roel) PhD students in insurance data science

--

### You

A quick roundtable of names, fields/interests, and coding background.

---

name: checklist

# Checklist

☑ Do you have a fairly recent version of R?
  ```{r eval=FALSE}
  version$version.string
  ```

☑ Do you have a fairly recent version of RStudio? 
  ```{r eval=FALSE}
  RStudio.Version()$version
  ## Requires an interactive session but should return something like "[1] ‘1.2.5001’"
  ```

☑ Have you installed the R packages listed in the software requirements? 
  
☑ Do you have a fairly recent version of Python via Anaconda? Did you install tensorflow and keras in Python?

☑ Do you have the R interface to `h2o` and `keras` installed?

---

name: why-this-course # inspired by Grant McDermott intro lecture

# Why this course?

### The goals of this course .font140[`r fa(name = "fas fa-rocket", fill = KULbg)`]

--

* develop practical .KULbginline[machine learning (ML) foundations]

--

* .KULbginline[fill in the gaps] left by traditional training in actuarial science or econometrics

--

* focus on the use of ML methods for the .KULbginline[analysis of frequency + severity data], but also .KULbginline[non-standard data] such as images 

--

* .KULbginline[explore] a substantial range of .KULbginline[methods (and data types)] (from GLMs to deep learning), but - most importantly - .KULbginline[build foundation] so that you can explore other methods (and data types) yourself. 

--

<br>

> *"In short, we will cover things that we wish someone had taught us in our undergraduate programs."* 
> <br>
> .font80[This quote is from the [Data science for economists course](http://github.com/uo-ec607/lectures) by Grant McDermott.]

---

name: why-R

# Why R and RStudio? 

.center[
```{r, indeeddotcom, echo = F, fig.align = 'center', fig.width = 10, fig.height = 4.5, dev = "svg"}
# The popularity data (by Katrien on Jan 12, 2020 via indeed.com)
pop_df <- 
  data.frame(
  lang = c("SQL", "Python", "R", "SAS", "Matlab", "SPSS", "Stata"),
  n_jobs = c(80329, 71894, 51865, 24355, 11710, 3497, 1874),
  free = c(T, T, T, F, F, F, F)
  )
## Plot it
pop_df %>% 
  mutate(lang = lang %>% factor(ordered = T)) %>%
  ggplot(aes(x = lang, y = n_jobs, fill = free)) +
  geom_col() +
  geom_hline(yintercept = 0) +
  aes(x = reorder(lang, -n_jobs), fill = reorder(free, -free)) +
  xlab("Statistical language") +
  scale_y_continuous(label = scales::comma) +
  ylab("Number of jobs") +
  labs(
    title = "Comparing statistical languages",
    subtitle = "Number of job postings on Indeed.com, 2020/01/12"
    ) +
  scale_fill_manual(
    "Free?",
    labels = c("True", "False"),
    values = c("#116E8A", "slategray")
    ) +
  ggthemes::theme_pander(base_size = 17) +
  # theme_ipsum() +
  theme(legend.position = "bottom")
```
]

<br>

.footnote[This graph is created from the search results obtained via [www.indeed.com](https://www.indeed.com) (on Jan 12, 2020), using Grant McDermott's code for `ggplot2`, see lecture 1 in his [Data science for economists course](http://github.com/uo-ec607/lectures).]

---

# Why R and RStudio? (cont.)

### Data science positivism

- Next to Python, R has become the *de facto* language for data science, with a cutting edge *machine learning toolbox*.
- See: [The Popularity of Data Science Software](http://r4stats.com/articles/popularity/)
- R is open-source with a very active community of users spanning academia and industry.

--

### Bridge to actuarial science, econometrics and other tools

- R has all of the statistics and econometrics support, and is amazingly adaptable as a “glue” language to other programming languages and APIs.
- R does not try to be everything to everyone. The RStudio IDE and ecosystem allow for further, seemless integration (with e.g. python, keras, tensorflow or C).
- Widely used in actuarial undergraduate programs 

--

### Disclaimer + Read more

- It's also the language that we know best.
- If you want to read more: [R-vs-Python](https://blog.rstudio.com/2019/12/17/r-vs-python-what-s-the-best-for-language-for-data-science/), [when to use Python or R](https://www.datacamp.com/community/blog/when-to-use-python-or-r) or [Hadley Wickham on the future of R](https://qz.com/1661487/hadley-wickham-on-the-future-of-r-python-and-the-tidyverse/)
---

name: workshop-outline

# Today's Outline

.pull-left[

* [Prologue](#prologue)

* [Data sets used in the course](#data-sets)

* [Knowing me, knowing you: <br> statistical and machine learning](#knowing)

  - Supervised and unsupervised learning
  - Regression and classification
  - Statistical modeling: the two cultures
  - Creating models in R and tidy model output with {broom}

* [Machine learning foundations](#basics)

  - Model accuracy and loss functions
  - Overfitting and bias-variance tradeoff
  - Data splitting, Resampling methods with {caret} and {rsample}
]

.pull-right[

* [Machine learning foundations](#basics)

  - Parameter tuning with {caret}, {rsample} and {purrr}.

* [Target and feature engineering](#engineering)

  - Data leakage
  - Pre-processing steps
  - Specifying blue-prints with {recipes}
  - Putting it all together: {recipes} and {caret}/{rsample}

* [Regression models](#regression)

  - GLMs with {glm}
  - GAMs with {mgcv}
  - Regularized (G)LMs with {glmnet}.

]


---
name: map-ML-world
class: right, middle, clear
background-image: url("img/map_ML_world.jpg")
background-size: 45% 
background-position: left


.KULbginline[Some roadmaps to explore the ML landscape...] 

<img src = "img/AI_ML_DL.jpg" height = "350px" />

.font60[Source: [Machine Learning for Everyone In simple words. With real-world examples. Yes, again.](https://vas3k.com/blog/machine_learning/)]


---

name: map-ML-world
class: right, middle, clear
background-image: url("img/main_types_ML.jpg")
background-size: 85% 
background-position: middle


---

class: inverse, center, middle
name: data-sets

# Data sets used in the course 

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>


---

name: data-sets-used

# Data sets used in this course - MTPL <img src="img/pipe.png" class="title-hex"> <img src="img/dplyr.png" class="title-hex"> <img src="img/ggplot2.png" class="title-hex">

We will use the Motor Third Party Liability data set. There are 163,231 policyholders in this data set. 

The frequency of claiming (`nclaims`) and corresponding severity (`avg`, the amount paid on average per claim reported by a policyholder) are the .KULbginline[target variables] in this data set. 

Predictor variables are: 

* the exposure-to-risk, the duration of the insurance coverage (max. 1 year)
* factor variables, e.g. gender, coverage, fuel
* continuous, numeric variables, e.g. age of the policyholder, age of the car
* spatial information: postal code (in Belgium) of the municipality where the policyholder resides.

More details in [Henckaerts et al. (2018, Scandinavian Actuarial Journal)](https://katrienantonio.github.io/projects/2019/06/13/machine-learning/#data-driven) and [Henckaerts et al. (2019, arxiv)](https://katrienantonio.github.io/projects/2019/06/13/machine-learning/#tree-based-pricing).

---

name: data-sets-used

# Data sets used in this course - MTPL <img src="img/pipe.png" class="title-hex"> <img src="img/dplyr.png" class="title-hex"> <img src="img/ggplot2.png" class="title-hex">

You can load the data from a script in the `scripts` folder as follows:

```{r eval = FALSE}
# install.packages("rstudioapi")
dir <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(dir)
mtpl_orig <- read.table('./data/P&Cdata.txt', 
                                  header = TRUE)
mtpl_orig <- as_tibble(mtpl_orig)
```

If you work in the R notebook stored in the `notebooks` folder, you can also go for:
```{r eval = FALSE}
# install.packages("here")
library(here)
dir <- here::here()   
setwd(dir) 
mtpl_orig <- read.table('./data/P&Cdata.txt', 
                                  header = TRUE)
mtpl_orig <- as_tibble(mtpl_orig)
```

Some basic exploratory steps with this data follow on the next sheet.

---

name: data-sets-used

# Data sets used in this course - MTPL <img src="img/pipe.png" class="title-hex"> <img src="img/dplyr.png" class="title-hex"> <img src="img/ggplot2.png" class="title-hex">

Note that the data `mtpl_orig` uses capitals for the variable names

```{r echo=FALSE}
# install.packages("here")
library(here)
dir <- here::here()   
setwd(dir) 
mtpl_orig <- read.table('./data/P&Cdata.txt', 
                                  header = TRUE)
mtpl_orig <- as_tibble(mtpl_orig)
```

```{r, out.width='35%'}
mtpl_orig %>% slice(1:3) %>% select(-LONG, -LAT) %>% kable(format = 'html')
```

We change this to lower case variables, and rename `exp` to `expo`.

```{r, eval = F}
mtpl <- mtpl_orig %>%
  # rename all columns 
  rename_all(function(.name) {
    .name %>% 
      # replace all names with the lowercase versions
      tolower 
    })
mtpl <- rename(mtpl, expo = exp)
```

---

class: clear
name: first-steps-MTPL

```{r get-dir-notebook, echo = F}
dir <- here::here()   # where is this file stored?
setwd(dir)            # change working directory to directory stored in `dir`
```

```{r import-mtpl-data, echo = F}
mtpl_orig <- read.table('./data/P&Cdata.txt', 
                        header = TRUE)
mtpl_orig <- as_tibble(mtpl_orig)
```

```{r prepare-mtpl, echo = F}
mtpl <- mtpl_orig %>%
  # rename all columns 
  rename_all(function(.name) {
    .name %>% 
      # replace all names with the lowercase versions
      tolower 
      # replace all spaces with underscores is also useful, with `str_replace(" ", "-")`
    })
mtpl <- rename(mtpl, expo = exp)
```

.pull-left[
```{r first-inspection-mtpl, eval = F}
dim(mtpl)
```

```{r , echo = F}
dim(mtpl)
```

```{r first-risk-calculations-mtpl-2, eval = F}
mtpl %>% summarize(emp_freq = 
                      sum(nclaims) / sum(expo))
```

```{r, echo = F}
mtpl %>% summarize(emp_freq = sum(nclaims) / sum(expo)) %>% kable(format = 'html')
```

```{r first-risk-calculations-mtpl-3, eval = F}
mtpl %>% 
  group_by(sex) %>% 
  summarize(emp_freq = sum(nclaims) / sum(expo))
```

```{r, echo = F}
mtpl %>% 
  group_by(sex) %>% 
  summarize(emp_freq = sum(nclaims) / sum(expo)) %>% kable(format = "html") 
```
]

.pull-right[
```{r first-graphs-mtpl, eval = F}
g <- ggplot(mtpl, aes(nclaims)) + theme_bw() + 
     geom_bar(aes(weight = expo), col = KULbg, 
                               fill = KULbg) + 
     labs(y = "Abs freq (in exposure)") +
     ggtitle("MTPL - number of claims")
g
```

```{r , echo = F, out.width = '80%'}
g <- ggplot(mtpl, aes(nclaims)) + theme_bw() + 
     geom_bar(aes(weight = expo), col = KULbg, 
                               fill = KULbg) + 
     labs(y = "Abs freq (in exposure)") +
     ggtitle("MTPL - number of claims")
g
```

]

---

name: data-sets-used

# Data sets used in this course - Housing data <img src="img/pipe.png" class="title-hex"> <img src="img/dplyr.png" class="title-hex"> <img src="img/ggplot2.png" class="title-hex">

We will use the Ames Iowa housing data. There are 2,930 properties in the data set. 

The `Sale_Price` (target or response) was recorded along with 80 predictors, including:

* location (e.g. neighborhood) and lot information
* house components (garage, fireplace, pool, porch, etc.)
* general assessments such as overall quality and condition
* number of bedrooms, baths, and so on. 

More details in [De Cock (2011, Journal of Statistics Education)](http://ww2.amstat.org/publications/jse/v19n3/decock.pdf).

The raw data are at [`http://bit.ly/2whgsQM`](http://bit.ly/2whgsQM) but we will use a processed version found in the [`AmesHousing`](https://github.com/topepo/AmesHousing) package. 

You will load the data with the `make_ames()` function from the `AmesHousing` library, and store the data in the object `ames`:

```{r, eval = FALSE}
ames <- AmesHousing::make_ames()
```

A first exploration of the response variable then follows. 

---

class: clear
name: first-steps-MTPL

```{r get-ames-data, echo = F}
ames <- AmesHousing::make_ames()
```

.pull-left[

```{r first-inspection-ames, eval = F}
dim(ames)
```

```{r , echo = F}
dim(ames)
```

```{r risk-calculations-ames, eval = F}
ames %>% summarize(avg_price = mean(Sale_Price))
```

```{r, echo = F}
ames %>% summarize(avg_price = mean(Sale_Price)) %>% kable(format = 'html')
```

```{r risk-calculations-ames-2, eval = F}
ames %>% 
  group_by(Neighborhood) %>% 
  summarize(avg_price = mean(Sale_Price)) %>%
  slice(1:3)
```

```{r, echo = F}
ames %>% 
  group_by(Neighborhood) %>% 
  summarize(avg_price = mean(Sale_Price)) %>% slice(1:3) %>% kable(format = "html") 
```
]

.pull-right[

```{r distribution-response-ames, eval = F}
g_dens <- ggplot(ames, aes(x = Sale_Price)) + 
                                 theme_bw() +
          geom_density(col = KULbg, fill = KULbg, 
                                        alpha= .5) +
          ggtitle("Ames housing data - sale prices")
g_dens
```

```{r , echo = F, out.width = '80%'}
g_dens <- ggplot(ames, aes(x = Sale_Price)) + theme_bw() +
          geom_density(data = ames, col = KULbg, fill = KULbg, alpha = .5) +
          ggtitle("Ames housing data - sale prices")
g_dens
```

]

---

class: clear, center, middle

background-image: url("img/MnistExamples.png")
background-size: cover
background-size: 95% 
background-position: left

.font1000.bold[MNIST]

---

name: data-sets-used

# Data sets used in this course - MNIST

Not all data are in tabular format. 

We analyze an .KULbginline[image database] from the Modified National Institute of Standards and Technology, short [MNIST](https://en.wikipedia.org/wiki/MNIST_database). 

Working with MNIST will learn us how machine learning methods can be used to work with new data sources, such as images. 

.pull-left[
* Large database of 70,000 labeled images of handwritten digits.

* Images are preprocessed, i.e. scaled and centered.

* Classic test case for machine learning classification algorithms. Current models achieve an accuracy of more than 99.5%. 
]

.pull-right[
.center[
<img src = "img/neural_network_sample.gif" height = "350px" />
]
]


---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

To get warmed up, let's load the `mtpl` data and do some .KULbginline[basic investigations] into the variables. The idea is to get a feel for the data. 

Your starting point are the instructions in the R notebook `data_sets.nb` or the R script `data_sets.R` from the [course material](https://github.com/katrienantonio/workshop-ML). 

.hi-pink[Q]: you will work through the following exploratory steps.

1. Visualize the distribution of the `ageph` with a histogram.

2. For each age recorded in the data set `mtpl`: what is the total number of observations, the total exposure, and the corresponding total number of claims reported? 

3. Calculate the empirical claim frequency, per unit of exposure, for each age and picture it. Discuss this figure.

4. Repeat the above for `bm`, the level occupied by the policyholder in the Belgian bonus-malus scale. 

<br>

`r countdown(minutes = 10, right = "0%", bottom = "5%")`

]

---

class: clear

.pull-left[

For .hi-pink[Q.1] a histogram of `ageph`

```{r eval=FALSE}
ggplot(data = mtpl, aes(ageph)) + theme_bw() + 
      geom_histogram(binwidth = 2, col = "black", 
                                  fill = KULbg) +
      labs(y = "Absolute frequency") +
      ggtitle("MTPL - age policyholder")
```

```{r echo=FALSE, out.width='75%'}
g_hist_age <- ggplot(data = mtpl, aes(ageph)) + theme_bw() + 
              geom_histogram(binwidth = 2, col = KULbg, fill = KULbg, alpha = .5) +
              labs(y = "Absolute frequency") +
              ggtitle("MTPL - age policyholder")
g_hist_age
```


]

.pull-right[

For .hi-pink[Q.2] for each `ageph` recorded

```{r eval=FALSE}
mtpl %>% 
  group_by(ageph) %>% 
  summarize(tot_claims = sum(nclaims), 
            tot_expo = sum(expo), tot_obs = n())
```

```{r echo=FALSE, out.width='75%'}
freq_by_age <- mtpl %>% 
  group_by(ageph) %>% 
  summarize(tot_claims = sum(nclaims), tot_expo = sum(expo), tot_obs = n())


freq_by_age %>% slice(1:8) %>% kable(format = 'html')
```


]

---

class: clear

For .hi-pink[Q.3] for each `ageph` recorded
.pull-left[
```{r eval=FALSE}
freq_by_age <- mtpl %>% 
  group_by(ageph) %>% 
  summarize(emp_freq = sum(nclaims) / sum(expo))

ggplot(freq_by_age, aes(x = ageph, y = emp_freq)) + 
                                       theme_bw() +
  geom_bar(stat = "identity", color = KULbg, 
                        fill = KULbg, alpha = .5) +
  ggtitle("MTPL - empirical claim freq per 
                                age policyholder")
```

For .hi-pink[Q.4] recycle the above instructions and replace `ageph` with `bm`.


]
.pull-right[
```{r echo=FALSE, out.width='85%'}
freq_by_age <- mtpl %>% 
  group_by(ageph) %>% 
  summarize(emp_freq = sum(nclaims) / sum(expo))

ggplot(freq_by_age, aes(x = ageph, y = emp_freq)) + theme_bw() +
  geom_bar(stat = "identity", color = KULbg, fill = KULbg, alpha = .5) +
  ggtitle("MTPL - empirical claim freq per age policyholder")
```
]

---

class: inverse, center, middle
name: knowing

# Knowing me, knowing you: 
<br> <br>
# statistical and machine learning 

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

name: supervised-learning

# Supervised learning

.pull-left-alt[

Supervised learning builds ("learns") a .blue[model] $\color{#3b3b9a}{f}$ (*the Signal*) such that the .orange[outcome or target] $\color{#fb6107}{Y}$ can be written as

$$\color{#FFA500}{Y} = \color{#3b3b9a}{f}(\color{#e64173}{x_1, \ldots, x_p}) + \epsilon$$
with .pink[features] $\color{#e64173}{x_1, \ldots, x_p}$ and error term $\epsilon$ (*the Noise*).

Supervised learners construct .KULbginline[predictive models].

<br> <br>

.footnote[Picture taken from [Machine Learning for Everyone. In simple words. With real-world examples. Yes, again](https://vas3k.com/blog/machine_learning/)]

]

.pull-right-alt[

.center[
```{r out.width = '100%', echo=FALSE}
knitr::include_graphics("img/supervised_unsupervised_drawing.jpg")
```
]




]
---



name: unsupervised-learning

# Unsupervised learning

.pull-left-alt[

With unsupervised learning there is .KULbginline[NO] .orange[outcome or target] $\color{#FFA500}{Y}$, only the feature vector $\color{#e64173}{x = (x_1, \ldots, x_p)}$. 

Let $n$ denote the sample size and $p$ the number of features. 

Then, $\color{#e64173}{X}$ is the $n \times p$ matrix of features, with $\color{#e64173}{x}_{i,j}$ observation $i$ on variable or feature $j$.

Unsupervised learners construct .KULbginline[descriptive models], without any *supervising* output, letting the data "speak for itself".


]

.pull-right-alt[

.center[
```{r out.width = '80%', out.height = '55%', echo=FALSE}
knitr::include_graphics("img/K-means_drawing.jpg")
```
]

.footnote[Picture taken from [Machine Learning for Everyone. In simple words. With real-world examples. Yes, again](https://vas3k.com/blog/machine_learning/)]


]
---

class: clear

<br> <br>

.center[
```{r out.width = '90%', echo=FALSE}
knitr::include_graphics("img/supervised_unsupervised_robot.jpg")
```
]

.footnote[Picture taken from [this source](https://twitter.com/athena_schools/status/1063013435779223553).]

---

name: classification-vs-regression

# Regression and classification <img src="img/broom.png" class="title-hex">

.pull-left[

.KULbginline[Regression:] (cfr. AMES housing data)

problems with a .KULbginline[quantitative] response (e.g. `Sale_Price`)

```{r regression, fig.align = "center", echo = FALSE, out.width = "65%"}
library(ggplot2)
library(tidyverse)
ggplot2::theme_set(ggplot2::theme_light())

model_1 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames)

p_1 <- model_1 %>%
  broom::augment() %>%
  ggplot(aes(Gr_Liv_Area, Sale_Price)) + 
  geom_point(size = 1, alpha = 0.3) +
  geom_smooth(se = FALSE, method = "lm") +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("Regression with AMES housing data")

p_1
```

.footnote[See code in [notebook](link to the notebook).]
]

--

.pull-right[

.KULbginline[Classification:] (cfr. MNIST digit classification)

problems with a .KULbginline[qualitative] response (e.g. is this a six?)

.center[
```{r out.width = '55%', echo = FALSE}
knitr::include_graphics("img/classification_MNIST.png")
```
]

.footnote[Picture taken from [this source](https://www.oreilly.com/library/view/mastering-opencv-4/9781789344912/bff52211-85fb-4b6b-93d8-0f56bd4b4238.xhtml).]

]

---

name: what's-in-a-name

# What's in a name?

.KULbginline[Machine learning] constructs algorithms that learn from data. 

--

.KULbginline[Statistical learning] emphasizes statistical models and the assessment of uncertainty.

--

.KULbginline[Data science] applies mathematics, statistics, machine learning, engineering, etc. to extract knowledge form data.
--

> *"Data Science is statistics on a Mac `r fa(name = "apple", fill = KULbg)`. "*

.center[
<img src="img/ElementsStatLearning.png" alt="Drawing" style="height: 250px;"/>  &nbsp;&nbsp;&nbsp; <img src="img/ISL.png" alt="Drawing" style="height: 250px;"/> 
&nbsp;&nbsp;&nbsp; <img src="img/AppliedPredMod.png" alt="Drawing" style="height: 250px;"/> &nbsp;&nbsp;&nbsp; <img src="img/boehmke_greenwell.jpg" alt="Drawing" style="height: 250px;"/> &nbsp;&nbsp;&nbsp; <img src="img/molnar.png" alt="Drawing" style="height: 250px;"/>
]

Source: Brandon M. Greenwell on [Introduction to Machine Learning in `r fa(name = "r-project", fill = "#F92672")`](https://github.com/bgreenwell/intro-ml-r).

---

name: two-cultures

# Statistical modeling: the two cultures

Consider a vector of input variables $\color{#e64173}{x}$, being transformed into some vector of response variables $\color{#FFA500}{y}$ via a black box algorithm. 

.center[
<img src="img/Breiman_nature.png" alt="Drawing" style="width: 300px;"/>  
]

--

.pull-left[

.KULbginline[Statistical learning or data modeling culture]

* assume statistical model, estimate parameter values
* validate with goodness-of-fit tests and residual inspection

.center[
<img src="img/Breiman_data_modeling.png" alt="Drawing" style="width: 300px;"/>  
]
]
--

.pull-right[

.KULbginline[Machine learning or algo modeling culture]

* inside of the box is complex and unknown
* find algorithm $\color{#3b3b9a}{f}(\color{#e64173}{x})$ to predict $\color{#FFA500}{y}$
* measure performance by predictive accuracy.

.center[
<img src="img/Breiman_algo_modeling.png" alt="Drawing" style="width: 300px;"/>  
]

]

Source: Breiman (2001, Statistical Science) on *Statistical modeling: the two cultures.*
---

name: statistical-machine-learning

# Newspeak from the two cultures

<br> 
<br>

| Statistical learning           |  Machine learning
:------:|:-------------------------:|:-------------------------:
.KULbginline[origin] | statistics | computer science 
.KULbginline[*f(x)*] | model | algorithm
.KULbginline[emphasis] | interpretability, precision and uncertainty | large scale applicability, prediction accuracy
.KULbginline[jargon] | parameters, estimation | weights, learning
.KULbginline[CI] | uncertainty of parameters | no notion of uncertainty 
.KULbginline[assumptions] | explicit a priori assumption | no prior assumption, learn from the data

<br>

Source: read the blog [Why a mathematician, statistician and machine learner solve the same problem differently](https://blog.galvanize.com/why-a-mathematician-statistician-machine-learner-solve-the-same-problem-differently-2/)

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

As discussed in the lecture, many problems in ML can be approached as a .KULbginline[regression], .KULbginline[classification] or .KULbginline[clustering] problem. 

<br>

.hi-pink[Q]: consider the following .hi-pink[three problem settings] and .hi-pink[label them] as regression, classification or clustering.

<br>

1. In disability insurance: how do disability rates depend on the state of the economy (e.g. GDP)?

2. In MTPL insurance: predict whether a claim is attritional or large, *in casu* a claim that exceeds the threshold of 100 000 EUR?

3. How can we group customers based on the insurance products they bought from the company? 

]

---

name: models-in-R

# Creating models in R

We introduced and illustrated a first, simple model on the `ames` housing data.

The **formula** interface using R's [formula rules](https://cran.r-project.org/doc/manuals/r-release/R-intro.html#Formulae-for-statistical-models) to specify a *symbolic* representation of the terms:

* response ~ variable, with `model_fn` referring to the specific model function you want to use, e.g. `lm` for linear regression

```{r eval=FALSE}
model_fn(Sale_Price ~ Gr_Liv_Area, data = ames)
```

* response ~ variable_1 + variable_2

```{r eval=FALSE}
model_fn(Sale_Price ~ Gr_Liv_Area + Neighborhood, data = ames)
```

* response ~ variable_1 + variable_2 + their interaction

```{r eval=FALSE}
model_fn(Sale_Price ~ Gr_Liv_Area + Neighborhood + Neighborhood:Gr_Liv_Area, data = ames)
```

* shorthand for all predictors

```{r eval=FALSE}
model_fn(Sale_Price ~ ., data = ames)
```



---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

You will now fit some linear regression models on the `ames` housing data. 
<br> <br> 
You will explore the model fits with `base` R instructions as well as the functionalities offered by the `broom` package.
<br> <br>
.hi-pink[Q]: load the `ames` housing data set via `ames <- AmesHousing::make_ames()`

1. Fit a linear regression model with `Sale_Price` as response and `Gr_Liv_Area` as covariate. Store the resulting object as `model_1`.

2. Repeat your instruction, but now put it between brackets. What happens?

3. Inspect `model_1` with the following set of instructions

- `summary(___)`
- extract the fitted coefficients, using `___$coefficients`
- what happens with `summary(___)$coefficients`?
- extract fitted values, using `___$fitted.values`
- now try to extract the R<sup>2</sup> of this model. 

]

---

class: clear

```{r, echo = FALSE}
ames <- AmesHousing::make_ames()
```

.hi-pink[Q.1] Linear model with `Sale_Price` as a function of `Gr_Live_Area`

```{r, eval = FALSE}
model_1 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames)
```

```{r, echo = FALSE}
model_1 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames)
```


.hi-pink[Q.3] Check `model_1` - What happens - do you *like* this display?

```{r, eval = FALSE}
summary(model_1)
```

Now let's extract some meaningful information from `model_1` (using `base` R instructions)

.pull-left[

```{r, eval = FALSE}
model_1$coefficients
```

```{r, echo = FALSE}
coef(model_1)
```

```{r, eval = FALSE}
summary(model_1)$coefficients
```

```{r, echo = FALSE}
summary(model_1)$coefficients
```

]

.pull-right[

```{r eval = FALSE}
head(model_1$fitted.values)
```

```{r echo = FALSE}
head(model_1$fitted.values)
```

```{r eval = FALSE}
summary(model_1)$r.squared
```

```{r echo = FALSE}
summary(model_1)$r.squared
```


]

---

# Tidy model output <img src="img/broom.png" class="title-hex">

The package {broom} allows to summarize key information about statistical objects (e.g. a linear regression model) in so-called tidy tibbles. 

This makes it easy to report results, create plots and consistently work with large numbers of models at once. 

We briefly illustrate the three essential verbs of `broom`: `tidy()`, `glance()` and `augment()`.

```{r eval = FALSE}
model_1 %>% broom::tidy() 
```

```{r echo = FALSE}
model_1 %>% broom::tidy() %>% kable(format = 'html')
```

```{r eval = FALSE}
model_1 %>% broom::glance() 
```

```{r echo = FALSE}
model_1 %>% broom::glance() %>% kable(format = 'html')
```

---

# Tidy model output <img src="img/broom.png" class="title-hex">

The package {broom} allows to summarize key information about statistical objects (e.g. a linear regression model) in so-called tidy tibbles. 

This makes it easy to report results, create plots and consistently work with large numbers of models at once. 

We briefly illustrate the three essential verbs of `broom`: `tidy()`, `glance()` and `augment()`.

```{r eval = FALSE}
model_1 %>% broom::augment() %>% slice(1:5)
```

```{r echo = FALSE}
model_1 %>% broom::augment() %>% slice(1:5) %>% kable(format = 'html')
```

---

class: clear

.pull-left[

```{r eval = FALSE}
g_lm_1 <- ggplot(data = ames, 
                 aes(Gr_Liv_Area, Sale_Price)) + 
  theme_bw() +
  geom_point(size = 1, alpha = 0.3) +
  geom_smooth(se = TRUE, method = "lm") +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("Regression with AMES housing data")
g_lm_1
```

```{r echo = FALSE, out.width = '70%'}
g_lm_1 <- ggplot(data = ames, aes(Gr_Liv_Area, Sale_Price)) + theme_bw() +
  geom_point(size = 1, alpha = 0.3) +
  geom_smooth(se = TRUE, method = "lm") +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("Regression with AMES housing data")
g_lm_1
```

]

.pull-right[

```{r eval = FALSE}
g_lm_2 <- model_1 %>% broom::augment() %>% 
ggplot(aes(Gr_Liv_Area, Sale_Price)) + 
    theme_bw() +
    geom_point(size = 1, alpha = 0.3) +
    geom_line(aes(y = .fitted), col = KULbg) +
    scale_y_continuous(labels = scales::dollar) +
    ggtitle("Regression with AMES housing data")
g_lm_2

```


```{r echo = FALSE, out.width = '70%'}
model_1 %>% broom::augment() %>% ggplot(aes(Gr_Liv_Area, Sale_Price)) + 
            theme_bw() +
            geom_point(size = 1, alpha = 0.3) +
            geom_line(aes(y = .fitted), col = KULbg) +
            scale_y_continuous(labels = scales::dollar) +
            ggtitle("Regression with AMES housing data")
```


]

---

class: inverse, center, middle
name: basics

# Machine learning foundations

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

# Predictive modeling

How to use the observed data to learn or to estimate the unknown $\color{#3b3b9a}{f}(.)$?

$$\begin{eqnarray*}
\color{#FFA500}{y} &=& \color{#3b3b9a}{f}(\color{#e64173}{x_1,x_2,\ldots,x_p})+\epsilon.
\end{eqnarray*}$$

--

How do I .KULbginline[estimate] $\color{#3b3b9a}{f}(.)$ - one way to phrase *all questions* that underly statistical & machine learning.

--

.font140[.KULbginline[Take-aways]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`] &nbsp; - &nbsp; main reasons we want to .KULbginline[learn about] $\color{#3b3b9a}{f}(.)$ 

--

.pull-left[
.font120[.KULbginline[prediction]] 
<br>
predict the target $\color{#FFA500}{y}$ as $\hat{\color{#3b3b9a}{f}}(\color{#e64173}{x})$
<br>
.font140[`r fa(name = "fas fa-box", fill = KULbg)`] - as black box setting? 
<br> <br>
.font120[.KULbginline[inference]]
<br>
how does target $\color{#FFA500}{y}$ depend on features $\color{#e64173}{x}$? 
<br>
.font140[`r fa(name = "fas fa-box-open", fill = KULbg)`] - as white box setting? 
]
--
.pull-right[

```{r fig.align = 'center', fig.width = 8, fig.height = 7, echo = FALSE}
knitr::include_graphics("img/prediction_inference.png")
```
]


---

name: prediction-error

# Prediction errors

Why we're stuck with .KULbginline[irreducible error]

assume $\hat{\color{#3b3b9a}{f}}$ and $\color{#e64173}{x}$ given, then

$$
\begin{aligned}
  \mathop{E}\left[ \left\{ \color{#FFA500}{y} - \hat{\color{#FFA500}{y}} \right\}^2 \right]
  &=
  \mathop{E}\left[ \left\{ \color{#3b3b9a}{f}(\color{#e64173}{x}) + \epsilon + \hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}) \right\}^2 \right] \\
  &= \underbrace{\left[ \color{#3b3b9a}{f}(\color{#e64173}{x}) - \hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}) \right]^2}_{\text{Reducible}} + \underbrace{\mathop{\text{Var}} \left( \color{#e64173}{\epsilon} \right)}_{\text{Irreducible}}
\end{aligned}
$$

In .KULbginline[less math]:

- if $\epsilon$ exists, then $\color{#e64173}{x}$ cannot perfectly explain $\color{#FFA500}{y}$

- so even if $\hat{\color{#3b3b9a}{f}} = \color{#3b3b9a}{f}$, we still have irreducible error.

--

Thus, to form our .KULbginline[best predictors], we will .KULbginline[minimize reducible error].

---

name: model-accuracy

# Model accuracy

We assess .KULbginline[model] or .KULbginline[predictive accuracy] by 
evaluating how well predictions actually match observed data.

--

Use .KULbginline[loss functions], i.e. metrics that compare predicted values to actual values.

--

.pull-left[.KULbginline[Regression], use e.g. the .hi-pink[Mean Squared Error (MSE)]

$$\begin{eqnarray*}
\frac{1}{n} \sum_{i=1}^n (\color{#FFA500}{y}_i - \hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_i))^2,
\end{eqnarray*}$$

Recall: $\color{#FFA500}{y}_i - \hat{\color{#FFA500}{y}}_i = \color{#FFA500}{y}_i - \hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_i)$ is the prediction error.

Objective `r fa(name = "fas fa-bullseye", fill = KULbg)` : minimize!]

--

.pull-right[.KULbginline[Classification], use e.g. the .hi-pink[cross-entropy] or .hi-pink[log loss]
$$\begin{eqnarray*}
-\frac{1}{n} \sum_{i=1}^n \left(\color{#FFA500}{y}_i \cdot \log{(p)_i} + (1-\color{#FFA500}{y}_i) \cdot \log{(1-p_i)}\right).
\end{eqnarray*}$$
 
Objective `r fa(name = "fas fa-bullseye", fill = KULbg)` : minimize!

]

--

<br>

.KULbginline[Many other useful loss functions] (e.g. deviance in regression, Gini index in classification).

.font140[.KULbginline[Take-away]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`] &nbsp; - &nbsp; a loss function emphasizes certain types of errors over others `r fa(name = "fas fa-long-arrow-alt-right", fill = KULbg)` pick a meaningful one!


---

name: data-splitting

# Data splitting

We fit our model on past data $\{(\color{#e64173}{x}_1,\color{#FFA500}{y}_1),(\color{#e64173}{x}_2,\color{#FFA500}{y}_2),\ldots, (\color{#e64173}{x}_n,\color{#FFA500}{y}_n)\}$
and get $\hat{\color{#3b3b9a}{f}}$. 

*What we want*: how does our model .KULbginline[generalize] to new, unseen data $(\color{#e64173}{x}_0,\color{#FFA500}{y}_0)$, or: &nbsp; is $\hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_0)$ close to $\color{#FFA500}{y}_0$?

.left-column[

.KULbginline[Training set]

* to develop, to train, to tune, to compare different settings, ...

.KULbginline[Test set]

* to obtain unbiased estimate of final model's performance.

]

.right-column[

```{r fig.align = 'center', out.width = '70%', echo=FALSE}
knitr::include_graphics("img/data_splitting.png")
```

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .footnote[Picture taken from [Introduction to Machine Learning in `r fa(name = "r-project", fill = "#F92672")`](https://github.com/bgreenwell/intro-ml-r).]


]
---

# Data splitting in base

We first demonstrate the splitting of the `ames` housing data into a training and test set, using `base` R instructions. 

.pull-left[

```{r eval = FALSE}
set.seed(123) #<< 
index_1 <- sample(1 : nrow(ames), 
                  size = round(nrow(ames) * 0.7))  
train_1 <- ames[index_1, ]   
test_1  <- ames[-index_1, ]  

nrow(train_1)/nrow(ames)
```

]

.pull-right[

Use `set.seed()` for reproducibility.

]

---

# Data splitting in base

We first demonstrate the splitting of the `ames` housing data into a training and test set, using `base` R instructions. 

.pull-left[

```{r eval = FALSE}
set.seed(123) 
index_1 <- sample(1 : nrow(ames), #<< 
                  size = round(nrow(ames) * 0.7))  #<< 
train_1 <- ames[index_1, ]   
test_1  <- ames[-index_1, ]  

nrow(train_1)/nrow(ames)
```

]

.pull-right[

Sample indices from `1 : nrow(ames)` such that in total 70% of the records is selected.

Vector `index_1` now stores the row numbers of the selected records.

]

---

# Data splitting in base

We first demonstrate the splitting of the `ames` housing data into a training and test set, using `base` R instructions. 

.pull-left[

```{r eval = FALSE}
set.seed(123) 
index_1 <- sample(1 : nrow(ames), 
                  size = round(nrow(ames) * 0.7))   
train_1 <- ames[index_1, ]   #<<
test_1  <- ames[-index_1, ]  

nrow(train_1)/nrow(ames)
```

]

.pull-right[

Put the selected records in training set `train_1` by subsetting the original data frame `ames` with the row numbers stored in `index_1`.

]

---

# Data splitting in base

We first demonstrate the splitting of the `ames` housing data into a training and test set, using `base` R instructions. 

.pull-left[

```{r eval = FALSE}
set.seed(123) 
index_1 <- sample(1 : nrow(ames), 
                  size = round(nrow(ames) * 0.7))   
train_1 <- ames[index_1, ]   
test_1  <- ames[-index_1, ]  #<<

nrow(train_1)/nrow(ames)
```

]

.pull-right[

Put the not selected records in test set `test_1`.

]

---

# Data splitting in base

We first demonstrate the splitting of the `ames` housing data into a training and test set, using `base` R instructions. 

.pull-left[

```{r eval = FALSE}
set.seed(123) 
index_1 <- sample(1 : nrow(ames), 
                  size = round(nrow(ames) * 0.7))   
train_1 <- ames[index_1, ]   
test_1  <- ames[-index_1, ]  

nrow(train_1)/nrow(ames)   #<<
```

]

.pull-right[

What is the ratio of the number of records in `train_1` versus original data set `ames`?

]

---

# Data splitting in {caret}

The {caret} package - short for Classification And REgression Training - contains functions to streamline the model training process for complex regression and classification problems.

With the {caret} package, the function `createDataPartition` will do the job. 

.pull-left[

```{r eval = FALSE}
library(caret) #<<
set.seed(123)  #<<
index_2 <- caret::createDataPartition(
                    y = ames$Sale_Price, 
                    p = 0.7, 
                    list = FALSE)
train_2 <- ames[index_2, ]
test_2  <- ames[-index_2, ]

nrow(train_2)/nrow(ames) 
```

]

.pull-right[

Load the library {caret}.

Use `set.seed()` for reproducibility.

]

---

# Data splitting in {caret}

The {caret} package - short for Classification And REgression Training - contains functions to streamline the model training process for complex regression and classification problems.

With the {caret} package, the function `createDataPartition` will do the job. 

.pull-left[

```{r eval = FALSE}
library(caret) 
set.seed(123)  
index_2 <- caret::createDataPartition(    #<<
                    y = ames$Sale_Price,  #<<
                    p = 0.7,          #<<
                    list = FALSE)     #<<
train_2 <- ames[index_2, ]
test_2  <- ames[-index_2, ]

nrow(train_2)/nrow(ames) 
```

]

.pull-right[

`createDataPartition` takes in `y` the vector of outcomes of the data set we wish to split. `createDataPartition` will do stratified sampling based on levels of `y` (for factor) or groups determined by the percentiles of `y` (for numeric).

The percentage of data that goes to training is `p`.

`list = FALSE` tells the function not to store the results in a list, but in a matrix (here: with 1 column)

]



---

# Data splitting in {rsample}  <img src="img/rsample.png" class="title-hex">

The {rsample} package, part of the {tidymodels} initiative of RStudio, is home to a wide very variety of resampling functions. 

The documentation is at [rsample: the basics](https://tidymodels.github.io/rsample/articles/Basics.html).

.pull-left[
```{r eval = FALSE}
library(rsample) #<<
set.seed(123)  #<<
split_1  <- rsample::initial_split(ames, prop = 0.7)
train_3  <- training(split_1)
test_3   <- testing(split_1)

nrow(train_3)/nrow(ames)
```
]

.pull-right[

Load the `rsample` package.

Use `set.seed()` for reproducibility.

]

---

# Data splitting in {rsample}  <img src="img/rsample.png" class="title-hex">

The {rsample} package, part of the {tidymodels} initiative of RStudio, is home to a wide very variety of resampling functions. 

The documentation is at [rsample: the basics](https://tidymodels.github.io/rsample/articles/Basics.html).

.pull-left[
```{r eval = FALSE}
library(rsample)
set.seed(123) 
split_1  <- rsample::initial_split(ames, prop = 0.7)  #<<
train_3  <- training(split_1)
test_3   <- testing(split_1)

nrow(train_3)/nrow(ames)
```
]

.pull-right[

`initial_split` from the {rsample} package. 

Split the data `ames` into a training set and testing set.

`prop` is the proportion of data to be retained as training

]

---

# Data splitting in {rsample} <img src="img/rsample.png" class="title-hex">

The {rsample} package, part of the {tidymodels} initiative of RStudio, is home to a wide very variety of resampling functions. 

The documentation is at [rsample: the basics](https://tidymodels.github.io/rsample/articles/Basics.html).

.pull-left[
```{r eval = FALSE}
library(rsample)
set.seed(123) 
split_1  <- rsample::initial_split(ames, prop = 0.7)  
train_3  <- training(split_1) #<<
test_3   <- testing(split_1)  #<<

nrow(train_3)/nrow(ames)
```
]

.pull-right[

The result of `rsample::initial_split` is an `rset` object.

It is stored in `split_1` and ready for inspection.

Apply the functions `training` and `test` to this object to extract the data in each split.

]

---

# Data splitting in {rsample}  <img src="img/rsample.png" class="title-hex">

As a check, we plot the `Sale_Price` as available in the train (in black) vs test (in red) data sets, created by each of the three demonstrated methods.

```{r plot-train-test, echo=FALSE}
ames <- AmesHousing::make_ames()

set.seed(123) 
index_1 <- sample(1 : nrow(ames), 
                  size = round(nrow(ames) * 0.7))   
train_1 <- ames[index_1, ]   
test_1  <- ames[-index_1, ] 

library(caret) 
set.seed(123)  
index_2 <- caret::createDataPartition(
                    y = ames$Sale_Price,  
                    p = 0.7,          
                    list = FALSE)     
train_2 <- ames[index_2, ]
test_2  <- ames[-index_2, ]

library(rsample)
set.seed(123) 
split_1  <- rsample::initial_split(ames, prop = 0.7)  
train_3  <- training(split_1) 
test_3   <- testing(split_1) 

p_1 <- ggplot(train_1, aes(x = Sale_Price)) + theme_bw() +
       geom_density(trim = TRUE) +
       geom_density(data = test_1, trim = TRUE, col = "red") +
       ggtitle("base R")

p_2 <- ggplot(train_2, aes(x = Sale_Price)) + theme_bw() +
    geom_density(trim = TRUE) +
    geom_density(data = test_2, trim = TRUE, col = "red") +
    theme(axis.title.y = element_blank(),
          axis.ticks.y = element_blank(),
          axis.text.y = element_blank()) +
    ggtitle("caret") 

p_3 <- ggplot(train_3, aes(x = Sale_Price)) + theme_bw() +
    geom_density(trim = TRUE) + 
    geom_density(data = test_3, trim = TRUE, col = "red") +
    theme(axis.title.y = element_blank(),
          axis.ticks.y = element_blank(),
          axis.text.y = element_blank()) +
    ggtitle("rsample")
```

```{r echo = FALSE, fig.align = 'center', fig.width = 10, fig.height = 4.5, dev = "svg"}
# side-by-side plots
gridExtra::grid.arrange(p_1, p_2, p_3, nrow = 1)

# clean up
rm(p_1, p_2, p_3)
```

---

name: overfitting

# Overfitting

The .KULbginline[Signal and the Noise] discussion!

--

Which of the following three models (in green-blue-ish) will best generalize to new data? 

```{r overfitting-linear-regression, echo=FALSE, fig.width=7, fig.height=6/3, out.width = "100%"}
# Simulate some data 
n <- 100
set.seed(8451)
KULbg <- "#116E8A"
df <- tibble::tibble(
  x = runif(n, min = -2, max = 2),
  y = rnorm(n, mean = 1 + 2*x + x^2, sd = 1)
)
p <- ggplot(df, aes(x, y)) + 
  geom_point(alpha = 0.3) +  
  theme_bw()
p1 <- p + 
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, size = 1.5, color = KULbg) +
  ggtitle("Underfitting")
p2 <- p + 
  geom_smooth(method = "lm", formula = y ~ x + I(x^2), se = FALSE, size = 1.5, color = KULbg) +
  ggtitle("Just right?")
p3 <- p + 
  geom_smooth(method = "loess", span = 0.1, se = FALSE, size = 1.5, color = KULbg) +
  ggtitle("Overfitting")
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

.footnote[Inspired by Brandon Greenwell's [Introduction to Machine Learning in `r fa(name = "r-project", fill = "#F92672")`](https://github.com/bgreenwell/intro-ml-r).]

---

# Overfitting (cont.)

With a small training error, but large test error, the model is .KULbginline[overfitting] or working too hard!

--

The expected value of the .hi-pink[test MSE]:

$$\begin{eqnarray*}
    E\left(\color{#FFA500}{y}_0-\hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_0)\right)^2 &=& \text{Var}(\hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_0))+[\text{Bias}(\hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_0))]^2+\text{Var}(\epsilon).
    \end{eqnarray*}$$

--

.font140[.KULbginline[In general]] - with more flexible methods

* variance .font140[`r fa(name = "arrow-circle-up", fill = KULbg)`] and bias .font140[`r fa(name = "arrow-circle-down", fill = KULbg)`]

* their relative rate of change determines whether the test error increases or decreases

--

.font140[.KULbginline[Take-aways]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`]

* U-shape curves of .hi-pink[test MSE] w.r.t model flexibility

* the .hi-pink[bias-variance tradeoff] is central to quality prediction.

---

# Bias-variance trade off

<br> 

.center[
<img src="img/bias_variance_trade_off.png" alt="Drawing" style="width: 600px;"/>  
]

Source: James et al. (2013) on [http://faculty.marshall.usc.edu/gareth-james/ISL/](An introduction to statistical learning).

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

Data are generated from: $\color{#FFA500}{y} = \color{#3b3b9a}{f}(\color{#e64173}{x})+\epsilon$, with the black curve as the true $\color{#3b3b9a}{f}$. The orange (linear regression), blue (smoothing splines) and green (smoothing splines) curves are three estimates for $\color{#3b3b9a}{f}$, with increasing level of complexity.

.hi-pink[Q]: which model do you prefer (orange, blue, green) for each of the following examples? Why? Discuss with your neighbour.

.center[
<img src="img/2.9 ISL.png" alt="Drawing" style="width: 500px;"/> 
]

.footnote[Example from James et al. (2013) on [http://faculty.marshall.usc.edu/gareth-james/ISL/](An introduction to statistical learning).]

]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

Data are generated from: $\color{#FFA500}{y} = \color{#3b3b9a}{f}(\color{#e64173}{x})+\epsilon$, with the black curve as the true $\color{#3b3b9a}{f}$. The orange (linear regression), blue (smoothing splines) and green (smoothing splines) curves are three estimates for $\color{#3b3b9a}{f}$, with increasing level of complexity.

.hi-pink[Q]: which model do you prefer (orange, blue, green) for each of the following examples? Why? Discuss with your neighbour.

.center[
<img src="img/2.10 ISL.png" alt="Drawing" style="width: 500px;"/> 
]

.footnote[Example from James et al. (2013) on [http://faculty.marshall.usc.edu/gareth-james/ISL/](An introduction to statistical learning).]

]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

Data are generated from: $\color{#FFA500}{y} = \color{#3b3b9a}{f}(\color{#e64173}{x})+\epsilon$, with the black curve as the true $\color{#3b3b9a}{f}$. The orange (linear regression), blue (smoothing splines) and green (smoothing splines) curves are three estimates for $\color{#3b3b9a}{f}$, with increasing level of complexity.

.hi-pink[Q]: which model do you prefer (orange, blue, green) for each of the following examples? Why? Discuss with your neighbour.

.center[
<img src="img/2.11 ISL.png" alt="Drawing" style="width: 500px;"/> 
]

.footnote[Example from James et al. (2013) on [http://faculty.marshall.usc.edu/gareth-james/ISL/](An introduction to statistical learning).]

]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

.font120[The *K*-nearest neighbors (KNN) classifier]

- take the *K* observations in the training data set that are 'closest' to test observation $\color{#e64173}{x}_0$, calculate 


$$\begin{eqnarray*}
\text{Pr}(\color{#FFA500}{Y}=j|\color{#e64173}{X} = \color{#e64173}{x}_0) &=& \frac{1}{K} \sum_{i \in \mathcal{N}_0} \mathbb{I}(\color{#FFA500}{y}_i=j).
\end{eqnarray*}$$

- KNN then assigns the test observation $\color{#e64173}{x}_0$ to the class $j$ with the highest probability, e.g. with *K=3* (from James et al., 2013)

.center[
<img src="img/2.14 ISL.png" alt="Drawing" style="width: 300px;"/> 
]

.hi-pink[Q]: is KNN a supervised learning or unsupervised learning method? Discuss with your neighbour.

]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

.font120[The *K*-nearest neighbors (KNN) classifier (cont.)]

Now compare KNN with *K* equals 1, 10 and 100.

.center[
<img src="img/KNN_K_1.png" alt="Drawing" style="height: 250px;"/> 
&nbsp;&nbsp;&nbsp; <img src="img/KNN_K_10.png" alt="Drawing" style="height: 250px;"/> 
&nbsp;&nbsp;&nbsp; <img src="img/KNN_K_100.png" alt="Drawing" style="height: 250px;"/> 
]

.hi-pink[Q]: which classifier do you prefer? Which of these classifiers is under-fitting, which one is over-fitting?

]

---

name: black-white-box

# A toolbox of methods - but no *free lunch*

.font140[.KULbginline[Take-aways]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`] : trade-off *flexibility* and *interpretability*!

.center[
```{r out.width = '55%', echo=FALSE}
knitr::include_graphics("img/ISL_overview_models.png")
```
]

Source: James et al. (2013) on [An introduction to statistical learning](http://faculty.marshall.usc.edu/gareth-james/ISL/).

---

name: black-white-box
class: clear

.font140[.KULbginline[Take-aways]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`] : *feature pre-processing*, *automatic feature selection* and *tuning parameters*.

.center[
```{r out.width = '60%', echo=FALSE}
knitr::include_graphics("img/Applied_Pred_overview_models.jpg")
```
]

Source: Kuhn & Johnson (2013) on [Applied predictive modeling](http://appliedpredictivemodeling.com/).

---

# Tuning parameters 

.pull-left[Finding the optimal level of flexibility highlights the bias-variance tradeoff.

.hi-pink[Bias] : the error that comes from inaccurately estimating $\color{#3b3b9a}{f}$.

.hi-pink[Variance] : the amount $\hat{\color{#3b3b9a}{f}}$ would change with a different training sample.

.font140[.KULbginline[Take-aways]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`] : high variance models more prone to overfitting

* use .hi-pink[resampling methods] to reduce this risk

* hyperparameters (or *tuning parameters*) control complexity, 
and thus the bias-variance trade-off

* identify their optimal setting, e.g. with a *grid search*

* no analytic expression for these hyperparameters.
]

.pull-right[

```{r bias-variance-knn, echo = FALSE, fig.width = 7, fig.height = 3, out.width = "100%"}
library(caret)
# Simulate some nonlinear monotonic data
set.seed(123)  # for reproducibility
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
df <- data.frame(x, y) %>%
  filter(x < 4.5)

KULbg <- "#116E8A"

# Single biased model fit
bias_model <- lm(y ~ I(x^3), data = df)
df$predictions <- predict(bias_model, df)
p_1 <- ggplot(df, aes(x, y)) +
  geom_point(alpha = .3) +
  geom_line(aes(x, predictions), size = 1.5, color = KULbg) +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) + theme_bw() +
  ggtitle("Biased model fit")

p_1
# Single high variance model fit
variance_model <- knnreg(y ~ x, k = 3, data = df)
df$predictions <- predict(variance_model, df)
p_2 <- ggplot(df, aes(x, y)) +
  geom_point(alpha = .3) +
  geom_line(aes(x, predictions), size = 1.5, color = KULbg) +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) + theme_bw() +
  ggtitle("High variance model fit")

p_2
```

.footnote[Code from Boehmke & Greenwell (2019, Chapter 2) on [Hands-on machine learning with R](https://koalaverse.github.io/homlr/).]

]



---

# Tuning parameters via grid search

.pull-left-alt[

.center[
```{r out.width = '85%', echo = FALSE}
knitr::include_graphics("img/flow_chart_applied_predictive_modeling.jpg")
```
]
]

.pull-right-alt[

.hi-pink[Model training & validation phase]

- define a set of candidate values (a *grid*)

- assess model utility across the candidates (use clever *resampling*)

- choose the optimal settings (optimize *loss*)

- refit the model on entire training data with final tuning parameters

- evaluate performance of the model on the test data (under `r fa('fas fa-lock', fill = KULbg)`). 

.hi-pink[Model selection] 

- repeat the above steps for different models 

- compare performance of these models that will generalize to new data (via test data, under `r fa('fas fa-lock', fill = KULbg)`).

.footnote[Flow chart from Kuhn & Johnson (2013) on [Applied predictive modeling](http://appliedpredictivemodeling.com/).]
]

---

# Resampling methods

In [Data splitting](#data-splitting), we discussed *training* and *test* set. Let's now dive deeper into *resampling* methods.

.hi-KUL[Validation set] (visual inspired by [Ed Rubin's course](https://github.com/edrubin/EC524W20))

- we hold out a subset of the training data (e.g. 30%) and then evaluate the model on this held out validation set

- calculate the loss function on this validation set, as approximation of the true test error

- .font140[`r fa(name = "far fa-thumbs-down", fill = KULbg)`] high variability + inefficient use of data

- picture .hi-KUL[validation set (30%)] and .hi-pink[training set (70%)]

```{r, data-validation-set, include = F, cache = T}
library(tidyverse)
# generate data
X = 40
Y = 12
set.seed(12345)
v_df = expand_grid(
  x = 1:X,
  y = 1:Y
) %>% mutate(grp = sample(
  x = c("Train", "Validate"),
  size = X * Y,
  replace = T,
  prob = c(0.7, 0.3)
)) %>% mutate(
  grp_2 = c(
    rep("Validate", sum(grp == "Validate")),
    rep("Train", sum(grp == "Train"))
  )
)
```

```{r, plot-validation-set, echo = F, dependson = "data-validation-set", fig.height = 2.3, cache = T}
ggplot(data = v_df, aes(x, y, fill = grp_2, color = grp_2)) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual(" ", values = c("white", KULbg)) +
scale_color_manual(" ", values = c(red_pink , KULbg)) +
theme_void() +
theme(legend.position = "none")
```

---

# Resampling methods (cont.)

In [Data splitting](#data-splitting), we discussed *training* and *test* set. Let's now dive deeper into *resampling* methods.

.hi-KUL[*k* fold cross validation] (visual inspired by [Ed Rubin's course](https://github.com/edrubin/EC524W20))

- divide training data into *k* equally sized groups (e.g. .hi-KUL[group 1] on the picture)

- iterate over the *k* groups, treating each as validation set once (and train model on the other *k-1* groups) (e.g. get .hi-KUL[MSE<sub>1<sub/>] corresponding to fold 1)

- average the folds' loss to estimate the true test error

- .font140[`r fa(name = "far fa-thumbs-up", fill = KULbg)`] greater accuracy (compared to validation set).

```{r, data-CV-set, include = F, cache = T}
library(tidyverse)
# Generate data
X = 40
Y = 12
set.seed(12345)
cv_df = expand_grid(
  x = 1:X,
  y = 1:Y
) %>% mutate(
  id = 1:(X*Y),
  grp = sample(X * Y) %% 5 + 1
)
# Find groups
a = seq(1, X*Y, by = X*Y/5)
b = c(a[-1] - 1, X*Y)
```

```{r, plot-CV-set, echo = F, dependson = "data-CV-set", fig.height = 2.3, cache = T}
ggplot(
  data = cv_df,
  aes(x, y, color = between(id, a[1], b[1]), fill = between(id, a[1], b[1]))
) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", KULbg)) +
scale_color_manual("", values = c(red_pink, KULbg)) +
theme_void() +
theme(legend.position = "none")
```

---

# Resampling methods (cont.)

In [Data splitting](#data-splitting), we discussed *training* and *test* set. Let's now dive deeper into *resampling* methods.

.hi-KUL[*k* fold cross validation] (visual inspired by [Ed Rubin's course](https://github.com/edrubin/EC524W20))

- divide training data into *k* equally sized groups (e.g. .hi-KUL[group 1] on the picture)

- iterate over the *k* groups, treating each as validation set once (and train model on the other *k-1* groups) (e.g. get .hi-KUL[MSE<sub>1<sub/>] corresponding to fold 1)

- average the folds' loss to estimate the true test error

- .font140[`r fa(name = "far fa-thumbs-up", fill = KULbg)`] greater accuracy (compared to validation set).

```{r, plot-CV3-set, echo = F, dependson = "data-CV-set", fig.height = 2.3, cache = T}
ggplot(
  data = cv_df,
  aes(x, y, color = between(id, a[3], b[3]), fill = between(id, a[3], b[3]))
) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", KULbg)) +
scale_color_manual("", values = c(red_pink, KULbg)) +
theme_void() +
theme(legend.position = "none")
```

---

# Resampling methods (cont.)

In [Data splitting](#data-splitting), we discussed *training* and *test* set. Let's now dive deeper into *resampling* methods.

.hi-KUL[*k* fold cross validation] (picture from [Boehmke & Greenwell](https://koalaverse.github.io/homlr/))

.center[
```{r out.width = '95%', echo=FALSE}
knitr::include_graphics("img/k_fold_CV.png")
```
]

---

# Resampling methods (cont.)

In [Data splitting](#data-splitting), we discussed *training* and *test* set. Let's now dive deeper into *resampling* methods.

.hi-KUL[Leave-one-out cross validation] (visual inspired by [Ed Rubin's course](https://github.com/edrubin/EC524W20))

- each observation takes a turn as the validation set (e.g. get .hi-KUL[MSE<sub>3<sub/>])

- other *n-1* observations are the training set

- average the folds' loss to estimate the true test error

- .font140[`r fa(name = "far fa-thumbs-down", fill = KULbg)`] very computationally demanding. 

```{r, data-loocv, include = F, cache = T}
# Generate data
X = 40
Y = 12
loocv_df = expand_grid(
  x = 1:X,
  y = -(1:Y)
) %>% mutate(
  i = 1:(X * Y),
  grp_1 = if_else(i == 1, "Validate", "Train"),
  grp_2 = if_else(i == 2, "Validate", "Train"),
  grp_3 = if_else(i == 3, "Validate", "Train"),
  grp_4 = if_else(i == 4, "Validate", "Train"),
  grp_5 = if_else(i == 5, "Validate", "Train"),
  grp_n = if_else(i == X*Y, "Validate", "Train")
)
```


```{r, plot-LOOCV-set, echo = F, dependson = "data-loocv", fig.height = 2.3, cache = T}
ggplot(data = loocv_df, aes(x, y, fill = grp_3, color = grp_3)) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", KULbg)) +
scale_color_manual("", values = c(red_pink, KULbg)) +
theme_void() +
theme(legend.position = "none")
```

---

# Resampling methods in {caret}

We set up 5-fold cross validation using the {caret} package.

.pull-left[

```{r eval = FALSE}
set.seed(123)  
cv_folds <- caret::createFolds(y = ames$Sale_Price,        #<<
                               k = 5, list = TRUE,     #<<
                               returnTrain = TRUE)     #<<
```

```{r eval = FALSE}
str(cv_folds)
```

```{r echo = FALSE}
set.seed(123)  
cv_folds <- caret::createFolds(ames$Sale_Price, 
                        k = 5, list = TRUE, 
                        returnTrain = TRUE)
str(cv_folds)
```
]

.pull-right[

The `createFolds` function from {caret} splits the data into `k` groups.

`list = TRUE` indicates that the results should be stored in a list

`returnTrain = TRUE` indicates that the values returned (and stored) in the elements of the list are - per fold - the row numbers of the observations selected for training.

]

---

# Resampling methods in {caret}

We set up 5-fold cross validation using the {caret} package.

.pull-left[

```{r eval = FALSE}
set.seed(123)  
cv_folds <- caret::createFolds(y = ames$Sale_Price,        
                               k = 5, list = TRUE,     
                               returnTrain = TRUE)    
```

```{r eval = FALSE}
str(cv_folds) #<<
```

```{r echo = FALSE}
set.seed(123)  
cv_folds <- caret::createFolds(ames$Sale_Price, 
                        k = 5, list = TRUE, 
                        returnTrain = TRUE)
str(cv_folds)
```
]

.pull-right[

Inspect the list `cv_folds` that was returned by `createFolds(.)`. 

This list has `k` elements, each storing the row numbers of the observations in the training set of the fold under consideration.

]

---

# Resampling methods in {caret} <img src="img/purrr.png" class="title-hex">

.pull-left[

```{r eval = FALSE}
mean(ames[cv_folds$Fold1, ]$Sale_Price) #<<
```

```{r echo = FALSE}
mean(ames[cv_folds$Fold1, ]$Sale_Price)
```

```{r eval = FALSE}
map_dbl(cv_folds,                     
        function(x) {                   
          mean(ames[x, ]$Sale_Price)    
        })                              
```

```{r echo = FALSE}
map_dbl(cv_folds,
        function(x) {
          mean(ames[x, ]$Sale_Price)
        })
```

]

.pull-right[

We calculate the average `Sale_Price` per fold, that is: we average the `Sale_Price` over all observations selected in the training set of a particular fold. 

That would go as follows, for `Fold1` in the list `cv_folds`

```{r eval=FALSE}
mean(ames[cv_folds$Fold1, ]$Sale_Price)
```

and similarly for `Fold2`, ..., `Fold5`. 

]

---

# Resampling methods in {caret} <img src="img/purrr.png" class="title-hex">

.pull-left[

```{r eval = FALSE}
mean(ames[cv_folds$Fold1, ]$Sale_Price) 
```

```{r echo = FALSE}
mean(ames[cv_folds$Fold1, ]$Sale_Price)
```

```{r eval = FALSE}
map_dbl(cv_folds,                     #<<
        function(x) {                 #<<  
          mean(ames[x, ]$Sale_Price)  #<<  
        })                            #<<  
```

```{r echo = FALSE}
map_dbl(cv_folds,
        function(x) {
          mean(ames[x, ]$Sale_Price)
        })
```



]

.pull-right[

We apply the function `mean(ames[___, ]$Sale_Price)` over all `k` elements of the list `cv_folds`.

`map_dbl(.x, .f)` is one of the `map` functions from the `purrr` package (part of `tidyverse`), used for functional programming in R.

`map_dbl(.x, .f)` applies function `.f` to each element of list `.x`.

The result is a double-precision vector, hence `map_dbl` and not just `map`.

Btw, it is a historical anomaly that R has two names for its floating-point vectors, `double` and `numeric`.

]

---

# Resampling methods in {rsample} <img src="img/rsample.png" class="title-hex">

.pull-left[

```{r eval = FALSE}
set.seed(123)  
cv_rsample <- rsample::vfold_cv(ames, v = 5) #<<
cv_rsample$splits
```

```{r echo = FALSE}
set.seed(123)  
cv_rsample <- rsample::vfold_cv(ames, v = 5)
cv_rsample$splits
```

]

.pull-right[

The function `vfold_cv` splits the data into `v` groups (called folds) of equal size. 

]

---

# Resampling methods in {rsample} <img src="img/rsample.png" class="title-hex">

.pull-left[

```{r eval = FALSE}
set.seed(123)  
cv_rsample <- rsample::vfold_cv(ames, v = 5) 
cv_rsample$splits  #<<
```

```{r echo = FALSE}
set.seed(123)  
cv_rsample <- rsample::vfold_cv(ames, v = 5)
cv_rsample$splits
```

]

.pull-right[

The function `vfold_cv` splits the data into `v` groups (called folds) of equal size. 

We store the result of `vfold_cv` in the object `cv_rsample`.

The resulting object stores `v` resamples of the original data set.

]

---

# Resampling methods in {rsample} <img src="img/rsample.png" class="title-hex">

.pull-left[

```{r eval = FALSE}
set.seed(123)  
cv_rsample <- rsample::vfold_cv(ames, v = 5) 
```

```{r echo = FALSE}
set.seed(123)  
cv_rsample <- rsample::vfold_cv(ames, v = 5)
```

```{r eval = FALSE}
cv_rsample$splits[[1]] #<<
```

```{r echo = FALSE}
cv_rsample$splits[[1]]
```

```{r eval = FALSE}
cv_rsample$splits[[1]] %>% analysis() %>% dim()
```

```{r echo = FALSE}
cv_rsample$splits[[1]] %>% analysis() %>% dim()
```

```{r eval = FALSE}
cv_rsample$splits[[1]] %>% assessment() %>% dim()
```

```{r echo = FALSE}
cv_rsample$splits[[1]] %>% assessment() %>% dim()
```

]

.pull-right[

Inspect the composition of the first resample: 

2,344 (out of 2,930) observations go to the analysis data (for training, i.e. `v-1` folds), 

586 (out of 2,930) observations go to the assessment data (for testing, the final fold).   

]

---

# Resampling methods in {rsample} <img src="img/rsample.png" class="title-hex">

.pull-left[

```{r eval = FALSE}
set.seed(123)  
cv_rsample <- rsample::vfold_cv(ames, v = 5) 
```

```{r echo = FALSE}
set.seed(123)  
cv_rsample <- rsample::vfold_cv(ames, v = 5)
```

```{r eval = FALSE}
cv_rsample$splits[[1]] 
```

```{r echo = FALSE}
cv_rsample$splits[[1]]
```

```{r eval = FALSE}
cv_rsample$splits[[1]] %>% analysis() %>% dim() #<<
```

```{r echo = FALSE}
cv_rsample$splits[[1]] %>% analysis() %>% dim()
```

```{r eval = FALSE}
cv_rsample$splits[[1]] %>% assessment() %>% dim() #<<
```

```{r echo = FALSE}
cv_rsample$splits[[1]] %>% assessment() %>% dim()
```

]

.pull-right[

Inspect the composition of the first resample: 

get the dimensions (`dim()`) of the analysis data (`analysis()`) of the first resample 

get the dimensions (`dim()`) of the assessment data (`assessment()`) of the first resample. 

]

---

# Resampling methods in {rsample} <img src="img/purrr.png" class="title-hex"> <img src="img/rsample.png" class="title-hex">

.pull-left[

```{r eval = FALSE}
map_dbl(cv_rsample$splits,
        function(x) {
          mean(rsample::analysis(x)$Sale_Price)
        })
```

```{r echo = FALSE}
map_dbl(cv_rsample$splits,
        function(x) {
          mean(rsample::analysis(x)$Sale_Price)
        })
```

```{r eval = FALSE}
map_dbl(cv_rsample$splits,
        function(x) {
          nrow(rsample::analysis(x))
        })
```

```{r echo = FALSE}
map_dbl(cv_rsample$splits,
        function(x) {
          nrow(rsample::analysis(x))
        })
```

]

.pull-right[

As before, use `map_dbl(.x, .f)` to apply a function `.f` over all elements of a list `.x`. 

Here the list is stored in `cv_rsample$splits`, with `v = 5` elements. 

]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

Now you're going to combine the .hi-pink[resampling and model fitting instructions] and set up a first example of .hi-pink[tuning a parameter] over a grid of possible values: the *K* in a .hi-pink[KNN regression model]. 

You will use the `caret::knnreg` function to fit a KNN, as follows (with formula syntax)

```{r eval=FALSE}
knnreg(y ~ x_1 + x_2, k = ___, data = ___)
```

You will use a simulated $(x,y)$ data set with one feature $x$ and a continuous, numeric target $y$ (see notebook instructions).

.hi-pink[Q]: explore the use of `caret::knnreg` on the simulated $(x,y)$ data set

1. Plot the simulated data in a scatterplot.
2. Pick a value for *K*, and fit the *K*-nearest neighbour regression of $y$ as a function of $x$. 
3. Get the fitted values $\hat{y}$. Try to use `broom`. What happens? Then, use the `predict(.object, .newdata)` that comes with `knnreg`.
4. Add the fitted values to your scatterplot with `geom_line(data = ___, aes(___, ___))`.
5. Play with the value of *K*.


]

---

class: clear

.pull-left[

.hi-pink[Q.1] Plot the data in a scatterplot

```{r eval=FALSE}
ggplot() + theme_bw() +
  geom_point(data = df, aes(x, y), alpha = .3) +
  scale_y_continuous(limits = c(-1.75, 1.75), 
                             expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), 
                             expand = c(0, 0))
```

```{r echo=FALSE, out.width='65%'}
set.seed(123)  # for reproducibility
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
df <- data.frame(x, y) %>%
  filter(x < 4.5)

ggplot() + theme_bw() +
  geom_point(data = df, aes(x, y), alpha = .3) +
  scale_y_continuous(limits = c(-1.75, 1.75), 
                     expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), 
                     expand = c(0, 0))
```

]

.pull-right[

.hi-pink[Q.2-3]: fit a *KNN* with a handpicked value of *K*

```{r eval = FALSE}
k <- 3
fit <- caret::knnreg(y ~ x, k = k, data = df)
fit %>% broom::augment()  # does not work!
df$pred <- predict(fit, df)
df %>% slice(1:5)
```


```{r echo = FALSE}
k <- 3
fit <- caret::knnreg(y ~ x, k = k, data = df)
df$pred <- predict(fit, df)
df %>% slice(1:6) %>% kable(format = 'html')
```


]

---

class: clear

.pull-left[

.hi-pink[Q.4] Plot the data in a scatterplot, add the fitted values

```{r eval=FALSE}
ggplot(data = df) + theme_bw() + 
  geom_point(aes(x, y), alpha = .3) +
  geom_line(aes(x, pred), color = KULbg, size = 1.0) +
  scale_y_continuous(limits = c(-1.75, 1.75), 
                                expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), 
                                expand = c(0, 0))
```

.hi-pink[Q.5] Change the value of *K* in your code and see what happens.


]

.pull-right[

```{r echo=FALSE, out.width='100%'}
ggplot(data = df) + theme_bw() + 
  geom_point(aes(x, y), alpha = .3) +
  geom_line(aes(x, pred), color = KULbg, size = 1.0) +
  scale_y_continuous(limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0))
```


]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

Now you're going to combine the .hi-pink[resampling and model fitting instructions] and set up a first example of .hi-pink[tuning a parameter] over a grid of possible values: the *K* in a .hi-pink[KNN regression model]. 

Try to fill in the blanks in the code printed below for some manual tuning of *K*.

```{r caret-manual-tuning, eval = FALSE}
k_results <- NULL
k <- ___     # specify a grid of values for k

# fit the different models and store the results
for(i in ___) {
  df_sim <- df
  fit <- knnreg(___ ~ ___, k = ___, data = ___)
  df_sim$predictions <- predict(___, ___)
  df_sim$model <- paste0("k = ", stringr::str_pad(k[i], 3, pad = " "))
  k_results <- rbind(k_results, df_sim)
}
# visualize: (x,y) + fitted values, one plot per value of k
ggplot() +
  geom_point(data = ___, aes(x, y), alpha = .3) +
  geom_line(data = k_results, aes(___, ___), color = col, size = 1.0) +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) +
  facet_wrap(~ ___)
```


]

---

class: clear

Here is the code:

```{r eval = FALSE}
k_results <- NULL
k <- c(2, 5, 10, 20, 50, 150)

# fit the different models
for(i in seq_along(k)) {
  df_sim <- df
  fit <- knnreg(y ~ x, k = k[i], data = df_sim)
  df_sim$pred <- predict(fit, df_sim)
  df_sim$model <- paste0("k = ", stringr::str_pad(k[i], 3, pad = " "))
  k_results <- rbind(k_results, df_sim)
}

ggplot() + theme_bw() +
  geom_point(data = df, aes(x, y), alpha = .3) +
  geom_line(data = k_results, aes(x, pred), color = KULbg, size = 1.0) +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) +
  facet_wrap(~ model)
```

---

class: clear

```{r echo = FALSE, fig.width = 10, fig.height = 6.0, dev = "svg"}
k_results <- NULL
k <- c(2, 5, 10, 20, 50, 150)

# fit the different models
for(i in seq_along(k)) {
  df_sim <- df
  fit <- knnreg(y ~ x, k = k[i], data = df_sim)
  df_sim$pred <- predict(fit, df_sim)
  df_sim$model <- paste0("k = ", stringr::str_pad(k[i], 3, pad = " "))
  k_results <- rbind(k_results, df_sim)
}

ggplot() + theme_bw() +
  geom_point(data = df, aes(x, y), alpha = .3) +
  geom_line(data = k_results, aes(x, pred), color = KULbg, size = 1.0) +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) +
  facet_wrap(~ model)
```


---

# Training a model with {caret}

.pull-left[

```{r eval=FALSE}
set.seed(123)
cv <- trainControl(method = "cv", number = 5, #<<
                   returnResamp = "all",  
                   selectionFunction = "best")
hyper_grid <- expand.grid(k = seq(2, 150, by = 2)) 
knn_fit <- train(y ~ x, data = df, method = "knn", 
                        trControl = cv, 
                        tuneGrid = hyper_grid)
knn_fit$bestTune
```

]

.pull-right[

Use `trainControl` from {caret} to set some control parameters that will be used in the actual `train` function. 

Here, we use `method = cv` and `number = 5` for 5-fold cross validation. 

]

---

# Training a model with {caret}

.pull-left[

```{r eval=FALSE}
set.seed(123)
cv <- trainControl(method = "cv", number = 5, 
                   returnResamp = "all",  #<<
                   selectionFunction = "best") #<<
hyper_grid <- expand.grid(k = seq(2, 150, by = 2)) 
knn_fit <- train(y ~ x, data = df, method = "knn", 
                        trControl = cv, 
                        tuneGrid = hyper_grid)
knn_fit$bestTune
```

]

.pull-right[

In `trainControl` we put `returnResamp = "all"` to store all resampled summary metrics.

`selectionFunction = "best"` specifies how we select the optimal tuning parameter. With `"best"` the value that minimizes the performance (here: RMSE) is selected. 

Alternative: `selectionFunction = "oneSE"` applies the one standard error rule. 

]

---

# Training a model with {caret}

.pull-left[

```{r eval=FALSE}
set.seed(123)
cv <- trainControl(method = "cv", number = 5, 
                   returnResamp = "all",  
                   selectionFunction = "best") 
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))  #<<
knn_fit <- train(y ~ x, data = df, method = "knn", 
                        trControl = cv, 
                        tuneGrid = hyper_grid)
knn_fit$bestTune
```

]

.pull-right[

Set the grid of *K*-values that will be searched. 

`expand.grid` creates a data frame with one row for each value of *K* to consider. 

]

---

# Training a model with {caret}

.pull-left[

```{r eval=FALSE}
set.seed(123)
cv <- trainControl(method = "cv", number = 5, 
                   returnResamp = "all",  
                   selectionFunction = "best") 
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))  
knn_fit <- train(y ~ x, data = df, method = "knn", #<<
                        trControl = cv, #<<
                        tuneGrid = hyper_grid) #<<
knn_fit$bestTune
```

]

.pull-right[

{caret} will `train` the method `knn` using the settings in `trControl = cv`, across the values of *K* stored in `tuneGrid = hyper_grid`.

The data `df` and formula `y ~ x` are used.

]

---

# Training a model with {caret}

.pull-left[

```{r eval=FALSE}
set.seed(123)
cv <- trainControl(method = "cv", number = 5, 
                   returnResamp = "all",  
                   selectionFunction = "best") 
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))  
knn_fit <- train(y ~ x, data = df, method = "knn", 
                        trControl = cv, 
                        tuneGrid = hyper_grid) 
knn_fit$bestTune  #<<
```

```{r echo=FALSE}
set.seed(123)
cv <- trainControl(method = "cv", number = 5, 
                   returnResamp = "all",  
                   selectionFunction = "best") 
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))  
knn_fit <- train(y ~ x, data = df, method = "knn", 
                        trControl = cv, 
                        tuneGrid = hyper_grid) 
knn_fit$bestTune  #<<
```

]

.pull-right[

We retrieve the optimal value of the tuning parameter, according to the `selectionFunction`. 

For the folds created here and with `selectionFunction = "best"` the optimal *K* value is 28. 

What happens when you change to `selectionFunction = "oneSE"`?

]

---

# Training a model with {caret}

.pull-left[

```{r echo=FALSE, fig.width = 10, fig.height = 6.0}
set.seed(123)
cv <- trainControl(method = "cv", number = 5, 
                   returnResamp = "all",  
                   selectionFunction = "best") 
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))  
knn_fit <- train(y ~ x, data = df, method = "knn", 
                        trControl = cv, 
                        tuneGrid = hyper_grid) 
knn_fit$bestTune  

ggplot() + theme_bw() +
  geom_line(data = knn_fit$results, aes(k, RMSE)) +
  geom_point(data = knn_fit$results, aes(k, RMSE)) +
  geom_point(data = filter(knn_fit$results, k == as.numeric(knn_fit$bestTune)),
             aes(k, RMSE),
             shape = 21,
             fill = "yellow",
             color = "black",
             stroke = 1,
             size = 4) +
  scale_y_continuous("Error (RMSE)")
```

]

.pull-right[

```{r echo=FALSE, fig.width = 10, fig.height = 6.0}
set.seed(123)
cv <- trainControl(method = "cv", number = 5, 
                   returnResamp = "all",  
                   selectionFunction = "oneSE") 
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))  
knn_fit <- train(y ~ x, data = df, method = "knn", 
                        trControl = cv, 
                        tuneGrid = hyper_grid) 
knn_fit$bestTune  

ggplot() + theme_bw() +
  geom_line(data = knn_fit$results, aes(k, RMSE)) +
  geom_point(data = knn_fit$results, aes(k, RMSE)) +
  geom_point(data = filter(knn_fit$results, k == as.numeric(knn_fit$bestTune)),
             aes(k, RMSE),
             shape = 21,
             fill = "yellow",
             color = "black",
             stroke = 1,
             size = 4) +
  scale_y_continuous("Error (RMSE)")
```

]

---

# Training a model with {rsample} <img src="img/rsample.png" class="title-hex"> <img src="img/purrr.png" class="title-hex">

.pull-left[

Our starting point is the simulated data stored in `df`, resampled with 5-fold cross-validation.

```{r eval=FALSE}
set.seed(123)  # for reproducibility
cv_rsample <- vfold_cv(df, 5)
cv_rsample$splits 
```

```{r echo=FALSE}
set.seed(123)  # for reproducibility
cv_rsample <- vfold_cv(df, 5)
cv_rsample$splits 
```
]

.pull-right[

We fit the *KNN* on the holdout data in split *s*, using a given *K* value. 

```{r eval=FALSE}
holdout_results <- function(s, k_val) {
  # Fit the model to the analysis data in split s
  df_train <- analysis(s)
  mod <- knnreg(y ~ x, k = k_val, data = df_train)
  # Get the remaining group
  holdout <- assessment(s)
  # Get predictions with the holdout data set
  res <- predict(mod, newdata = holdout)
  # Return observed and predicted values 
  #                            on holdout set
  res <- tibble(obs = holdout$y, pred = res)
  res
}
```

]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

Now you're going to combine the .hi-pink[resampling and model fitting instructions] and set up a first example of .hi-pink[tuning a parameter] over a grid of possible values: the *K* in a .hi-pink[KNN regression model]. 

.hi-pink[Q]: use the function `holdout_results(.s, .k)` as defined on the previous sheet. You will use this function to calculate the RMSE<sub>k</sub> of fold *k*. 

1. Specify a grid of values of *K*, store it in `hyper_grid`. Use `expand.grid(.)`

2. Pick one of the resamples stored in `cv_rsample$splits` and pick a value from the grid. Calculate the RMSE on the holdout data of this split. 

3. For all values in the tuning grid, calculate the RMSE averaged over all folds, and the corresponding standard error. 

4. Use the results from .hi-pink[Q.3] to pick the value of *K* via minimal RMSE.

5. Pick the largest value of *K* such that the corresponding RMSE is below the minimal RMSE from .hi-pink[Q.4] plus its corresponding SE. 

]

---

class: clear

.pull-left[

.hi-pink[Q.1] We set up the grid

```{r eval=FALSE}
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))  
hyper_grid %>% slice(1:3) 
```

```{r echo=FALSE}
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))  
hyper_grid %>% slice(1:3) %>% kable()
```

.hi-pink[Q.2] We apply the function `holdout_results(.s, .k)` on the 
third resample, with the first value for *K* in the grid.

```{r eval=FALSE}
res <- holdout_results(cv_rsample$splits[[3]], 
                                  hyper_grid[1, ])
sqrt(sum((res$obs - res$pred)^2)/nrow(res))
```

```{r echo=FALSE}
holdout_results <- function(s, k_val) {
  # Fit the model to the analysis data in split s
  df_train <- analysis(s)
  mod <- knnreg(y ~ x, k = k_val, data = df_train)
  # Get the remaining group
  holdout <- assessment(s)
  # Get predictions with the holdout data set
  res <- predict(mod, newdata = holdout)
  # Return observed and predicted values 
  #                            on holdout set
  res <- tibble(obs = holdout$y, pred = res)
  res
}
res <- holdout_results(cv_rsample$splits[[3]], hyper_grid[1, ])
sqrt(sum((res$obs - res$pred)^2)/nrow(res))
```
]

.pull-right[

.hi-pink[Q.3] Mean RMSE over the 5 folds and corresponding SE.

```{r eval=FALSE}
RMSE <- numeric(nrow(hyper_grid))
SE <- numeric(nrow(hyper_grid))
for(i in 1:nrow(hyper_grid)){
  cv_rsample$results <- map(cv_rsample$splits,
                      holdout_results,
                      hyper_grid[i, ])
  res <- map_dbl(cv_rsample$results, 
                 function(x) mean((x$obs - x$pred)^2))
  RMSE[i] <- mean(sqrt(res)) ; SE[i] <- sd(sqrt(res))
}


```

.hi-pink[Q.4] Choose *K* via minimal RMSE 

```{r echo=FALSE}
RMSE <- numeric(nrow(hyper_grid))
SE <- numeric(nrow(hyper_grid))
for(i in 1:nrow(hyper_grid)){
  cv_rsample$results <- map(cv_rsample$splits,
                      holdout_results,
                      hyper_grid[i, ])
  res <- map_dbl(cv_rsample$results, 
                 function(x) mean((x$obs - x$pred)^2))
  RMSE[i] <- mean(sqrt(res)) ; SE[i] <- sd(sqrt(res))
}

df <- tibble(RMSE, SE, k = hyper_grid$k)
df <- df %>% mutate(lower = RMSE-SE, upper = RMSE + SE)
best <- df %>% filter(RMSE == min(RMSE)) 
best %>% kable(format = 'html')
```

.hi-pink[Q.5] Choose *K* via the one-standard-error rule
```{r echo=FALSE}
one_SE <- df %>% filter(RMSE <= best$upper) %>% filter(k == max(k)) 
one_SE %>% kable(format = 'html')
```

]

---

class: clear


```{r one-SE-rule, echo=FALSE, fig.width = 10, fig.height = 6}
big_g <- ggplot(data = df) + theme_bw() +
  geom_line(aes(k, RMSE)) +
  geom_point(aes(k, RMSE)) +
  geom_pointrange(aes(k, RMSE, ymin = lower, ymax = upper), alpha = 0.5) +
  geom_point(data = best,
             aes(k, RMSE),
             shape = 21,
             fill = "yellow",
             color = "black",
             stroke = 1,
             size = 2) +
  geom_point(data = one_SE,
             aes(k, RMSE),
             shape = 21,
             fill = "yellow",
             color = "black",
             stroke = 1,
             size = 2) +
  geom_hline(data = best,
            aes(yintercept = upper)) +
  scale_y_continuous("Error (RMSE)")
big_g
```

---

# Putting it all together

.pull-left[

During the tuning process we inspect plots like the one on the right. 

.font140[.KULbginline[Take-aways]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`] &nbsp; &nbsp; *Less is more*: 

- we prefer simple over more complex 

- choose tuning parameters based on the numerically optimal value .KULbginline[OR]

- choose a simpler model that is within a certain tolerance of the
numerically best value 

- use the .hi-pink['one-standard-error' rule].

With the selected tuning parameters, we refit the model on the complete training set and use it to predict the test set (under `r fa('fas fa-lock', fill = KULbg)`). 

]

.pull-right[

```{r echo=FALSE, dependson='one-SE-rule', out.width='85%'}
big_g
```

]

---

class: inverse, center, middle
name: engineering

# Target and feature engineering


<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

# What is feature engineering?

.pull-left-alt[

.center[
<img src="img/feature_engineering_Kuhn.jpg" alt="Drawing" style="width: 165px;"/>
<br> <br> <img src="img/boehmke_greenwell.jpg" alt="Drawing" style="width: 165px;"/>  
]

]

.pull-right-alt[
Feature engineering:

- applies .hi-pink[pre-processing steps] to predictor (features) variables

- .hi-pink[creates new input features] from your existing ones (e.g. network features derived from a social network in a fraud detection model).

Target engineering: 

* transforms the response variable (or target) to improve the performance of a predictive model.

The goal is to .KULbginline[make models more effective].

See Kuhn & Johnson (2019) on [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/) for a detailed discussion. 

]

---

name: black-white-box
class: clear

.font140[.KULbginline[Take-aways]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`] : *different models* have *different sensitivities* to the type of target and feature values in the model.

.center[
```{r out.width = '65%', echo=FALSE}
knitr::include_graphics("img/Applied_Pred_overview_models.jpg")
```
]

Source: Kuhn & Johnson (2013) on [Applied predictive modeling](http://appliedpredictivemodeling.com/).

---

# Target engineering <img src="img/rsample.png" class="title-hex"> 

We load the `ames` data set from the {AmesHousing} package and apply a .hi-pink[stratified split] of the data into a training (70%) and test (30%) set.

We stratify on the distribution of the target variable `Sale_Price` using the `strata` argument in `rsample::initial_split`.

```{r eval=FALSE}
ames <- AmesHousing::make_ames()
set.seed(123)  
split  <- rsample::initial_split(ames, prop = 0.7, 
                                       `strata = "Sale_Price"`) 
ames_train  <- rsample::training(split)
ames_test   <- rsample::testing(split)
```

```{r echo=FALSE}
ames <- AmesHousing::make_ames()
set.seed(123)  
split  <- rsample::initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- rsample::training(split)
ames_test   <- rsample::testing(split)
```

We check the distribution of `Sale_Price` in both `ames_train` and `ames_test`.

```{r eval=FALSE}
summary(ames_train$Sale_Price)
summary(ames_test$Sale_Price)
```

```{r echo=FALSE}
summary(ames_train$Sale_Price)
summary(ames_test$Sale_Price)
```
---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

Inference with linear models often assumes normally distributed errors.

.hi-pink[Q]: let's examine whether the following models satisfy this assumption

```{r two-linear-models, eval=TRUE}
m_1 <- lm(Sale_Price ~ Year_Built, data = ames_train)
m_2 <- lm(log(Sale_Price) ~ Year_Built, data = ames_train)
```

1. Get the residuals for `m_1` and `m_2`. Hint: think `broom`.

2. Plot a histogram of the residuals. Is normality a meaningful assumption?

3. If the code you wrote is quite repetitive, try to rewrite using the `map` functions from `purrr`. 
]

---

class: clear

.pull-left[

.hi-pink[Q.1] using the linear model objects `m_1` and `m_2` we get the residuals as follows

```{r eval=TRUE}
res_1 <- m_1 %>% broom::augment()
res_2 <- m_2 %>% broom::augment()
```

```{r eval=FALSE}
res_1 %>% slice(1:2) %>% select(Sale_Price, 
                                Year_Built, .resid)
```

```{r echo=FALSE}
res_1 %>% slice(1:2) %>% select(Sale_Price, Year_Built, .resid) %>% kable()
```

.hi-pink[Q.2] to plot the residuals of `m_1`

```{r eval=FALSE}
g_res_1 <- ggplot(data = res_1, aes(.resid)) + 
  theme_bw() +
  geom_histogram(bins = 75, col = col, fill = fill) +
  ylab(NULL) + ggtitle("AMES - original target") +
  xlab("Residuals")
g_res_1
```
]

.pull-right[

```{r echo=FALSE, out.width='100%'}
library(gridExtra)
m_1 <- lm(Sale_Price ~ Year_Built, data = ames_train)
m_2 <- lm(log(Sale_Price) ~ Year_Built, data = ames_train)

res_1 <- m_1 %>% broom::augment()
res_2 <- m_2 %>% broom::augment()

g_res_1 <- ggplot(data = res_1, aes(.resid)) + theme_bw() +
    geom_histogram(bins = 75, col = KULbg, fill = KULbg, alpha = .5) +
    ylab(NULL) + ggtitle("AMES - original target") +
    xlab("Residuals")

g_res_2 <- ggplot(data = res_2, aes(.resid)) + theme_bw() +
    geom_histogram(bins = 75, col = KULbg, fill = KULbg, alpha = 0.5) +
    ylab(NULL) + ggtitle("AMES - log transformed target") +
    xlab("Residuals")

gridExtra::grid.arrange(g_res_1, g_res_2, nrow = 1)

```

]

---

class: clear

.pull-left[
.hi-pink[Q.3] To avoid the repetitive code from the previous sheet, we explore using `map_dbl(.x, .f)` and `map2_dbl(.x, .y, .f)`

```{r eval=FALSE}
models <- c("Non-log transformed model residuals", 
            "Log transformed model residuals")

l <- list(
  m1 = lm(Sale_Price ~ Year_Built, data = ames_train),
  m2 = lm(log(Sale_Price) ~ Year_Built, data = 
                                          ames_train)
)

# with map_df
f_1 <- map_df(l, function(x){broom::augment(x)}) #<<
# or even better with map2_df
f_2 <- map2_df(l, models, function(x,y){  #<<
        broom::augment(x) %>% mutate(model = y)}) #<<

g <- ggplot(data = f_2, aes(.resid)) + theme_bw() +
    geom_histogram(bins = 75, col = KULbg, 
                   fill = KULbg, alpha = .5) +
    facet_wrap(~ model, scales = "free_x") +
    ylab(NULL) +
    xlab("Residuals")
g
```
]

.pull-right[

```{r echo=FALSE}
models <- c("Non-log transformed model residuals", 
            "Log transformed model residuals")

l <- list(
  m1 = lm(Sale_Price ~ Year_Built, data = ames_train),
  m2 = lm(log(Sale_Price) ~ Year_Built, data = ames_train)
)

# try map_df
f_1 <- map_df(l, function(x){broom::augment(x)})
# or even better map2_df
f_2 <- map2_df(l, models, function(x,y){ broom::augment(x) %>% mutate(model = y)})

g <- ggplot(data = f_2, aes(.resid)) + theme_bw() +
    geom_histogram(bins = 75, col = KULbg, fill = KULbg, alpha = .5) +
    facet_wrap(~ model, scales = "free_x") +
    ylab(NULL) +
    xlab("Residuals")
g
```

]

---

# Feature engineering steps

Examples of common pre-processing steps:

* Some models (e.g. KNN, Lasso, neural networks) require that the predictor variables are on the same scale. 
<br>
.hi-pink[Centering (C)] and .hi-pink[scaling (S)] the predictors can be used for this purpose.

* Other models are very sensitive to correlations between the predictors and filters or PCA signal extraction can improve the model.

* Some models find .hi-pink[(near) zero-variance (NZV)] predictors problematic, and these should be removed before fitting the model. 

* In other cases, the data should be .hi-pink[encoded] in a specific way to make sure all predictors are numeric (e.g. one-hot encoding of factor variables in neural networks). 

* Many models cannot cope with .hi-pink[missing data] so .hi-pink[imputation strategies] might be necessary.

* Development of new features that represent something important to the outcome. 

* (add your own example here!)

This list is inspired by Max Kuhn (2019) on [Applied Machine Learning](https://github.com/topepo/aml-london-2019). 

---

# A blueprint for feature engineering

.font140[.KULbginline[Take-aways]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`] : *a proper implementation*

.pull-left[

* draft a .hi-pink[blueprint] of the necessary pre-processing steps, and their order 

* [Boehme & Greenwell (2019)](https://bradleyboehmke.github.io/HOML/engineering.html#proper-implementation) suggest

&nbsp; &nbsp; &nbsp; &nbsp; 1. Filter out zero or near-zero variance features. <br>
&nbsp; &nbsp; &nbsp; &nbsp; 2. Perform imputation if required. <br>
&nbsp; &nbsp; &nbsp; &nbsp; 3. Normalize to resolve numeric feature skewness. <br>
&nbsp; &nbsp; &nbsp; &nbsp; 4. Standardize (center and scale) numeric features. <br>
&nbsp; &nbsp; &nbsp; &nbsp; 5. Perform dimension reduction (e.g., PCA) on <br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; numeric features. <br>
&nbsp; &nbsp; &nbsp; &nbsp; 6. One-hot or dummy encode categorical features.

]

.pull-right[

* avoid .hi-pink[data leakage] in the pre-processing steps when applied to resampled data sets! 

.center[
```{r out.width = '100%', echo=FALSE}
knitr::include_graphics("img/data_leakage.png")
```
]

]

---

# Feature engineering with {recipes} <img src="img/recipes.png" class="title-hex"> 

.pull-left[

We already detected the necessity of log-transforming `Sale_Price` when building linear models. 

We add another pre-processing step, inspired by the .hi-pink[high cardinality] feature `Neighborhood`. 

```{r eval=FALSE}
ames_train %>% group_by(Neighborhood) %>% 
  summarize(n_obs = n()) %>% 
  arrange(n_obs) %>% slice(1:4)
```

```{r echo=FALSE}
ames_train %>% group_by(Neighborhood) %>% summarize(n_obs = n()) %>% arrange(n_obs) %>% slice(1:4) %>% kable(format = 'html')
```

]

.pull-right[

```{r echo=FALSE, out.width='95%'}
df <- ames_train %>% group_by(Neighborhood) %>% summarize(n_obs = n()) %>% arrange(n_obs)

ggplot(ames_train, aes(x = fct_infreq(Neighborhood))) + theme_bw() +
  geom_bar(col = KULbg, fill = KULbg, alpha = .5) + 
  coord_flip() + 
  xlab("") 
```

]

---

# Feature engineering with {recipes} <img src="img/recipes.png" class="title-hex"> 

.pull-left[
We'll use `recipe()` from the {recipes} package. 

The main idea is to .hi-pink[preprocess multiple datasets] using a single `recipe()`.

Before we start, keep the following .KULbginline[fundamentals] of {recipes} in mind!
]

--

.pull-right[

Creating a `recipe` takes the following steps:

* get the *ingredients* (`recipe()`): specify the response and predictor variables

* write the recipe (`step_zzz()`): define the *pre-processing steps*, such as imputation, creating dummy variables, scaling, and more

* *prepare* the recipe (`prep()`): provide a dataset to base each step on (e.g. *calculate* constants to do centering and scaling)

* *bake* the recipe (`bake()`): *apply* the pre-processing steps to your datasets.

.footnote[Source: [Rebecca Barter's blog](http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/)]

]
---

# Feature engineering with {recipes} <img src="img/recipes.png" class="title-hex"> 

.pull-left[

Use `recipe()` to create the preprocessing blueprint (to be applied later)

```{r eval = FALSE}
library(recipes)
mod_rec <- recipe(Sale_Price ~ ., data = ames_train)
mod_rec
```

```{r echo = FALSE}
library(recipes)
mod_rec <- recipe(Sale_Price ~ ., data = ames_train)
mod_rec
```

Now, `mod_rec` knows the role of each variable (`predictor` or `outcome`). 

We can use selectors such as `all_predictors()`, `all_outcomes()` or `all_nominal()`.

]

.pull-right[

Extend `mod_rec` with two pre-processing steps: 

`step_log(all_outcomes())`

`step_other(Neighborhood, threshold = 0.05)` to lump the levels that occur in less than 5% of data as "other".

```{r eval=TRUE}
mod_rec <- mod_rec %>% step_log(all_outcomes()) %>%
           step_other(Neighborhood, threshold = 0.05)
mod_rec
```

]

---

# Feature engineering with {recipes} <img src="img/recipes.png" class="title-hex"> 

.center[
```{r out.width = '100%', echo=FALSE}
knitr::include_graphics("img/recipes_verbs.png")
```
]

.footnote[Source Max Kuhn (2019) on [Applied Machine Learning](https://github.com/topepo/aml-london-2019).]

Now that we have a preprocessing *specification*, we run on it on the `ames_train` to *prepare* (or `prep()`) the recipe.

```{r eval = FALSE}
mod_rec_trained <- prep(mod_rec, training = ames_train, verbose = TRUE, retain = TRUE)
```

```{r trained_recipe, echo = TRUE}
mod_rec_trained <- prep(mod_rec, training = ames_train, verbose = TRUE, retain = TRUE)
```

The `retain = TRUE` indicates that the preprocessed training set should be saved.
---

# Feature engineering with {recipes} <img src="img/recipes.png" class="title-hex"> 

```{r eval = FALSE}
mod_rec_trained
```

```{r echo = FALSE, dependson='trained_recipe'}
mod_rec_trained
```

Once the recipe is prepared, it can be applied to any data set using `bake()`. There is no need to `bake()` the data used in the `prep()` step; you get the processed training set with `juice()`.

```{r}
ames_test_prep <- bake(mod_rec_trained, new_data = ames_test)
```

---

# Feature engineering with {recipes} <img src="img/recipes.png" class="title-hex"> 

.pull-left[

```{r eval=FALSE}
ames_test_prep %>% group_by(Neighborhood) %>% 
  summarize(n_obs = n()) %>% 
  arrange(n_obs) 
```

```{r echo=FALSE}
ames_test_prep %>% group_by(Neighborhood) %>% 
  summarize(n_obs = n()) %>% 
  arrange(n_obs) %>% kable(format = 'html')
```
]

.pull-right[
```{r eval=FALSE}
juice(mod_rec_trained) %>% group_by(Neighborhood) %>% 
  summarize(n_obs = n()) %>% 
  arrange(n_obs) 
```

```{r echo=FALSE}
juice(mod_rec_trained) %>% group_by(Neighborhood) %>% 
  summarize(n_obs = n()) %>% 
  arrange(n_obs) %>% kable(format = 'html')
```
]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

Now you will extend the existing recipe in `mod_rec`, prepare and bake it again!

.hi-pink[Q]: consult the [`recipes` manual](https://tidymodels.github.io/recipes/reference/index.html) and specify a recipe for the housing data that includes the following pre-processing steps (in this order)

* log-transform the outcome variable
* remove any zero-variance predictors
* lump factor levels that occur in <= 5% of data as "other" for both `Neighborhood` as well as `House_Style`
* center and scale all numeric features.

1. Specify the above recipe on the training set and store it in the object `mod_rec`. 
2. Inspect the object `mod_rec` using `summary(mod_rec)`. What can you learn from this summary?
3. Prepare the recipe on the training data and then apply it to the test set. 

]

---

class: clear

.pull-left[

First, let's try to get a grasp of the `House_Style` feature as well as the presence of zero-variance predictors. 

```{r eval=FALSE}
ames_train %>% group_by(House_Style) %>% 
              summarize(n_obs = n()) %>% 
               arrange(n_obs)
```

```{r echo=FALSE}
ames_train %>% group_by(House_Style) %>% summarize(n_obs = n()) %>% arrange(n_obs) %>% kable(format = 'html')
```


]

.pull-right[

```{r echo=FALSE, out.width='100%'}
ggplot(ames_train, aes(x = fct_infreq(House_Style))) + theme_bw() +
  geom_bar(col = KULbg, fill = KULbg, alpha = .5) + 
  coord_flip() + 
  xlab("") 
```

]

---

class: clear

To detect the presence of zero-variance and near-zero-variance features the `caret` library has the function `nearZeroVar`

```{r eval=TRUE}
library(caret)
nzv <- caret::nearZeroVar(ames_train, saveMetrics = TRUE)
```

```{r eval=FALSE}
names(ames_train)[nzv$zeroVar]
```

```{r echo=FALSE}
names(ames_train)[nzv$zeroVar]
```

```{r eval=FALSE}
names(ames_train)[nzv$nzv]
```

```{r echo=FALSE}
names(ames_train)[nzv$nzv]
```

So, no features have zero- variance, but 21 features have near-zero-variance.

---

class: clear 

.pull-left[

We put the recipe together with the following steps

```{r eval=FALSE}
mod_rec <- recipe(Sale_Price ~ ., data = ames_train) %>%
        step_log(all_outcomes()) %>%
        step_other(Neighborhood, threshold = 0.05) %>%
        step_other(House_Style, threshold = 0.05) %>%
        step_zv(all_predictors()) %>% 
        step_nzv(all_predictors()) %>%
        step_center(all_numeric(), -all_outcomes()) %>%
        step_scale(all_numeric(), -all_outcomes())
summary(mod_rec) %>% slice(1:6)
```

```{r echo=FALSE}
mod_rec <- recipe(Sale_Price ~ ., data = ames_train) %>%
           step_log(all_outcomes()) %>%
           step_other(Neighborhood, threshold = 0.05) %>%
           step_other(House_Style, threshold = 0.05) %>%
           step_zv(all_predictors()) %>% 
           step_nzv(all_predictors()) %>%
           step_center(all_numeric(), -all_outcomes()) %>%
           step_scale(all_numeric(), -all_outcomes())
summary(mod_rec) %>% slice(1:6) %>% kable(format = 'html')
```

]

.pull-right[

```{r eval=FALSE}
mod_rec
```


```{r echo=FALSE}
mod_rec <- recipe(Sale_Price ~ ., data = ames_train) %>%
                  step_log(all_outcomes()) %>%
            step_other(Neighborhood, 
                         threshold = 0.05) %>%
            step_other(House_Style, 
                         threshold = 0.05) %>%
            step_zv(all_predictors()) %>% 
            step_nzv(all_predictors()) %>%
            step_center(all_numeric(), 
                          -all_outcomes()) %>%
            step_scale(all_numeric(), 
                          -all_outcomes())
mod_rec
```

]


---

class: clear

.pull-left[

We prep the recipe on `ames_train`

```{r eval = TRUE}
mod_rec_trained <- prep(mod_rec, 
                        training = ames_train, 
                        verbose = TRUE, retain = TRUE)
```

and bake it on the `ames_test` data

```{r}
ames_test_prep <- bake(mod_rec_trained, 
                                new_data = ames_test)
```

We inspect the processed training and test set

```{r eval=FALSE}
dim(juice(mod_rec_trained)) 
```

```{r echo=FALSE}
dim(juice(mod_rec_trained)) 
```

]

.pull-right[

Verify that `Sale_Price` is log-transformed (but not centred and scaled)

```{r eval=FALSE}
head(juice(mod_rec_trained)$Sale_Price) 
head(ames_train$Sale_Price)
head(ames_test_prep$Sale_Price)
head(ames_test$Sale_Price)
```

```{r echo=FALSE}
options(digits = 4)
head(juice(mod_rec_trained)$Sale_Price) 
```

```{r echo=FALSE}
options(digits = 4)
head(ames_train$Sale_Price) 
```

```{r echo=FALSE}
options(digits = 4)
head(ames_test_prep$Sale_Price) 
```

```{r echo=FALSE}
options(digits = 4)
head(ames_test$Sale_Price) 
```

```{r eval=FALSE}
levels(juice(mod_rec_trained)$House_Style)
```

```{r eval=FALSE}
levels(ames_test_prep$House_Style)
```

```{r echo=FALSE}
levels(juice(mod_rec_trained)$House_Style)[1:2]
levels(juice(mod_rec_trained)$House_Style)[3:4]
```

```{r echo=FALSE}
levels(ames_test_prep$House_Style)[1:2]
levels(ames_test_prep$House_Style)[3:4]
```

]

---

# Putting it all together {rsample} and {recipes} <img src="img/recipes.png" class="title-hex"> <img src="img/rsample.png" class="title-hex"> 

Let's redo the KNN example, with centering and scaling of the x-feature, by combining {rsample}/{caret} with a recipe. 

.pull-left[

```{r eval = TRUE}
# get the simulated data
set.seed(123)  # for reproducibility
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
df <- data.frame(x, y) %>% filter(x < 4.5)
```

```{r eval = TRUE}
# specify the recipe
library(recipes)
rec <- recipe(y ~ x, data = df)
rec <- rec %>% step_center(all_predictors()) %>%
               step_scale(all_predictors())
```

]

.pull-right[

```{r eval = TRUE}
# doing this on complete data set df
rec_df <- prep(rec, training = df)
mean(juice(rec_df)$x) # centered!
sd(juice(rec_df)$x)   # scaled!
```

```{r}
# now we combine the recipe with rsample steps
library(rsample)
set.seed(123)  # for reproducibility
cv_rsample <- vfold_cv(df, 5)
```

```{r}
# we apply the steps in the recipe to each fold
library(purrr)
cv_rsample$recipes <- map(cv_rsample$splits, prepper, 
                          recipe = rec)
# check `?prepper`
```

]

---

# Putting it all together {rsample} and {recipes} <img src="img/recipes.png" class="title-hex"> <img src="img/rsample.png" class="title-hex"> 

Let's redo the KNN example, with centering and scaling of the x-feature, by combining {rsample}/{caret} with a recipe. 

.pull-left[

Now you can inspect `cv_rsample` as follows

```{r eval = FALSE}
cv_rsample$recipes[[1]]
juice(cv_rsample$recipes[[1]])
bake(cv_rsample$recipes[[1]], 
     new_data = assessment(cv_rsample$splits[[1]]))
```


]

.pull-right[

```{r}
holdout_results <- function(s, rec, k_val) {
  # Fit the model to the analysis data in split s
  df_train <- juice(rec)
  mod <- knnreg(y ~ x, k = k_val, data = df_train)
  # Get the remaining group
  holdout <- bake(rec, new_data = assessment(s))
  # Get predictions with the holdout data set
  res <- predict(mod, newdata = holdout)
  # Return observed and predicted values 
  #                            on holdout set
  res <- tibble(obs = holdout$y, pred = res)
  res
}
```

```{r}
res <- holdout_results(cv_rsample$splits[[2]], 
                       cv_rsample$recipes[[2]], 
                       k_val = 58)
sqrt(sum((res$obs - res$pred)^2)/nrow(res))
```

]

---

# Putting it all together {rsample} and {recipes} <img src="img/recipes.png" class="title-hex"> <img src="img/rsample.png" class="title-hex"> 

Let's redo the KNN example, with centering and scaling of the x-feature, by combining {rsample}/{caret} with a recipe. 

```{r}
RMSE <- numeric(nrow(hyper_grid))
SE <- numeric(nrow(hyper_grid))
for(i in 1:nrow(hyper_grid)){
  cv_rsample$results <- map2(cv_rsample$splits, cv_rsample$recipes,
                            holdout_results,
                            hyper_grid[i, ])
  res <- map_dbl(cv_rsample$results, 
                 function(x) mean((x$obs - x$pred)^2))
  RMSE[i] <- mean(sqrt(res)) ; SE[i] <- sd(sqrt(res))
}
```



---

class: inverse, center, middle
name: regression

# Regression models

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

# Linear and Generalized Linear Models

.pull-left-alt[

.center[
<img src="img/esbjorn_GLM.jpg" alt="Drawing" style="width: 150px; height: 220px;"/>
<br> <br> <img src="img/de_jong_GLM.jpg" alt="Drawing" style="width: 150px; height: 220px;"/>  
]

]

.pull-right-alt[

With .hi-pink[linear regression models] `lm(.)`

- model specification

$$\begin{eqnarray*}
    \color{#FFA500}{Y} = \color{#e64173}{x}^{'}\color{#20B2AA}{\beta} + \epsilon.
\end{eqnarray*}$$

- $\epsilon$ is normally distributed with mean 0 and common variance $\sigma^2$, thus: $\color{#FFA500}{Y}$ is normal with mean $\color{#e64173}{x}^{'}\color{#20B2AA}{\beta}$ and variance $\sigma^2$

With .hi-pink[generalized linear regression models] `glm(.)`

- model specification

$$\begin{eqnarray*}
    g(E[\color{#FFA500}{Y}]) = \color{#e64173}{x}^{'}\color{#20B2AA}{\beta}.
\end{eqnarray*}$$

- $g(.)$ is the link function

- $\color{#FFA500}{Y}$ follows a distribution from the exponential family.


]

---

# Generalized Linear Models (GLMs) 

We return to the `mtpl` data set.

.pull-left[

Target variable `nclaims` (frequency)

```{r echo=FALSE, out.width='65%'}
g_freq <- ggplot(mtpl, aes(nclaims)) + theme_bw() + 
     geom_bar(aes(weight = expo), col = KULbg, 
                               fill = KULbg, alpha = .5) + 
     labs(y = "Abs freq (in exposure)") +
     ggtitle("MTPL - number of claims")
g_freq
```


Suitable distributions: Poisson, Negative Binomial.

]

.pull-right[

...  and `avg` (severity).

```{r echo=FALSE, out.width='65%'}
g_sev <- ggplot(mtpl, aes(x = avg)) + theme_bw() +
  geom_histogram(bins = 30, boundary = 0, color = KULbg, fill = KULbg, alpha = .5) + 
  labs(x = "claim severity") +
  xlim(c(0, 20000))
g_sev
```


Suitable distributions: log-normal, gamma.

]

---

# A Poisson GLM 

.pull-left[

A brief recap..

```{r eval=FALSE}
freq_by_gender <- mtpl %>% 
  group_by(sex) %>% 
  summarize(emp_freq = sum(nclaims) / sum(expo))
```

```{r echo=FALSE}
freq_by_gender <- mtpl %>% 
  group_by(sex) %>% 
  summarize(emp_freq = sum(nclaims) / sum(expo)) 
freq_by_gender %>% kable(format = 'html')
```

Let's picture the empirical gender-specific claim frequency...

```{r eval=FALSE}
ggplot(freq_by_gender, aes(x = sex, y = emp_freq)) +
  geom_bar(col = KULbg, fill = KULbg, alpha = .5)
```

]

.pull-right[


```{r echo=FALSE, out.width='100%'}
ggplot(freq_by_gender, aes(x = sex, y = emp_freq)) + theme_bw() +
  geom_bar(stat = "identity", col = KULbg, fill = KULbg, alpha = .5)
```

]

---

# A Poisson GLM (cont.)

.pull-left[

```{r eval=FALSE}
freq_glm_1 <- `glm`(nclaims ~ sex, offset = log(expo), 
                  `family = poisson(link = "log")`, 
                  `data = mtpl`) 
```

]

.pull-right[
  
Fit a .KULbginline[Poisson GLM], with .KULbginline[logarithmic link] function.

This implies: 

$\color{#FFA500}{Y}$ ~ Poisson, with

$$\begin{eqnarray*}
    \log(E[\color{#FFA500}{Y}]) &=& \color{#e64173}{x}^{'}\color{#20B2AA}{\beta},
\end{eqnarray*}$$

or, 

$$E[\color{#FFA500}{Y}] = \exp{(\color{#e64173}{x}^{'}\color{#20B2AA}{\beta})}.$$

Fit this model on `data = mtpl`. 
  
]

---
  
# A Poisson GLM (cont.)

.pull-left[

```{r eval=FALSE}
freq_glm_1 <- glm(`nclaims ~ sex`, `offset = log(expo)`, 
                  family = poisson(link = "log"), 
                  data = mtpl)
```

]

.pull-right[
  
Use `nclaims` as $\color{#FFA500}{Y}$. 

Use `gender` as the only (factor) variable in the linear predictor.

Include `log(exp)` as an offset term in the linear predictor.

Then, 

$$\begin{eqnarray*}
\color{#e64173}{x}^{'}\color{#20B2AA}{\beta} = \log{(\texttt{expo})}+\beta_0 + \beta_1 \mathbb{I}(\texttt{male}). \end{eqnarray*}$$

Put otherwise, 

$$\begin{eqnarray*}
E[\color{#FFA500}{Y}] = \texttt{expo} \cdot \exp{(\beta_0 + \beta_1 \mathbb{I}(\texttt{male}))} \end{eqnarray*},$$
where $\texttt{expo}$ refers to `expo` the exposure variable.
]

---

class: clear

.pull-left[

```{r eval=FALSE}
freq_glm_1 <- glm(nclaims ~ sex, `offset = log(expo)`, 
                  family = poisson(link = "log"), 
                  data = mtpl)
```

```{r eval=FALSE}
freq_glm_1 %>% broom::tidy()
```

```{r echo=FALSE}
freq_glm_1 <- glm(nclaims ~ sex, offset = log(expo), 
                  family = poisson(link = "log"), 
                  data = mtpl)
freq_glm_1 %>% broom::tidy() %>% kable(format = 'html')
```

Mind the specification of `type.predict` when using `augment` with a GLM!

```{r eval=FALSE}
freq_glm_1 %>% broom::augment(type.predict = 
                                        "response")
```

```{r echo=FALSE}
freq_glm_1 %>% broom::augment(type.predict = "response") %>% slice(1:2) %>% select(nclaims, sex, .fitted, .se.fit) %>% kable(format = 'html')
```



]

.pull-right[

The `predict` function of a GLM object offers 3 options: `"link"`, `"response"` or `"terms"`. 

The same options hold when `augment()` is applied to a GLM object.

Let's see how the fitted values at `"response"` level are constructed:

```{r}
exp(coef(freq_glm_1)[1])
exp(coef(freq_glm_1)[1] + coef(freq_glm_1)[2])
```

Do you recognize these numbers?

Last step: 

try `freq_glm_1 %>% glance()` or `summary(freq_glm_1)` for deviances. 

]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

You will further explore GLMs in R with the `glm(.)` function.

.hi-pink[Q]: continue with the `freq_glm_1` object that was created, you will now explicitly call the `predict()` function on this object. 

1. Verify the arguments of `predict.glm` using `? predict.glm`.

2. The help reveals the following structure `predict(.object, .newdata, type = ("..."))` where `.object` is the fitted GLM object, `.newdata` is (optionally) a data frame to look for the features used in the model, and `type` is  `"link"`, `"response"` or `"terms"`. <br> Use `predict` with `freq_glm_1` and a newly created data frame. <br> Explore the different options for `type`, and their connections. 

3. Fit a gamma GLM for `avg` (the claim severity) with log link. <br>
Use `sex` as the only variable in the model. What do you conclude?
]

---

class: clear

.pull-left[
.hi-pink[Q.1] You can access the documentation via `? predict.glm`.

.hi-pink[Q.2] You create new data frames (or tibbles) as follows

```{r eval = FALSE}
male_driver <- data.frame(exp = 1, sex = "male")
female_driver <- data.frame(exp = 1, sex = "female")
```

```{r echo = FALSE}
male_driver <- data.frame(expo = 1, sex = "male")
female_driver <- data.frame(expo = 1, sex = "female")
```

Next, you apply `predict` with the GLM object `freq_glm_1` and one of these data frames, e.g.

```{r eval = FALSE}
predict(freq_glm_1, newdata = male_driver, 
                        type = "response")
```

```{r echo = FALSE}
predict(freq_glm_1, newdata = male_driver, type = "response")
```

]

.pull-right[

.hi-pink[Q.2] Next, you apply `predict` with the GLM object `freq_glm_1` and one of these data frames, e.g.

```{r eval = FALSE}
predict(freq_glm_1, newdata = male_driver, 
                        type = "response")
```

```{r echo = FALSE}
predict(freq_glm_1, newdata = male_driver, type = "response")
```

At the level of the linear predictor: 

```{r eval = FALSE}
predict(freq_glm_1, newdata = male_driver, 
                        type = "link")
```

```{r echo = FALSE}
predict(freq_glm_1, newdata = male_driver, type = "link")
```

```{r eval = FALSE}
exp(predict(freq_glm_1, newdata = male_driver, 
                        type = "link"))
```

```{r echo = FALSE}
exp(predict(freq_glm_1, newdata = male_driver, 
                        type = "link"))
```

]

---

class: clear

.hi-pink[Q.3] For the gamma regression model

```{r eval=FALSE}
sev_glm_1 <- glm(avg ~ sex, family = Gamma(link = "log"), data = mtpl)
sev_glm_1
```

```{r echo=FALSE}
sev_glm_1 <- glm(avg ~ sex, family = Gamma(link = "log"), data = mtpl)
sev_glm_1
```



---

# Generalized Additive Models (GAMs)

.pull-left-alt[

.center[
<img src="img/GAM_Hastie_Tibshirani.jpg" alt="Drawing" style="width: 150px; height: 220px;"/>
<br> <br> <img src="img/GAM_Wood.jpg" alt="Drawing" style="width: 150px; height: 220px;"/>  
]

]

.pull-right-alt[

With .hi-pink[GLMs] `glm(.)`

- transformation of the mean modelled with a linear predictor $$\color{#e64173}{x}^{'}\color{#20B2AA}{\beta}$$

- not well suited for continuous risk factors that relate to the response in
a non-linear way.

With .hi-pink[Generalized Additive Models (GAMs)] 

- the predictor allows for smooth effects of continuous risk factors and spatial covariates, next to the linear terms, e.g.

$$\color{#e64173}{x}^{'}\color{#20B2AA}{\beta}+\sum_j f_j(x_j) + f(\texttt{lat}, \texttt{long})$$

- predictor is still additive

- preferred R package is {mgcv} by Simon Wood.


]

---

# A Poisson GAM

.pull-left[
We continue working with `mtpl` and now focus on `ageph`.

```{r echo=FALSE, out.width='75%'}
mtpl %>% group_by(ageph) %>% 
  summarize(emp_freq = sum(nclaims) / sum(expo)) %>% 
  ggplot(aes(x = ageph, y = emp_freq)) + theme_bw() +
  geom_point(color = KULbg)
```

]

.pull-right[

We will now explore .hi-pink[four different model specifications]: 

1. `ageph` as linear effect in `glm`

2. `ageph` as factor variable in `glm`

3. `ageph` split manually into bins using `cut`, then used as factor in `glm`

4. a smooth effect of `ageph` in `mgcv::gam`.

Let's go!

Grid of observed `ageph` values

```{r}
a <- min(mtpl$ageph):max(mtpl$ageph)
```

]

---

class: clear

.pull-left[

.hi-pink[Model 1]: linear effect of `ageph`

```{r eval=FALSE}
freq_glm_age <- glm(nclaims ~ `ageph`, 
                    offset = log(expo), 
                    data = mtpl, 
                    family = poisson(link = "log"))
pred_glm_age <- predict(freq_glm_age, 
          newdata = data.frame(ageph = a, expo = 1), 
          `type = "terms"`, se.fit = TRUE)
b_glm_age <- pred_glm_age$fit
l_glm_age <- pred_glm_age$fit 
                  - qnorm(0.975)*pred_glm_age$se.fit
u_glm_age <- pred_glm_age$fit 
                  + qnorm(0.975)*pred_glm_age$se.fit
df <- data.frame(a, b_glm_age, l_glm_age, u_glm_age)
```


```{r echo=FALSE}
freq_glm_age <- glm(nclaims ~ ageph, offset = log(expo), data = mtpl, family = poisson(link = "log"))
pred_glm_age <- predict(freq_glm_age, newdata = data.frame(ageph = a, expo = 1), type = "terms", se.fit = TRUE)
b_glm_age <- pred_glm_age$fit
l_glm_age <- pred_glm_age$fit - qnorm(0.975)*pred_glm_age$se.fit
u_glm_age <- pred_glm_age$fit + qnorm(0.975)*pred_glm_age$se.fit
df <- data.frame(a, b_glm_age, l_glm_age, u_glm_age)
```

]

.pull-right[

```{r echo=FALSE, out.width='100%'}
p_glm_age <- ggplot(df, aes(x = a)) + ylim(-0.5, 1)
p_glm_age <- p_glm_age + geom_line(aes(a, b_glm_age), size = 1, col = KULbg)   
p_glm_age <- p_glm_age + geom_line(aes(a, u_glm_age), size = 0.5, linetype = 2, col = KULbg) + geom_line(aes(a, l_glm_age), size = 0.5, linetype = 2, col = KULbg)
p_glm_age <- p_glm_age + xlab("ageph") + ylab("fit") + theme_bw()
p_glm_age
```

]
---

class: clear

.pull-left[

.hi-pink[Model 2]: `ageph` as factor variable in `glm`

```{r eval=FALSE}
freq_glm_age_f <- glm(nclaims ~ `as.factor(ageph)`, 
                    offset = log(expo), 
                    data = mtpl, 
                    family = poisson(link = "log"))
pred_glm_age_f <- predict(freq_glm_age_f, 
          newdata = data.frame(ageph = a, expo = 1), 
          `type = "terms"`, se.fit = TRUE)
b_glm_age_f <- pred_glm_age_f$fit
l_glm_age_f <- pred_glm_age_f$fit 
                  - qnorm(0.975)*pred_glm_age_f$se.fit
u_glm_age_f <- pred_glm_age_f$fit 
                  + qnorm(0.975)*pred_glm_age_f$se.fit
df <- data.frame(a, b_glm_age_f, 
                    l_glm_age_f, u_glm_age_f)
```


```{r echo=FALSE}
freq_glm_age_f <- glm(nclaims ~ as.factor(ageph), offset = log(expo), data = mtpl, family = poisson(link = "log"))
pred_glm_age_f <- predict(freq_glm_age_f, newdata = data.frame(ageph = a, expo = 1), type = "terms", se.fit = TRUE)
b_glm_age_f <- pred_glm_age_f$fit
l_glm_age_f <- pred_glm_age_f$fit - 
               qnorm(0.975)*pred_glm_age_f$se.fit
u_glm_age_f <- pred_glm_age_f$fit + 
               qnorm(0.975)*pred_glm_age_f$se.fit
df <- data.frame(a, b_glm_age_f, 
                    l_glm_age_f, u_glm_age_f)
```

]

.pull-right[

```{r echo=FALSE, out.width='100%'}
p_glm_age_f <- ggplot(df, aes(x = a)) + ylim(-0.5, 1)
p_glm_age_f <- p_glm_age_f + geom_line(aes(a, b_glm_age_f), size = 1, col = KULbg)   
p_glm_age_f <- p_glm_age_f + geom_line(aes(a, u_glm_age_f), size = 0.5, linetype = 2, col = KULbg) + geom_line(aes(a, l_glm_age_f), size = 0.5, linetype = 2, col = KULbg)
p_glm_age_f <- p_glm_age_f + xlab("ageph") + ylab("fit") + theme_bw()
p_glm_age_f
```

]

---

class: clear

.pull-left[

.hi-pink[Model 3]: `ageph` split into 5-year bins and then used in `glm`

```{r eval=FALSE}
level <- seq(min(mtpl$ageph), max(mtpl$ageph), by = 5) #<<
freq_glm_age_c <- glm(nclaims ~ `cut(ageph, level)`, 
                    offset = log(expo), 
                    data = mtpl, 
                    family = poisson(link = "log"))
pred_glm_age_c <- predict(freq_glm_age_c, 
          newdata = data.frame(ageph = a, expo = 1), 
          `type = "terms"`, se.fit = TRUE)
b_glm_age_c <- pred_glm_age_c$fit
l_glm_age_c <- pred_glm_age_c$fit 
                  - qnorm(0.975)*pred_glm_age_c$se.fit
u_glm_age_c <- pred_glm_age_c$fit 
                  + qnorm(0.975)*pred_glm_age_c$se.fit
df <- data.frame(a, b_glm_age_c, 
                    l_glm_age_c, u_glm_age_c)
```


```{r echo=FALSE}
level <- seq(min(mtpl$ageph), max(mtpl$ageph), by = 5)
freq_glm_age_c <- glm(nclaims ~ cut(ageph, level), offset = log(expo), data = mtpl, family = poisson(link = "log"))
pred_glm_age_c <- predict(freq_glm_age_c, newdata = data.frame(ageph = a, expo = 1), type = "terms", se.fit = TRUE)
b_glm_age_c <- pred_glm_age_c$fit
l_glm_age_c <- pred_glm_age_c$fit - 
               qnorm(0.975)*pred_glm_age_c$se.fit
u_glm_age_c <- pred_glm_age_c$fit + 
               qnorm(0.975)*pred_glm_age_c$se.fit
df <- data.frame(a, b_glm_age_c, 
                    l_glm_age_c, u_glm_age_c)
```

]

.pull-right[

```{r echo=FALSE, out.width='100%'}
p_glm_age_c <- ggplot(df, aes(x = a)) + ylim(-0.5, 1)
p_glm_age_c <- p_glm_age_c + geom_line(aes(a, b_glm_age_c), size = 1, col = KULbg)   
p_glm_age_c <- p_glm_age_c + geom_line(aes(a, u_glm_age_c), size = 0.5, linetype = 2, col = KULbg) + geom_line(aes(a, l_glm_age_c), size = 0.5, linetype = 2, col = KULbg)
p_glm_age_c <- p_glm_age_c + xlab("ageph") + ylab("fit") + theme_bw()
p_glm_age_c
```

]

---

class: clear

.pull-left[

.hi-pink[Model 4]: smooth effect of `ageph` in `mgcv::gam`

```{r eval=FALSE}
library(mgcv)
freq_gam_age <- gam(nclaims ~ `s(ageph)`, 
                    offset = log(expo), 
                    data = mtpl, 
                    family = poisson(link = "log"))
pred_gam_age <- predict(freq_gam_age, 
          newdata = data.frame(ageph = a, expo = 1), 
          `type = "terms"`, se.fit = TRUE)
b_gam_age <- pred_gam_age$fit
l_gam_age <- pred_gam_age$fit -
                  qnorm(0.975)*pred_gam_age$se.fit
u_gam_age <- pred_gam_age$fit +
                  qnorm(0.975)*pred_gam_age$se.fit
df <- data.frame(a, b_gam_age, 
                    l_gam_age, u_gam_age)
```


```{r echo=FALSE}
library(mgcv)
freq_gam_age <- gam(nclaims ~ s(ageph), 
                    offset = log(expo), 
                    data = mtpl, 
                    family = poisson(link = "log"))
pred_gam_age <- predict(freq_gam_age, 
          newdata = data.frame(ageph = a, expo = 1), 
          type = "terms", se.fit = TRUE)
b_gam_age <- pred_gam_age$fit
l_gam_age <- pred_gam_age$fit -
                  qnorm(0.975)*pred_gam_age$se.fit
u_gam_age <- pred_gam_age$fit +
                  qnorm(0.975)*pred_gam_age$se.fit
df <- data.frame(a, b_gam_age, 
                    l_gam_age, u_gam_age)
```

]

.pull-right[

```{r echo=FALSE, out.width='100%'}
p_gam_age <- ggplot(df, aes(x = a)) + ylim(-0.5, 1)
p_gam_age <- p_gam_age + geom_line(aes(a, b_gam_age), size = 1, col = KULbg)   
p_gam_age <- p_gam_age + geom_line(aes(a, u_gam_age), size = 0.5, linetype = 2, col = KULbg) + geom_line(aes(a, l_gam_age), size = 0.5, linetype = 2, col = KULbg)
p_gam_age <- p_gam_age + xlab("ageph") + ylab("fit") + theme_bw()
p_gam_age
```

]

---

class: clear

.hi-pink[Model 4] (revisited): picture smooth effect of `ageph` in `mgcv::gam` with built-in `plot`.

```{r, out.width='35%', out.height='10%'}
library(mgcv)
freq_gam <- gam(nclaims ~ s(ageph), offset = log(expo), family = poisson(link = "log"), data = mtpl)
plot(freq_gam, scheme = 4)
```

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

You will further explore GAMs in R with the `gam(.)` function from the {mgcv} package.

.hi-pink[Q]: you will combine insights from building `glm` as well as `gam` objects by working through the following coding steps. 

1. Fit a `gam` including some factor variables as well as a smooth effect of `ageph` and `bm`. Visualize the fitted smooth effects.

2. Specify risk profiles of drivers. Calculate their expected annual claim frequency from the constructed `gam`.

3. Explain (in words) which profiles would represent high vs low risk according to the constructed model. 
]

---

class: clear

.pull-left[

.hi-pink[Q.1] examine the following `gam` fit

```{r}
freq_gam_2 <- gam(nclaims ~ sex + fuel + use + 
                            s(ageph) + s(bm), 
                  offset = log(expo), 
                  family = poisson(link = "log"), 
                  data = mtpl)
```

```{r}
summary(freq_gam_2)
```


]

.pull-right[

```{r, out.width='45%'}
plot(freq_gam_2, select = 1)
```


```{r, out.width='45%'}
plot(freq_gam_2, select = 2)
```

]

---

class: clear

.pull-left[

.hi-pink[Q.2] define some risk profiles 

```{r eval = FALSE}
drivers <- data.frame(expo = c(1, 1, 1), 
                      sex = c("female", "female", "female"), 
                      fuel = c("diesel", "diesel", "diesel"), 
                      use = c("private", "private", "private"), 
                      ageph = c(18, 45, 65), bm = c(20, 5, 0))
drivers 
```

```{r echo = FALSE}
drivers <- data.frame(expo = c(1, 1, 1), sex = c("female", "female", "female"), fuel = c("diesel", "diesel", "diesel"), use = c("private", "private", "private"), ageph = c(18, 45, 65), bm = c(20, 5, 0))
drivers %>% kable(format = 'html')
```
]

.pull-right[
Now, you predict the annual expected claim frequency for these profiles. 

```{r eval = FALSE}
predict(freq_gam_2, newdata = drivers, 
        type = "response")
```

```{r echo = FALSE}
predict(freq_gam_2, newdata = drivers, 
        type = "response") %>% kable(format = 'html')
```

]

---

# Statistical learning with sparsity

.pull-left-alt[

.center[
<img src="img/sparsity_book.jpg" alt="Drawing" style="width: 150px; height: 220px;"/>
<br> <br> <img src="img/boehmke_greenwell.jpg" alt="Drawing" style="width: 150px; height: 220px;"/>  
]

]

.pull-right-alt[

Why?

* Sort through the mass of information and bring it down to .hi-pink[its bare essentials].

* One form of simplicity is .hi-pink[sparsity].

* Only a relatively small number of predictors play a role.

How? &nbsp; &nbsp; .KULbginline[Automatic feature selection!]

* Fit a model with all *p* predictors, but constrain or .hi-pink[regularize] the coefficient estimates. 

* Shrinking the coeffcient estimates can signifcantly reduce their variance. 

* Some types of shrinkage put some of the coefficients .KULbginline[exactly equal to zero]!

]

---

# Ridge and lasso (least squares) regression

.pull-left[
.KULbginline[Ridge] considers the least-squares optimization problem

$$\begin{eqnarray*}
\min_{\beta_0,\boldsymbol{\beta}} \ \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\right)^2 = \min_{\beta_0,\boldsymbol{\beta}} \ \text{RSS}
\end{eqnarray*}$$

subject to a .hi-pink[budget constraint]

$$\begin{eqnarray*}
\sum_{j=1}^p \beta_j^2 \leq \color{#e64173}{t},
\end{eqnarray*}$$

i.e. an $\ell_2$ penalty.

Shrinks the coefficient estimates (not the intercept) to zero. 


]

.pull-right[
.KULbginline[Lasso] considers the least-squares optimization problem

$$\begin{eqnarray*}
\min_{\beta_0,\boldsymbol{\beta}} \ \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\right)^2 = \min_{\beta_0,\boldsymbol{\beta}} \ \text{RSS}
\end{eqnarray*}$$

subject to a .hi-pink[budget constraint]

$$\begin{eqnarray*}
\sum_{j=1}^p |\beta_j| \leq \color{#e64173}{t},
\end{eqnarray*}$$

i.e. an $\ell_1$ penalty.

Shrinks the coefficient estimates (not the intercept) to zero and does variable selection!

Lasso is for .hi-pink[L]east .hi-pink[a]bsolute .hi-pink[s]hrinkage and .hi-pink[s]election .hi-pink[o]perator.


]

---

# Ridge and lasso (least squares) regression (cont.)

.pull-left[

The .KULbginline[dual problem] formulation:

* with ridge penalty: 

$$\begin{eqnarray*}
\min_{\beta_0,\boldsymbol{\beta}} \ \text{RSS} + \color{#e64173}{\lambda} \sum_{j=1}^p \beta_j^2
\end{eqnarray*}$$

* with lasso penalty:
$$\begin{eqnarray*}
\min_{\beta_0,\boldsymbol{\beta}} \ \text{RSS}+\color{#e64173}{\lambda} \sum_{j=1}^p |\beta_j|.
\end{eqnarray*}$$

$\color{#e64173}{\lambda}$ is a tuning parameter; use resampling methods to pick a value!

Both ridge and lasso require .hi-pink[centering and scaling] of the features. 

]

.pull-right[

.center[
```{r out.width = '100%', echo=FALSE}
knitr::include_graphics("img/6.7_ISL.png")
```
]

Ellipses (around least-squares solution) represent regions of constant RSS. 

Lasso budget on the left and ridge budget on the right.

Source: James et al. (2013) on [An introduction to statistical learning](http://faculty.marshall.usc.edu/gareth-james/ISL/).

]

---

# Regularized GLMs 

.pull-left[

We now focus on generalizations of linear models and the lasso.

Minimize

$$\begin{eqnarray*}
\min_{\beta_0,\ \beta} -\frac{1}{n} \mathcal{L}(\beta_0,\ \beta;\ y,\ X)+\lambda \|\boldsymbol{\beta}\|_1.
\end{eqnarray*}$$

Here: 

* $\mathcal{L}$ is the log-likelihood of a GLM.

* $n$ is the sample size

* $\|\boldsymbol{\beta}\|_1 = \sum_{j=1}^p \beta_j$ the $\ell_1$ penalty.

What happens if: 

* $\lambda \to 0$?

* $\lambda \to \infty$?

]

.pull-right[

.center[
```{r out.width = '100%', echo=FALSE}
knitr::include_graphics("img/lasso_path.png")
```
]

The R package {glmnet} fits linear, logistic and multinomial, Poisson, and Cox regression models.

]
---

# Fit a GLM with lasso regularization in {glmnet}

{glmnet} is a package that fits a generalized linear model via penalized maximum likelihood.

Main function call (with a selection of arguments, see `? glmnet` for a complete list)

```{r eval = FALSE}
fit <- glmnet(x, y, family = ., alpha = ., weights = ., offset = ., nlambda = ., standardize = ., intercept = .)
```

where

* `x` is the input matrix and `y` is the response variable
* `family` the response type, e.g. `family = poisson`
* `weights` and `offset` 
* `nlambda` is the number of $\lambda$ values, default is 100
* `standardize` should `x` be standardized prior to fitting the model sequence?
* `intercept` should incercept be fitted?
* `alpha` a value between 0 and 1, such that the penalty becomes
$$\begin{eqnarray*}
\lambda P_{\alpha}(\boldsymbol{\beta}) = \lambda \cdot \sum_{j=1}^p \left\{\frac{(1-\alpha)}{2}\beta_j^2 + \alpha |\beta_j|\right\}.
\end{eqnarray*}$$
Thus, with $\alpha = 1$ the lasso penalty and $\alpha = 0$ the ridge penalty results.

---

# A first example of {glmnet}

.pull-left[

Following [the vignette](https://web.stanford.edu/~hastie/Papers/Glmnet_Vignette.pdf) we start with penalized linear regression

```{r eval=TRUE}
library(glmnet)
data(QuickStartExample)
```

This example loads an input matrix `x` and vector `y` of outcomes. The input matrix `x` is not standardized yet (check this!).

We calibrate a lasso linear regression model

```{r eval=FALSE}
fit <- glmnet(x, y, family = "gaussian", 
              `alpha = 1`, standardize = TRUE, 
              intercept = TRUE)
summary(fit)
```


]

.pull-right[

```{r echo=FALSE, out.width='85%'}
library(glmnet)
data(QuickStartExample)
fit <- glmnet(x, y, family = "gaussian", alpha = 1, standardize = TRUE, intercept = TRUE)
```

Note that the formula notation `y ~ x` can not be used with `glmnet`.

Some `tidy` instructions are available for `glmnet` objects (but not all), e.g.

```{r eval=FALSE}
library(broom)
tidy(fit)
```

```{r echo=FALSE}
library(knitr)
library(broom)
tidy(fit) %>% slice(1:5) %>% kable(format = 'html')
```

]

---

class: clear

.pull-left[

```{r eval=FALSE, out.width='100%'}
plot(fit, `label = TRUE`)
```

```{r echo=FALSE, out.width='100%'}
plot(fit, label = TRUE)
```

]

.pull-right[

```{r eval=FALSE, out.width='100%'}
plot(fit, label = TRUE, `xvar = 'lambda'`)
```

```{r echo=FALSE, out.width='100%'}
plot(fit, label = TRUE, xvar = 'lambda')
```

]

---

class: clear

.pull-left[

```{r, eval=FALSE, out.width='100%'}
plot(fit, `xvar = 'dev'`, label = TRUE)
```

```{r, echo=FALSE, out.width='100%'}
plot(fit, xvar = 'dev', label = TRUE)
```

]

.pull-right[

```{r, eval=TRUE}
print(fit) 
```

]

---

class: clear

.pull-left[

Get estimated coefficients for handpicked value

```{r eval = FALSE}
coef(fit, `s = 0.1`)
```

```{r echo = FALSE}
coef(fit, s = 0.1)
```


]

.pull-right[

`glmnet` returns a sequence of models for the users to choose from, i.e. a model for every `lambda`. 

How do we select the most appropriate model? 

Use cross-validation to pick a `lambda` value. The default is 10-folds cross-validation.

```{r eval=FALSE}
cv_fit <- cv.glmnet(x, y)
```

```{r echo=FALSE}
library(glmnet)
data(QuickStartExample)
cv_fit <- cv.glmnet(x, y)
```

We can pick the `lambda` that minimizes the cross-validation error. 

```{r eval=TRUE}
cv_fit$lambda.min
```

Or we use the one-standard-error-rule. 

```{r eval=TRUE}
cv_fit$lambda.1se
```

]

---

class: clear

We plot the cross-validation error for the inspected grid of `lambda` values. 

```{r eval=FALSE}
plot(cv_fit)
```

```{r echo=FALSE, out.height= '40%', out.width='45%'}
plot(cv_fit)
```

---

class: clear

.pull-left[

For the selected `lambda` (via `cv_fit$lambda.min`) we inspect which parameters are non-zero (on the right).

Now, compare this to the selected variables obtained via `cv_fit$lambda.1se`.

]

.pull-right[

```{r eval = FALSE}
coef(fit, s = cv_fit$lambda.min)
```

```{r echo = FALSE}
coef(fit, s = cv_fit$lambda.min)
```

]

---

class: clear

.pull-left[

The variables `V1`, `V3`, `V5-8`, `V11`, `V14` and `V20` are selected in the regression model. 

However, the corresponding estimates (on the left) are biased, and shrunk to zero. 

To remove this bias, we refit the model, only using the selected variables. 

```{r eval = FALSE}
subset <- data.frame(y = y, V1 = x[, 1], V3 = x[, 3], 
                     V5 = x[, 5], V6 = x[, 6], 
                     V7 = x[, 7], V8 = x[, 8], 
                     V11 = x[, 11], V14 = x[, 14], 
                     V20 = x[, 20])
final_model <- lm(y ~ V1 + V3 + V5 + V6 + V7 + V8 + 
                      V11 + V14 + V20, data = subset)
final_model %>% broom::tidy()
```

What is your judgement about `V7` (see coefficients on the right)?

]

.pull-right[

What do you observe when comparing the estimates below with those shown on the previous sheet?

```{r echo = FALSE}
library(glmnet)
data(QuickStartExample)
col <- c(1, 3, 5:8, 11, 14, 20)
subset <- data.frame(y = y, V1 = x[, 1], V3 = x[, 3], V5 = x[, 5], V6 = x[, 6], V7 = x[, 7], V8 = x[, 8], V11 = x[, 11], V14 = x[, 14], V20 = x[, 20])
final_model <- lm(y ~ V1 + V3 + V5 + V6 + V7 + V8 + V11 + V14 + V20, data = subset)
final_model %>% broom::tidy() %>% kable(format = 'html')
```
]

---

# {glmnet} and the MTPL data set

.pull-left[

Next, we fit a .KULbginline[Poisson regression model with lasso penalty] on the `mtpl` data set. 

The regularization penalty helps us to select the interesting features from the data set. 

`glmnet` requires the features as input matrix `x` and the target as a vector `y`.

Recall: 

* `mtpl` has .hi-pink[continuous] features (e.g. `ageph`, `bm`, `power`)

* `mtpl` has .hi-pink[factor] variables with .hi-pink[two levels] (e.g. `sex`, `fleet`)

* but also factor variables with .hi-pink[more than 2 levels] (`coverage`)

]

.pull-right[

Consider different types of coding factor variables.

Apply the `contrasts` function to the variable `coverage`

```{r eval = TRUE}
map(mtpl[, c("coverage")], contrasts, 
    contrasts = FALSE)
```

```{r eval = TRUE}
map(mtpl[, c("coverage")], contrasts, 
    contrasts = TRUE)
```

What's the difference?
]

---

# {glmnet} and the MTPL data set (cont.)

We construct the input matrix for `glmnet`.

```{r, eval = FALSE}
y <- mtpl$nclaims #<<

x <- model.matrix( ~ coverage + fuel + use + fleet + sex + ageph + bm +
                     agec + power, data = mtpl, 
                   contrasts.arg = map(mtpl[, c("coverage")], contrasts, 
                                       contrasts = FALSE))[,-1]

x[1:10,]
```

Put the response or outcome variable in `y`. 

In the `mtpl` data set we build a Poisson model for `nclaims`. 

---

# {glmnet} and the MTPL data set (cont.)

We construct the input matrix for `glmnet`.

```{r, eval = FALSE}
y <- mtpl$nclaims

x <- `model.matrix`( ~ coverage + fuel + use + fleet + sex + ageph + bm +
                     agec + power, data = mtpl, 
                     `contrasts.arg = map(mtpl[, c("coverage")], contrasts,` 
                                      `contrasts = FALSE)`)[,-1]

```

Use `model.matrix` to create the input matrix `x`.

We code the factor variable `coverage` with one-hot-encoding. Here, three dummy variables will be created for the three levels of `coverage`. 

The other factor variables `fuel`, `use`, `fleet`, `sex` are dummy coded, with one dummy variable. 

---

# {glmnet} and the MTPL data set (cont.)

We construct the input matrix for `glmnet`.

```{r, eval = FALSE}
y <- mtpl$nclaims

x <- model.matrix( ~ coverage + fuel + use + fleet + sex + ageph + bm +
                     agec + power, data = mtpl, 
                     contrasts.arg = map(mtpl[, c("coverage")], contrasts, 
                                       contrasts = FALSE))`[,-1]`

```

Use `model.matrix` to create the input matrix `x`.

We remove the first column, representing the intercept, from the `model.matrix`. 

---

# {glmnet} and the MTPL data set (cont.)

Let's check the input matrix `x`

```{r, echo = FALSE}
y <- mtpl$nclaims

x <- model.matrix( ~ coverage + fuel + use + fleet + sex + ageph + bm +
                     agec + power, data = mtpl, 
                     contrasts.arg = map(mtpl[, c("coverage")], contrasts, 
                                       contrasts = FALSE))[,-1]

x[1:6,]
```

You are now ready to fit a regularized Poisson GLM for `y` with input `x`. 

Let's go!

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

You will fit a regularized Poisson GLM on the `mtpl` data with the {glmnet} package.

.hi-pink[Q]: using the constructed `y` and `x` 

1. Fit a `glmnet` with lasso penalty and store the fitted object in `mtpl_glmnet`. Use the following arguments `family = "poisson", offset = ___`.

2. Display the order of the variables and their names via `row.names(mtpl_glmnet$beta)`.

3. Plot the solutions path. Pick a meaningful value for `lambda` via cross-validation.

4. Which variables are selected in the lasso model? As a last step, you will fit a Poisson GLM with the selected variables. What do you see?

5. List some pros and cons of the above strategy.
]

---

class: clear

.pull-left[

.hi-pink[Q.1] fit a regularized Poisson GLM

```{r eval = FALSE}
alpha <- 1 # for lasso penalty
mtpl_glmnet <- glmnet(x = x, y = y, 
                      family = "poisson", 
                      offset = log(mtpl$expo), 
                      alpha = alpha, 
                      standardize = TRUE, 
                      intercept = TRUE)
```

```{r echo = FALSE}
y <- mtpl$nclaims

x <- model.matrix( ~ coverage + fuel + use + fleet + sex + ageph + bm +
                     agec + power, data = mtpl, 
                     contrasts.arg = map(mtpl[, c("coverage")], contrasts, 
                                       contrasts = FALSE))[,-1]
alpha <- 1 # for lasso penalty
mtpl_glmnet <- glmnet(x = x, y = y, 
                      family = "poisson", 
                      offset = log(mtpl$expo), 
                      alpha = alpha, 
                      standardize = TRUE, 
                      intercept = TRUE)
```

.hi-pink[Q.2] display the variables via

```{r}
row.names(mtpl_glmnet$beta) 
```

]

.pull-right[

.hi-pink[Q.3] plot the solutions path

```{r eval = TRUE, out.width='85%'}
plot(mtpl_glmnet, xvar = 'lambda', label = TRUE)  
```


]

---

class: clear

.pull-left[

.hi-pink[Q.3] pick a value for `lambda`

```{r eval = TRUE, out.width='45%'}
set.seed(123)
fold_id <- sample(rep(1:10, length.out = nrow(mtpl)), 
                  nrow(mtpl))
mtpl_glmnet_cv <- cv.glmnet(x, y, family = "poisson", 
                            alpha = alpha, 
                            nfolds = 10, 
                            foldid = fold_id, 
                            type.measure = "deviance", 
                            standardize = TRUE, 
                            intercept = TRUE)
plot(mtpl_glmnet_cv)
```


]

.pull-right[

```{r eval = TRUE}
coef(mtpl_glmnet_cv, s = "lambda.min")
```

]

---

class: clear

.pull-left[

.hi-pink[Q.3] pick a value for `lambda`

```{r eval = TRUE, out.width='45%'}
set.seed(123)
fold_id <- sample(rep(1:10, length.out = nrow(mtpl)), 
                  nrow(mtpl))
mtpl_glmnet_cv <- cv.glmnet(x, y, family = "poisson", 
                            alpha = alpha, 
                            nfolds = 10, 
                            foldid = fold_id,
                            type.measure = "deviance", 
                            standardize = TRUE, 
                            intercept = TRUE)
plot(mtpl_glmnet_cv)
```


]

.pull-right[

```{r eval = TRUE}
coef(mtpl_glmnet_cv, s = "lambda.1se")
```

]

---

class: clear

.pull-left[
.hi-pink[Q.4] refit the models using only the selected features

```{r eval = TRUE}
mtpl$coverage <- relevel(mtpl$coverage, "PO")
mtpl_formula_refit <- nclaims ~ 1 + coverage + 
                      fuel + use + fleet + sex + 
                      ageph + bm + agec + power
mtpl_glm_refit <- glm(mtpl_formula_refit, 
                      data = mtpl, 
                      offset = log(mtpl$expo), 
                      family = poisson())
```

]

.pull-right[

The selection obtained via `lambda.min`

```{r echo = FALSE}
mtpl$coverage <- relevel(mtpl$coverage, "PO")
mtpl_formula_refit <- nclaims ~ 1 + coverage + fuel + use + fleet + sex + ageph + bm + agec + power
mtpl_glm_refit <- glm(mtpl_formula_refit, data = mtpl, offset = log(mtpl$expo), family = poisson())
mtpl_glm_refit %>% broom::tidy() %>% kable(format = 'html')
```

]

---

class: clear

.pull-left[

.hi-pink[Q.4] refit the models using only the selected features

```{r eval = TRUE}
mtpl_formula_refit_2 <- nclaims ~ 1 + ageph + bm 
mtpl_glm_refit_2 <- glm(mtpl_formula_refit_2, 
                        data = mtpl, 
                        offset = log(mtpl$expo), 
                        family = poisson())
```

]

.pull-right[

The selection obtained via `lambda.1se`

```{r echo = FALSE}
mtpl_formula_refi_2t <- nclaims ~ 1 + ageph + bm 
mtpl_glm_refit_2 <- glm(mtpl_formula_refit_2, data = mtpl, offset = log(mtpl$expo), family = poisson())
mtpl_glm_refit_2 %>% broom::tidy() %>% kable(format = 'html')
```

]
---

# That's a wrap!

.pull-left[


* [Knowing me, knowing you: <br> statistical and machine learning](#knowing)

  - Supervised and unsupervised learning
  - Regression and classification
  - Statistical modeling: the two cultures
  - Creating models in R and tidy model output with {broom}

* [Machine learning foundations](#basics)

  - Model accuracy and loss functions
  - Overfitting and bias-variance tradeoff
  - Data splitting, Resampling methods with {caret} and {rsample}
]

.pull-right[

* [Machine learning foundations](#basics)

  - Parameter tuning with {caret}, {rsample} and {purrr}.

* [Target and feature engineering](#engineering)

  - Data leakage
  - Pre-processing steps
  - Specifying blue-prints with {recipes}
  - Putting it all together: {recipes} and {caret}/{rsample}

* [Regression models](#regression)

  - GLMs with {glm}
  - GAMs with {mgcv}
  - Regularized (G)LMs with {glmnet}.

]

---

# Thanks!  <img src="img/xaringan.png" class="title-hex">

<br>
<br>
<br>
<br>

Slides created with the R package [xaringan](https://github.com/yihui/xaringan).
<br> <br> <br>
Course material available via 
<br>
`r fa(name = "github", fill = KULbg)` https://github.com/katrienantonio/workshop-ML



```{r, eval=FALSE, include=FALSE}
# this code can be used to extract the R code from an R Markdown (Rmd) document
library(knitr)
path <- "C:/Users/u0043788/Dropbox/R tutorial/Basic R"
setwd(path)
file.exists("basic_R_workshop_NN.Rmd")
purl("basic_R_workshop_NN.Rmd")
```

